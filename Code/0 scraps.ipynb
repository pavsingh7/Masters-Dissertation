{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraps\n",
    "\n",
    "This notebook contains extra or scrap pieces of code that are no longer used or relevant for this thesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "data = pd.read_csv('/Users/pavansingh/Desktop/all_revs_1.csv')\n",
    "data.shape\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If We Wanted to Binarise the Results\n",
    "# step 4: calculate Classification Metrics (take the hidden ratings and the predicted ratings and binarise them) ==========================================================================\n",
    "\n",
    "# Binarise the hidden ratings and predicted ratings\n",
    "threshold = 3.5\n",
    "binary_prediction_ratings = (predicted_ratings_array >= threshold).astype(int) \n",
    "print(f\"If predicted rating is greater than or equal to {threshold}, then 1, else 0\\n\")\n",
    "print(\"Predicted Ratings:\", predicted_ratings_array)\n",
    "print(\"Binary Predictions:\", binary_prediction_ratings)\n",
    "binary_hidden_ratings = (hidden_ratings_array >= threshold).astype(int)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Hidden Ratings:\", hidden_ratings_array)\n",
    "print(\"Binary Hidden Ratings:\", binary_hidden_ratings)\n",
    "# calculate accuracy using sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# calculate accuracy using sklearn\n",
    "print(\"Using sklearn\")\n",
    "accuracy = accuracy_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "precision = precision_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "recall = recall_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "f1 = f1_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# calculate accuracy manually\n",
    "print(\"\\n\\nManually\")\n",
    "true_positives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 1))\n",
    "true_negatives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 0))\n",
    "false_positives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 1))\n",
    "false_negatives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 0))\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: calculate Classification Metrics (take the hidden ratings and the predicted ratings and binarise them) ==========================================================================\n",
    "\n",
    "# Binarise the hidden ratings and predicted ratings\n",
    "threshold = 3.5\n",
    "binary_prediction_ratings = (predicted_ratings_array >= threshold).astype(int) \n",
    "print(f\"If predicted rating is greater than or equal to {threshold}, then 1, else 0\\n\")\n",
    "print(\"Predicted Ratings:\", predicted_ratings_array)\n",
    "print(\"Binary Predictions:\", binary_prediction_ratings)\n",
    "binary_hidden_ratings = (hidden_ratings_array >= threshold).astype(int)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Hidden Ratings:\", hidden_ratings_array)\n",
    "print(\"Binary Hidden Ratings:\", binary_hidden_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy using sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# calculate accuracy using sklearn\n",
    "print(\"Using sklearn\")\n",
    "accuracy = accuracy_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "precision = precision_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "recall = recall_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "f1 = f1_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# calculate accuracy manually\n",
    "print(\"\\n\\nManually\")\n",
    "true_positives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 1))\n",
    "true_negatives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 0))\n",
    "false_positives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 1))\n",
    "false_negatives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 0))\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: calculate Classification Metrics (take the hidden ratings and the predicted ratings and binarise them) ==========================================================================\n",
    "\n",
    "# Binarise the hidden ratings and predicted ratings\n",
    "threshold = 3.5\n",
    "binary_prediction_ratings = (predicted_ratings_array >= threshold).astype(int) \n",
    "print(f\"If predicted rating is greater than or equal to {threshold}, then 1, else 0\\n\")\n",
    "print(\"Predicted Ratings:\", predicted_ratings_array)\n",
    "print(\"Binary Predictions:\", binary_prediction_ratings)\n",
    "binary_hidden_ratings = (hidden_ratings_array >= threshold).astype(int)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Hidden Ratings:\", hidden_ratings_array)\n",
    "print(\"Binary Hidden Ratings:\", binary_hidden_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy using sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# calculate accuracy using sklearn\n",
    "print(\"Using sklearn\")\n",
    "accuracy = accuracy_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "precision = precision_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "recall = recall_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "f1 = f1_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# calculate accuracy manually\n",
    "print(\"\\n\\nManually\")\n",
    "true_positives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 1))\n",
    "true_negatives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 0))\n",
    "false_positives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 1))\n",
    "false_negatives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 0))\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix factorization using stochastic gradient descent\n",
    "def matrix_factorization_sgd(R, K, steps=500, alpha=0.001, beta=0.02, overfitting=True, bias=True):\n",
    "    # R = user-item ratings matrix\n",
    "    # K = number of latent features\n",
    "    # steps = number of iterations\n",
    "    # alpha = learning rate\n",
    "    # beta = regularization parameter\n",
    "\n",
    "    # Initialize user and item latent feature matrices\n",
    "    N, M = R.shape\n",
    "    P = np.random.rand(N, K)\n",
    "    Q = np.random.rand(M, K)\n",
    "    Q = Q.T\n",
    "    \n",
    "    # apply stochastic gradient descent to update P and Q\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i, :], Q[:, j])\n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "        eR = np.dot(P, Q)\n",
    "        e = 0\n",
    "        # apply regularization to prevent overfitting\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i, :], Q[:, j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * (pow(P[i][k], 2) + pow(Q[k][j], 2))\n",
    "        # set threshold for error rate\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T\n",
    "\n",
    "# apply matrix factorization using stochastic gradient descent\n",
    "nP, nQ = matrix_factorization_sgd(R=x_hidden.values, K=2)\n",
    "nR = np.dot(nP, nQ.T)\n",
    "\n",
    "# view reconstructed matrix\n",
    "print(\"Reconstructed Matrix\")\n",
    "nR = pd.DataFrame(nR, index=x_hidden.index, columns=x_hidden.columns)\n",
    "nR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Sandbox\n",
    "\n",
    "Here we will test out the workings of item based collaborative filtering. The steps are as follows:\n",
    "\n",
    "1. Have User Item matrix\n",
    "2. Hide some ratings to simulate a test set\n",
    "3. Calculate similarity (cosine similarity)\n",
    "4. Calculate weighted average of ratings\n",
    "5. Fill in missing values with predicted ratings\n",
    "6. Take the predicted ratings and compare them to the hidden ratings\n",
    "7. Calculate MAE, RMSE, MSE\n",
    "8. Binarise the ratings \n",
    "9. Calculate classification metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data.csv\", index_col=0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the original matrix to store hidden ratings\n",
    "x_hidden = x.copy()\n",
    "indices_tracker = []\n",
    "\n",
    "# identifies rated books and randomly selects 2 books to hide ratings for each user\n",
    "np.random.seed(10)  # You can use any integer value as the seed\n",
    "for user_id in range(x_hidden.shape[0]):\n",
    "    rated_books = np.where(x_hidden.iloc[user_id, :] > 0)[0]\n",
    "    print(\"User:\", user_id)\n",
    "    print(\"Indices of Rated Books:\", rated_books)\n",
    "    hidden_indices = np.random.choice(rated_books, min(2, len(rated_books)), replace=False)\n",
    "    indices_tracker.append(hidden_indices)\n",
    "    print(\"Indices to Hide:\", hidden_indices, \"\\n\")\n",
    "    x_hidden.iloc[user_id, hidden_indices] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tracker - all hidden ratings \n",
    "indices_tracker = pd.DataFrame(indices_tracker).to_numpy()\n",
    "print(\"Indices of Ratings per user \\n\", indices_tracker)\n",
    "\n",
    "# flattened\n",
    "indices_tracker_flat = indices_tracker.flatten()\n",
    "print(\"Indices of Ratings per User joined\", indices_tracker_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see updated matrix with hidden ratings\n",
    "print(\"Updated Matrix with Hidden Ratings\")\n",
    "display(x_hidden)\n",
    "\n",
    "# see original matrix\n",
    "print(\"Original Matrix\")\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cosine sim matrix and change to pd dataframe and save to csv\n",
    "pd.DataFrame(cosine_similarity(x_hidden.T).round(2), index=x.columns, columns=x.columns).to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data_sim_mat_cosine.csv\")\n",
    "sim_mat_cos = cosine_similarity(x_hidden.T).round(2)\n",
    "print(\"Cosine Similarity Matrix\") \n",
    "sim_mat_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a predictions matrix\n",
    "predic_matrix = x_hidden.copy()\n",
    "\n",
    "# get predicted ratings for unread books for user 1 using cosine similarity\n",
    "user_ratings = predic_matrix.iloc[0, :].values.reshape(1, -1)\n",
    "unread_books_indices = np.where(user_ratings == 0)[1]\n",
    "rated_books_indices = np.where(user_ratings > 0)[1]\n",
    "\n",
    "for book_id in unread_books_indices:\n",
    "    similarity_i_j = sim_mat_cos[book_id, rated_books_indices]\n",
    "    ratings = user_ratings[0, rated_books_indices]\n",
    "    predicted_rating = np.sum(ratings * similarity_i_j) / np.sum(np.abs(similarity_i_j))\n",
    "    predic_matrix.iloc[0, book_id] = predicted_rating.round(2)\n",
    "\n",
    "# see updated matrix with predicted ratings\n",
    "print(\"Predicted Ratings for User 1\")\n",
    "display(predic_matrix)\n",
    "\n",
    "# save to csv\n",
    "predic_matrix.to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data_predic_matrix_cosine.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get predicted ratings for all users\n",
    "for user_id in range(predic_matrix.shape[0]):\n",
    "    user_ratings = predic_matrix.iloc[user_id, :].values.reshape(1, -1)\n",
    "    unread_books_indices = np.where(user_ratings == 0)[1]\n",
    "    rated_books_indices = np.where(user_ratings > 0)[1]\n",
    "    for book_id in unread_books_indices:\n",
    "        similarity_i_j = sim_mat_cos[book_id, rated_books_indices]\n",
    "        ratings = user_ratings[0, rated_books_indices]\n",
    "        \n",
    "        if np.any(similarity_i_j):\n",
    "            predicted_rating = np.sum(ratings * similarity_i_j) / np.sum(np.abs(similarity_i_j))\n",
    "        else:\n",
    "            # make predicted rating mean of user's ratings\n",
    "            predicted_rating = np.mean(ratings)\n",
    "        \n",
    "        predic_matrix.iloc[user_id, book_id] = predicted_rating.round(2)\n",
    "\n",
    "# see updated matrix with predicted ratings\n",
    "print(\"Predicted Ratings for All Users\")\n",
    "display(predic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now evaluate how good the predictions are vs the hidden ratings\n",
    "# step 1: identify the hidden ratings indices\n",
    "# step 2: extract hidden ratings indices and corresponding predicted ratings indices\n",
    "# step 3: calculate MAE, MSE and RMSE (take the hidden ratings as the true values and the predicted ratings as the predicted values)\n",
    "# step 4:  binarise to get classification metrics\n",
    "\n",
    "# step 1: identify the hidden ratings indices = indices_tracker and get the hidden ratings ==========================================================================\n",
    "hidden_ratings_ind = indices_tracker.copy()\n",
    "\n",
    "# Loop through users to append hidden ratings\n",
    "hidden_ratings_arrays = []\n",
    "\n",
    "# Loop through users to append hidden ratings arrays\n",
    "for user in range(x.shape[0]):\n",
    "    user_hidden_ratings = x.iloc[user, hidden_ratings_ind[user, :]].reset_index(drop=True).values\n",
    "    hidden_ratings_arrays.append(user_hidden_ratings)\n",
    "\n",
    "\n",
    "hidden_ratings_array = pd.DataFrame(hidden_ratings_arrays).to_numpy().flatten()\n",
    "print(\"Hidden Ratings:\", hidden_ratings_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: extract corresponding predicted ratings indices ==========================================================================\n",
    "\n",
    "# Create an empty list to store predicted ratings arrays\n",
    "predicted_ratings_arrays = []\n",
    "\n",
    "# Loop through users to append predicted ratings arrays\n",
    "for user in range(predic_matrix.shape[0]):\n",
    "    user_predicted_ratings = predic_matrix.iloc[user, hidden_ratings_ind[user, :]].reset_index(drop=True).values\n",
    "    predicted_ratings_arrays.append(user_predicted_ratings)\n",
    "\n",
    "predicted_ratings_array = pd.DataFrame(predicted_ratings_arrays).to_numpy().flatten()\n",
    "print(\"Corresponding Predicted Ratings:\", predicted_ratings_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: calculate MAE, MSE and RMSE (take the hidden ratings as the true values and the predicted ratings as the predicted values) ==========================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# calculate MAE, MSE and RMSE\n",
    "print(\"Using sklearn\")\n",
    "mae = mean_absolute_error(hidden_ratings_array, predicted_ratings_array)\n",
    "mse = mean_squared_error(hidden_ratings_array, predicted_ratings_array)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "\n",
    "# Manually\n",
    "print(\"\\n\\nManually\")\n",
    "mae = np.mean(np.abs(hidden_ratings_array - predicted_ratings_array)) # Calculate Mean Absolute Error (MAE)\n",
    "mse = np.mean((hidden_ratings_array - predicted_ratings_array) ** 2) # Calculate Mean Squared Error (MSE)\n",
    "rmse = np.sqrt(mse) # Calculate Root Mean Squared Error (RMSE)\n",
    "\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: calculate Classification Metrics (take the hidden ratings and the predicted ratings and binarise them) ==========================================================================\n",
    "\n",
    "# Binarise the hidden ratings and predicted ratings\n",
    "threshold = 3.5\n",
    "binary_prediction_ratings = (predicted_ratings_array >= threshold).astype(int) \n",
    "print(f\"If predicted rating is greater than or equal to {threshold}, then 1, else 0\\n\")\n",
    "print(\"Predicted Ratings:\", predicted_ratings_array)\n",
    "print(\"Binary Predictions:\", binary_prediction_ratings)\n",
    "binary_hidden_ratings = (hidden_ratings_array >= threshold).astype(int)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Hidden Ratings:\", hidden_ratings_array)\n",
    "print(\"Binary Hidden Ratings:\", binary_hidden_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy using sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# calculate accuracy using sklearn\n",
    "print(\"Using sklearn\")\n",
    "accuracy = accuracy_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "precision = precision_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "recall = recall_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "f1 = f1_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# calculate accuracy manually\n",
    "print(\"\\n\\nManually\")\n",
    "true_positives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 1))\n",
    "true_negatives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 0))\n",
    "false_positives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 1))\n",
    "false_negatives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 0))\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Sandbox\n",
    "\n",
    "Here we will test out the workings of item based collaborative filtering. The steps are as follows:\n",
    "\n",
    "1. Have User Item matrix\n",
    "2. Hide some ratings to simulate a test set\n",
    "3. Calculate similarity (cosine similarity)\n",
    "4. Calculate weighted average of ratings\n",
    "5. Fill in missing values with predicted ratings\n",
    "6. Take the predicted ratings and compare them to the hidden ratings\n",
    "7. Calculate MAE, RMSE, MSE\n",
    "8. Binarise the ratings \n",
    "9. Calculate classification metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data.csv\", index_col=0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the original matrix to store hidden ratings\n",
    "x_hidden = x.copy()\n",
    "indices_tracker = []\n",
    "\n",
    "# identifies rated books and randomly selects 2 books to hide ratings for each user\n",
    "np.random.seed(10)  # You can use any integer value as the seed\n",
    "for user_id in range(x_hidden.shape[0]):\n",
    "    rated_books = np.where(x_hidden.iloc[user_id, :] > 0)[0]\n",
    "    print(\"User:\", user_id)\n",
    "    print(\"Indices of Rated Books:\", rated_books)\n",
    "    hidden_indices = np.random.choice(rated_books, min(2, len(rated_books)), replace=False)\n",
    "    indices_tracker.append(hidden_indices)\n",
    "    print(\"Indices to Hide:\", hidden_indices, \"\\n\")\n",
    "    x_hidden.iloc[user_id, hidden_indices] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tracker - all hidden ratings \n",
    "indices_tracker = pd.DataFrame(indices_tracker).to_numpy()\n",
    "print(\"Indices of Ratings per user \\n\", indices_tracker)\n",
    "\n",
    "# flattened\n",
    "indices_tracker_flat = indices_tracker.flatten()\n",
    "print(\"Indices of Ratings per User joined\", indices_tracker_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see updated matrix with hidden ratings\n",
    "print(\"Updated Matrix with Hidden Ratings\")\n",
    "display(x_hidden)\n",
    "\n",
    "# see original matrix\n",
    "print(\"Original Matrix\")\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cosine sim matrix and change to pd dataframe and save to csv\n",
    "pd.DataFrame(cosine_similarity(x_hidden.T).round(2), index=x.columns, columns=x.columns).to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data_sim_mat_cosine.csv\")\n",
    "sim_mat_cos = cosine_similarity(x_hidden.T).round(2)\n",
    "print(\"Cosine Similarity Matrix\") \n",
    "sim_mat_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a predictions matrix\n",
    "predic_matrix = x_hidden.copy()\n",
    "\n",
    "# get predicted ratings for unread books for user 1 using cosine similarity\n",
    "user_ratings = predic_matrix.iloc[0, :].values.reshape(1, -1)\n",
    "unread_books_indices = np.where(user_ratings == 0)[1]\n",
    "rated_books_indices = np.where(user_ratings > 0)[1]\n",
    "\n",
    "for book_id in unread_books_indices:\n",
    "    similarity_i_j = sim_mat_cos[book_id, rated_books_indices]\n",
    "    ratings = user_ratings[0, rated_books_indices]\n",
    "    predicted_rating = np.sum(ratings * similarity_i_j) / np.sum(np.abs(similarity_i_j))\n",
    "    predic_matrix.iloc[0, book_id] = predicted_rating.round(2)\n",
    "\n",
    "# see updated matrix with predicted ratings\n",
    "print(\"Predicted Ratings for User 1\")\n",
    "display(predic_matrix)\n",
    "\n",
    "# save to csv\n",
    "predic_matrix.to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data_predic_matrix_cosine.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get predicted ratings for all users\n",
    "for user_id in range(predic_matrix.shape[0]):\n",
    "    user_ratings = predic_matrix.iloc[user_id, :].values.reshape(1, -1)\n",
    "    unread_books_indices = np.where(user_ratings == 0)[1]\n",
    "    rated_books_indices = np.where(user_ratings > 0)[1]\n",
    "    for book_id in unread_books_indices:\n",
    "        similarity_i_j = sim_mat_cos[book_id, rated_books_indices]\n",
    "        ratings = user_ratings[0, rated_books_indices]\n",
    "        \n",
    "        if np.any(similarity_i_j):\n",
    "            predicted_rating = np.sum(ratings * similarity_i_j) / np.sum(np.abs(similarity_i_j))\n",
    "        else:\n",
    "            # make predicted rating mean of user's ratings\n",
    "            predicted_rating = np.mean(ratings)\n",
    "        \n",
    "        predic_matrix.iloc[user_id, book_id] = predicted_rating.round(2)\n",
    "\n",
    "# see updated matrix with predicted ratings\n",
    "print(\"Predicted Ratings for All Users\")\n",
    "display(predic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now evaluate how good the predictions are vs the hidden ratings\n",
    "# step 1: identify the hidden ratings indices\n",
    "# step 2: extract hidden ratings indices and corresponding predicted ratings indices\n",
    "# step 3: calculate MAE, MSE and RMSE (take the hidden ratings as the true values and the predicted ratings as the predicted values)\n",
    "# step 4:  binarise to get classification metrics\n",
    "\n",
    "# step 1: identify the hidden ratings indices = indices_tracker and get the hidden ratings ==========================================================================\n",
    "hidden_ratings_ind = indices_tracker.copy()\n",
    "\n",
    "# Loop through users to append hidden ratings\n",
    "hidden_ratings_arrays = []\n",
    "\n",
    "# Loop through users to append hidden ratings arrays\n",
    "for user in range(x.shape[0]):\n",
    "    user_hidden_ratings = x.iloc[user, hidden_ratings_ind[user, :]].reset_index(drop=True).values\n",
    "    hidden_ratings_arrays.append(user_hidden_ratings)\n",
    "\n",
    "\n",
    "hidden_ratings_array = pd.DataFrame(hidden_ratings_arrays).to_numpy().flatten()\n",
    "print(\"Hidden Ratings:\", hidden_ratings_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: extract corresponding predicted ratings indices ==========================================================================\n",
    "\n",
    "# Create an empty list to store predicted ratings arrays\n",
    "predicted_ratings_arrays = []\n",
    "\n",
    "# Loop through users to append predicted ratings arrays\n",
    "for user in range(predic_matrix.shape[0]):\n",
    "    user_predicted_ratings = predic_matrix.iloc[user, hidden_ratings_ind[user, :]].reset_index(drop=True).values\n",
    "    predicted_ratings_arrays.append(user_predicted_ratings)\n",
    "\n",
    "predicted_ratings_array = pd.DataFrame(predicted_ratings_arrays).to_numpy().flatten()\n",
    "print(\"Corresponding Predicted Ratings:\", predicted_ratings_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: calculate MAE, MSE and RMSE (take the hidden ratings as the true values and the predicted ratings as the predicted values) ==========================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# calculate MAE, MSE and RMSE\n",
    "print(\"Using sklearn\")\n",
    "mae = mean_absolute_error(hidden_ratings_array, predicted_ratings_array)\n",
    "mse = mean_squared_error(hidden_ratings_array, predicted_ratings_array)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "\n",
    "# Manually\n",
    "print(\"\\n\\nManually\")\n",
    "mae = np.mean(np.abs(hidden_ratings_array - predicted_ratings_array)) # Calculate Mean Absolute Error (MAE)\n",
    "mse = np.mean((hidden_ratings_array - predicted_ratings_array) ** 2) # Calculate Mean Squared Error (MSE)\n",
    "rmse = np.sqrt(mse) # Calculate Root Mean Squared Error (RMSE)\n",
    "\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: calculate Classification Metrics (take the hidden ratings and the predicted ratings and binarise them) ==========================================================================\n",
    "\n",
    "# Binarise the hidden ratings and predicted ratings\n",
    "threshold = 3.5\n",
    "binary_prediction_ratings = (predicted_ratings_array >= threshold).astype(int) \n",
    "print(f\"If predicted rating is greater than or equal to {threshold}, then 1, else 0\\n\")\n",
    "print(\"Predicted Ratings:\", predicted_ratings_array)\n",
    "print(\"Binary Predictions:\", binary_prediction_ratings)\n",
    "binary_hidden_ratings = (hidden_ratings_array >= threshold).astype(int)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Hidden Ratings:\", hidden_ratings_array)\n",
    "print(\"Binary Hidden Ratings:\", binary_hidden_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy using sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# calculate accuracy using sklearn\n",
    "print(\"Using sklearn\")\n",
    "accuracy = accuracy_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "precision = precision_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "recall = recall_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "f1 = f1_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# calculate accuracy manually\n",
    "print(\"\\n\\nManually\")\n",
    "true_positives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 1))\n",
    "true_negatives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 0))\n",
    "false_positives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 1))\n",
    "false_negatives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 0))\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'K': [2, 3],         # Number of latent features\n",
    "    'alpha': [0.001, 0.01], # Learning rate\n",
    "    'beta': [0.01, 0.02]    # Regularization parameter\n",
    "}\n",
    "\n",
    "# Create an instance of the GridSearchCV\n",
    "np.random.seed(42)\n",
    "grid_search = GridSearchCV(estimator=matrix_factorization_sgd, param_grid=param_grid, cv=2)\n",
    "grid_search.fit(R)\n",
    "\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_K = grid_search.best_params_['K']\n",
    "best_beta = grid_search.best_params_['beta']\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "\n",
    "print(f\"Best K: {best_K}\")\n",
    "print(f\"Best beta: {best_beta}\")\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Re-train the model with best hyperparameters\n",
    "best_model = matrix_factorization_sgd(R=x_hidden.values, K=best_K, alpha=best_beta, beta=best_beta,\n",
    "                                      use_regularization=True, use_bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Sandbox\n",
    "\n",
    "Here we will test out the workings of matrix factorisation collaborative filtering. Specifically, we will be conducting non-negative matrix factorisation (NMF) and alternating least squares (ALS) matrix factorisation. We will be using the sample data created. The steps are as follows:\n",
    "\n",
    "1. Have User Item matrix\n",
    "2. Hide some ratings to simulate a test set\n",
    "3. Factorise the matrix - *either NMF or ALS or even SVD*\n",
    "4. Predict the hidden ratings - fill in missing values with predicted ratings\n",
    "6. Take the predicted ratings and compare them to the hidden ratings\n",
    "7. Calculate MAE, RMSE, MSE\n",
    "8. Binarise the ratings\n",
    "9. Calculate classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorisation w/SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data.csv\", index_col=0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the original matrix to store hidden ratings\n",
    "x_hidden = x.copy()\n",
    "indices_tracker = []\n",
    "\n",
    "# identifies rated books and randomly selects 2 books to hide ratings for each user\n",
    "np.random.seed(10)  # we can use any integer value as the seed\n",
    "for user_id in range(x_hidden.shape[0]):\n",
    "    rated_books = np.where(x_hidden.iloc[user_id, :] > 0)[0]\n",
    "    print(\"User:\", user_id)\n",
    "    print(\"Indices of Rated Books:\", rated_books)\n",
    "    hidden_indices = np.random.choice(rated_books, min(2, len(rated_books)), replace=False)\n",
    "    indices_tracker.append(hidden_indices)\n",
    "    print(\"Indices to Hide:\", hidden_indices, \"\\n\")\n",
    "    x_hidden.iloc[user_id, hidden_indices] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tracker - all hidden ratings \n",
    "indices_tracker = pd.DataFrame(indices_tracker).to_numpy()\n",
    "print(\"Indices of Ratings per user \\n\", indices_tracker)\n",
    "\n",
    "# flattened\n",
    "indices_tracker_flat = indices_tracker.flatten()\n",
    "print(\"Indices of Ratings per User joined\", indices_tracker_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see updated matrix with hidden ratings\n",
    "print(\"Updated Matrix with Hidden Ratings\")\n",
    "display(x_hidden)\n",
    "\n",
    "# see original matrix\n",
    "print(\"Original Matrix\")\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix factorization using stochastic gradient descent\n",
    "def matrix_factorization_sgd(R, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    # R = user-item ratings matrix\n",
    "    # K = number of latent features\n",
    "    # steps = number of iterations\n",
    "    # alpha = learning rate\n",
    "    # beta = regularization parameter\n",
    "\n",
    "    # Initialize user and item latent feature matrices\n",
    "    N, M = R.shape\n",
    "    P = np.random.rand(N, K)\n",
    "    Q = np.random.rand(M, K)\n",
    "    Q = Q.T\n",
    "    \n",
    "    # apply stochastic gradient descent to update P and Q\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i, :], Q[:, j])\n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "        eR = np.dot(P, Q)\n",
    "        e = 0\n",
    "        # apply regularization to prevent overfitting\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i, :], Q[:, j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * (pow(P[i][k], 2) + pow(Q[k][j], 2))\n",
    "        # set threshold for error rate\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T\n",
    "\n",
    "# apply matrix factorization using stochastic gradient descent\n",
    "nP, nQ = matrix_factorization_sgd(R=x_hidden.values, K=2)\n",
    "nR = np.dot(nP, nQ.T)\n",
    "\n",
    "# view reconstructed matrix\n",
    "print(\"Reconstructed Matrix\")\n",
    "nR = pd.DataFrame(nR, index=x_hidden.index, columns=x_hidden.columns)\n",
    "nR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now evaluate how good the predictions are vs the hidden ratings\n",
    "# step 1: identify the hidden ratings indices\n",
    "# step 2: extract hidden ratings indices and corresponding predicted ratings indices\n",
    "# step 3: calculate MAE, MSE and RMSE (take the hidden ratings as the true values and the predicted ratings as the predicted values)\n",
    "# step 4:  binarise to get classification metrics\n",
    "\n",
    "# step 1: identify the hidden ratings indices = indices_tracker and get the hidden ratings ==========================================================================\n",
    "hidden_ratings_ind = indices_tracker.copy()\n",
    "\n",
    "# Loop through users to append hidden ratings\n",
    "hidden_ratings_arrays = []\n",
    "\n",
    "# Loop through users to append hidden ratings arrays\n",
    "for user in range(x.shape[0]):\n",
    "    user_hidden_ratings = x.iloc[user, hidden_ratings_ind[user, :]].reset_index(drop=True).values\n",
    "    hidden_ratings_arrays.append(user_hidden_ratings)\n",
    "\n",
    "\n",
    "hidden_ratings_array = pd.DataFrame(hidden_ratings_arrays).to_numpy().flatten()\n",
    "print(\"Hidden Ratings:\", hidden_ratings_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: extract corresponding predicted ratings indices ==========================================================================\n",
    "\n",
    "# Create an empty list to store predicted ratings arrays\n",
    "predicted_ratings_arrays = []\n",
    "\n",
    "# Loop through users to append predicted ratings arrays\n",
    "for user in range(nR.shape[0]):\n",
    "    user_predicted_ratings = nR.iloc[user, hidden_ratings_ind[user, :]].reset_index(drop=True).values\n",
    "    predicted_ratings_arrays.append(user_predicted_ratings)\n",
    "\n",
    "predicted_ratings_array = pd.DataFrame(predicted_ratings_arrays).to_numpy().flatten()\n",
    "print(\"Corresponding Predicted Ratings:\", predicted_ratings_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: calculate MAE, MSE and RMSE (take the hidden ratings as the true values and the predicted ratings as the predicted values) ==========================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# calculate MAE, MSE and RMSE\n",
    "print(\"Using sklearn\")\n",
    "mae = mean_absolute_error(hidden_ratings_array, predicted_ratings_array)\n",
    "mse = mean_squared_error(hidden_ratings_array, predicted_ratings_array)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "\n",
    "# Manually\n",
    "print(\"\\n\\nManually\")\n",
    "mae = np.mean(np.abs(hidden_ratings_array - predicted_ratings_array)) # Calculate Mean Absolute Error (MAE)\n",
    "mse = np.mean((hidden_ratings_array - predicted_ratings_array) ** 2) # Calculate Mean Squared Error (MSE)\n",
    "rmse = np.sqrt(mse) # Calculate Root Mean Squared Error (RMSE)\n",
    "\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: calculate Classification Metrics (take the hidden ratings and the predicted ratings and binarise them) ==========================================================================\n",
    "\n",
    "# Binarise the hidden ratings and predicted ratings\n",
    "threshold = 3.5\n",
    "binary_prediction_ratings = (predicted_ratings_array >= threshold).astype(int) \n",
    "print(f\"If predicted rating is greater than or equal to {threshold}, then 1, else 0\\n\")\n",
    "print(\"Predicted Ratings:\", predicted_ratings_array)\n",
    "print(\"Binary Predictions:\", binary_prediction_ratings)\n",
    "binary_hidden_ratings = (hidden_ratings_array >= threshold).astype(int)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Hidden Ratings:\", hidden_ratings_array)\n",
    "print(\"Binary Hidden Ratings:\", binary_hidden_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy using sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# calculate accuracy using sklearn\n",
    "print(\"Using sklearn\")\n",
    "accuracy = accuracy_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "precision = precision_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "recall = recall_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "f1 = f1_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# calculate accuracy manually\n",
    "print(\"\\n\\nManually\")\n",
    "true_positives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 1))\n",
    "true_negatives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 0))\n",
    "false_positives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 1))\n",
    "false_negatives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 0))\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least Squares (ALS)\n",
    "\n",
    "The main difference is in the optimization method used to decompose the matrix. Instead of using NMF, you use the ALS algorithm.\n",
    "\n",
    "**Algorithm Process**:\n",
    "\n",
    "***Convert the data into a matrix:*** Same as in NMF, represent users, items, and ratings in a matrix.\n",
    "\n",
    "***Hide some ratings for testing:*** Randomly select a subset of ratings to be used as a test set.\n",
    "\n",
    "***Decompose the matrix using ALS:*** Utilize an ALS algorithm to iteratively update the user and item matrices until convergence. We may need to define hyperparameters such as the number of latent features and regularization terms.\n",
    "\n",
    "***Reconstruct the original matrix:*** Combine the user and item matrices obtained from the ALS optimization to reconstruct the original matrix.\n",
    "\n",
    "***Make predictions using the reconstructed matrix:*** Use the reconstructed matrix to predict the ratings for the items that were hidden in the test set.\n",
    "\n",
    "***Evaluate the performance of the algorithm:*** Compare the predicted ratings to the actual ratings in the test set to evaluate the accuracy and effectiveness of your collaborative filtering algorithm. Use appropriate evaluation metrics such as MSE, RMSE, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data.csv\", index_col=0)\n",
    "\n",
    "# create a copy of the original matrix to store hidden ratings\n",
    "x_hidden = x.copy()\n",
    "indices_tracker = []\n",
    "\n",
    "# identifies rated books and randomly selects 2 books to hide ratings for each user\n",
    "np.random.seed(10)  # You can use any integer value as the seed\n",
    "for user_id in range(x_hidden.shape[0]):\n",
    "    rated_books = np.where(x_hidden.iloc[user_id, :] > 0)[0]\n",
    "    hidden_indices = np.random.choice(rated_books, min(2, len(rated_books)), replace=False)\n",
    "    indices_tracker.append(hidden_indices)\n",
    "    x_hidden.iloc[user_id, hidden_indices] = 0\n",
    "\n",
    "# check tracker - all hidden ratings \n",
    "indices_tracker = pd.DataFrame(indices_tracker).to_numpy()\n",
    "\n",
    "# flattened\n",
    "indices_tracker_flat = indices_tracker.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_als(R, K, steps=5000, alpha=0.0001, reg_param=0.001):\n",
    "    N, M = R.shape\n",
    "    P = np.random.rand(N, K) * 0.01\n",
    "    Q = np.random.rand(M, K) * 0.01\n",
    "\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i, :], Q[j, :].T)\n",
    "                    P_norm = np.linalg.norm(P[i, :], 2)\n",
    "                    Q_norm = np.linalg.norm(Q[j, :], 2)\n",
    "                    P[i, :] = P[i, :] + alpha * (2 * eij * Q[j, :] - reg_param * P_norm)\n",
    "                    Q[j, :] = Q[j, :] + alpha * (2 * eij * P[i, :] - reg_param * Q_norm)\n",
    "\n",
    "        e = 0\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i, :], Q[j, :].T), 2)\n",
    "        if e < 0.001:\n",
    "            break\n",
    "\n",
    "    return P, Q\n",
    "\n",
    "\n",
    "# apply matrix factorization using alternating least squares\n",
    "nP, nQ = matrix_factorization_als(R = x_hidden.values, K=2)\n",
    "nR = np.dot(nP, nQ.T)\n",
    "\n",
    "# view reconstructed matrix\n",
    "print(\"Reconstructed Matrix\")\n",
    "nR = pd.DataFrame(nR, index=x_hidden.index, columns=x_hidden.columns)\n",
    "nR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now evaluate how good the predictions are vs the hidden ratings\n",
    "# step 1: identify the hidden ratings indices\n",
    "# step 2: extract hidden ratings indices and corresponding predicted ratings indices\n",
    "# step 3: calculate MAE, MSE and RMSE (take the hidden ratings as the true values and the predicted ratings as the predicted values)\n",
    "# step 4:  binarise to get classification metrics\n",
    "\n",
    "# step 1: identify the hidden ratings indices = indices_tracker and get the hidden ratings ==========================================================================\n",
    "hidden_ratings_ind = indices_tracker.copy()\n",
    "\n",
    "# Loop through users to append hidden ratings\n",
    "hidden_ratings_arrays = []\n",
    "\n",
    "# Loop through users to append hidden ratings arrays\n",
    "for user in range(x.shape[0]):\n",
    "    user_hidden_ratings = x.iloc[user, hidden_ratings_ind[user, :]].reset_index(drop=True).values\n",
    "    hidden_ratings_arrays.append(user_hidden_ratings)\n",
    "\n",
    "\n",
    "hidden_ratings_array = pd.DataFrame(hidden_ratings_arrays).to_numpy().flatten()\n",
    "\n",
    "\n",
    "# step 2: extract corresponding predicted ratings indices ==========================================================================\n",
    "\n",
    "# Create an empty list to store predicted ratings arrays\n",
    "predicted_ratings_arrays = []\n",
    "\n",
    "# Loop through users to append predicted ratings arrays\n",
    "for user in range(nR.shape[0]):\n",
    "    user_predicted_ratings = nR.iloc[user, hidden_ratings_ind[user, :]].reset_index(drop=True).values\n",
    "    predicted_ratings_arrays.append(user_predicted_ratings)\n",
    "\n",
    "predicted_ratings_array = pd.DataFrame(predicted_ratings_arrays).to_numpy().flatten()\n",
    "\n",
    "\n",
    "# step 3: calculate MAE, MSE and RMSE (take the hidden ratings as the true values and the predicted ratings as the predicted values) ==========================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# calculate MAE, MSE and RMSE\n",
    "print(\"\\nRATINGS PREDICTION PROBLEM\\nUsing sklearn\")\n",
    "mae = mean_absolute_error(hidden_ratings_array, predicted_ratings_array)\n",
    "mse = mean_squared_error(hidden_ratings_array, predicted_ratings_array)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "\n",
    "# Manually\n",
    "print(\"\\nManually\")\n",
    "mae = np.mean(np.abs(hidden_ratings_array - predicted_ratings_array)) # Calculate Mean Absolute Error (MAE)\n",
    "mse = np.mean((hidden_ratings_array - predicted_ratings_array) ** 2) # Calculate Mean Squared Error (MSE)\n",
    "rmse = np.sqrt(mse) # Calculate Root Mean Squared Error (RMSE)\n",
    "\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# step 4: calculate Classification Metrics (take the hidden ratings and the predicted ratings and binarise them) ==========================================================================\n",
    "\n",
    "# Binarise the hidden ratings and predicted ratings\n",
    "threshold = 3.5\n",
    "binary_prediction_ratings = (predicted_ratings_array >= threshold).astype(int) \n",
    "binary_hidden_ratings = (hidden_ratings_array >= threshold).astype(int)\n",
    "print(\"\\n\\nBINARISED or CLASSIFICATION PROBLEM\")\n",
    "\n",
    "# calculate accuracy using sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# calculate accuracy using sklearn\n",
    "print(\"Using sklearn\")\n",
    "accuracy = accuracy_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "precision = precision_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "recall = recall_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "f1 = f1_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# calculate accuracy manually\n",
    "print(\"\\nManually\")\n",
    "true_positives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 1))\n",
    "true_negatives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 0))\n",
    "false_positives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 1))\n",
    "false_negatives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 0))\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "R = x_hidden.values\n",
    "matrix_factorization_sgd = MatrixFactorizationSGD()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'K': [2, 3],         # Number of latent features\n",
    "    'alpha': [0.001, 0.01], # Learning rate\n",
    "    'beta': [0.01, 0.02]    # Regularization parameter\n",
    "}\n",
    "\n",
    "# Custom scoring function\n",
    "def custom_scoring_function(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return -mean_squared_error(y_true, y_pred)  # Use negative mean squared error as the score\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=matrix_factorization_sgd, param_grid=param_grid, cv=2, scoring=custom_scoring_function)\n",
    "grid_search.fit(R)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_K = grid_search.best_params_['K']\n",
    "best_beta = grid_search.best_params_['beta']\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "\n",
    "print(f\"Best K: {best_K}\")\n",
    "print(f\"Best beta: {best_beta}\")\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Re-train the model with best hyperparameters\n",
    "best_model = MatrixFactorizationSGD(K=best_K, alpha=best_beta, beta=best_beta,\n",
    "                                      use_regularization=True, use_bias=True)\n",
    "# Fit the model (i.e., find the optimal P and Q)\n",
    "best_model.fit(R)\n",
    "\n",
    "# Predictions with best model \n",
    "best_R_pred = best_model.predict(R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MatrixFactorizationSGD(BaseEstimator):\n",
    "    def __init__(self, K=2, alpha=0.001, beta=0.02, use_regularization=True, use_bias=True, steps=500):\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.use_regularization = use_regularization\n",
    "        self.use_bias = use_bias\n",
    "        self.steps = steps\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        N, M = X.shape\n",
    "        self.P = np.abs(np.random.randn(N, self.K))  # Initialize with non-negative values\n",
    "        self.Q = np.abs(np.random.randn(M, self.K))\n",
    "\n",
    "        # Initialize bias terms\n",
    "        if self.use_bias:\n",
    "            self.b_u = np.zeros(N)\n",
    "            self.b_i = np.zeros(M)\n",
    "            self.b = np.mean(X[np.where(X != 0)])  # global bias\n",
    "\n",
    "        for step in range(self.steps):\n",
    "            for i in range(N):\n",
    "                for j in range(M):\n",
    "                    if X[i][j] > 0:\n",
    "                        eij = X[i][j] - np.dot(self.P[i, :], self.Q[j, :])\n",
    "\n",
    "                        # Update P and Q\n",
    "                        for k in range(self.K):\n",
    "                            if self.use_regularization:\n",
    "                                self.P[i][k] += self.alpha * (2 * eij * self.Q[j][k] - self.beta * self.P[i][k])\n",
    "                                self.Q[j][k] += self.alpha * (2 * eij * self.P[i][k] - self.beta * self.Q[j][k])\n",
    "                            else:\n",
    "                                self.P[i][k] += self.alpha * (2 * eij * self.Q[j][k])\n",
    "                                self.Q[j][k] += self.alpha * (2 * eij * self.P[i][k])\n",
    "\n",
    "                        # Update bias terms\n",
    "                        if self.use_bias:\n",
    "                            self.b_u[i] += self.alpha * (eij - self.beta * self.b_u[i])\n",
    "                            self.b_i[j] += self.alpha * (eij - self.beta * self.b_i[j])\n",
    "\n",
    "            # Check for convergence within the loop\n",
    "            if np.sqrt(np.sum((X - np.dot(self.P, self.Q.T))**2)) < 0.001:\n",
    "                break\n",
    "\n",
    "        # Add bias terms to the prediction\n",
    "        if self.use_bias:\n",
    "            self.R_pred = np.dot(self.P, self.Q.T) + self.b + self.b_u[:, np.newaxis] + self.b_i[np.newaxis:,]\n",
    "        else:\n",
    "            self.R_pred = np.dot(self.P, self.Q.T)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.R_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Example\n",
    "\n",
    "This is a basic implementation of the NCF model. For more advanced features (like adding more layers to the model or using different activation functions), you might need to modify the code accordingly. Also, remember to handle overfitting and underfitting by tuning your model and using techniques like early stopping, regularization, etc.\n",
    "\n",
    "The code below will train a NCF model on your data and then use it to predict the ratings. The predicted ratings will be stored in the y_pred variable. You can adjust the parameters of the model (such as the number of epochs, the batch size, and the dimensions of the embedding layers) to better fit your data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset space\n",
    "%reset -f\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "# tensorflow libraries\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten, Concatenate, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get original data\n",
    "data = pd.read_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data.csv\", index_col=0)\n",
    "print(\"Original Data:\")\n",
    "display(data)\n",
    "\n",
    "# create a copy of the original matrix to store hidden ratings\n",
    "data_hidden = data.copy()\n",
    "\n",
    "# identifies rated books and randomly selects 2 books to hide ratings for each user\n",
    "np.random.seed(10)  # You can use any integer value as the seed\n",
    "for user_id in range(data_hidden.shape[0]):\n",
    "    rated_books = np.where(data_hidden.iloc[user_id, :] > 0)[0]\n",
    "    hidden_indices = np.random.choice(rated_books, min(2, len(rated_books)), replace=False)\n",
    "    data_hidden.iloc[user_id, hidden_indices] = 'Hidden'\n",
    "\n",
    "# get indices of hidden ratings\n",
    "test_data = data_hidden.copy()\n",
    "test_data = test_data.reset_index()\n",
    "test_data = test_data.melt(id_vars=test_data.columns[0], var_name='book', value_name='rating')\n",
    "test_data.columns = ['user', 'book', 'rating']\n",
    "indices_hidden = test_data[test_data['rating'] == 'Hidden'].index\n",
    "print(\"Indices of Hidden Ratings:\", indices_hidden)\n",
    "\n",
    "# check hidden data\n",
    "print(\"\\n\\nHidden Data:\")\n",
    "display(data_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame into a format where each row is a user-item interaction\n",
    "data_hidden = data_hidden.reset_index()\n",
    "data_hidden = data_hidden.melt(id_vars=data_hidden.columns[0], var_name='book', value_name='rating')\n",
    "\n",
    "# change rows with hidden ratings to 0\n",
    "data_hidden.iloc[indices_hidden, 2] = 0\n",
    "\n",
    "print(\"Melted Data:\")\n",
    "display(data_hidden)\n",
    "data_hidden.to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data_hidden.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "data_hidden.columns = ['user', 'book', 'rating']\n",
    "\n",
    "# Filter out the rows where rating is 0\n",
    "data_hidden = data_hidden[data_hidden['rating'] != 0]\n",
    "print(\"Training Data with Hidden Ratings and Non-Zero Ratings:\")\n",
    "display(data_hidden)\n",
    "data_hidden.to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data_hidden_zeros.csv\")\n",
    "\n",
    "# Convert user and book to categorical\n",
    "data_hidden['user'] = data_hidden['user'].astype('category')\n",
    "data_hidden['book'] = data_hidden['book'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user and book embedding layers\n",
    "user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "book_input = Input(shape=(1,), dtype='int32', name='book_input')\n",
    "\n",
    "user_embedding = Embedding(input_dim=len(data_hidden['user'].cat.categories), output_dim=50, name='user_embedding')(user_input)\n",
    "book_embedding = Embedding(input_dim=len(data_hidden['book'].cat.categories), output_dim=50, name='book_embedding')(book_input)\n",
    "\n",
    "# Flatten the embedding vectors\n",
    "user_vecs = Flatten()(user_embedding)\n",
    "book_vecs = Flatten()(book_embedding)\n",
    "\n",
    "# Concatenate the embedding vectors\n",
    "input_vecs = Concatenate()([user_vecs, book_vecs])\n",
    "\n",
    "# Add dense layers\n",
    "x = Dense(128, activation='relu')(input_vecs)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "y = Dense(1)(x)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[user_input, book_input], outputs=y)\n",
    "model.compile(optimizer=Adam(0.001), loss='mse')\n",
    "\n",
    "# Prepare the data - trining\n",
    "X = data_hidden[['user', 'book']].apply(lambda x: x.cat.codes)\n",
    "y = data_hidden['rating'].astype(np.float64)\n",
    "y = (y - 1) / 4\n",
    "\n",
    "# Prepare the data - testing\n",
    "copy = data.copy()\n",
    "copy = copy.reset_index()\n",
    "copy = copy.melt(id_vars=copy.columns[0], var_name='book', value_name='rating')\n",
    "copy.columns = ['user', 'book', 'rating']\n",
    "test_x = copy.iloc[indices_hidden, 0:2]\n",
    "test_x['user'] = test_x['user'].astype('category')\n",
    "test_x['book'] = test_x['book'].astype('category')\n",
    "test_x = test_x.apply(lambda x: x.cat.codes)\n",
    "test_y = copy.iloc[indices_hidden, 2].astype(np.float64)\n",
    "test_y = (test_y - 1) / 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see training X and y\n",
    "print(\"Training X:\")\n",
    "display(X)\n",
    "\n",
    "print(\"Training y:\")\n",
    "display(y)\n",
    "\n",
    "# save\n",
    "X.to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_X.csv\")\n",
    "y.to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see training X and y\n",
    "print(\"Testing X:\")\n",
    "display(test_x)\n",
    "\n",
    "print(\"Testing y:\")\n",
    "display(test_y)\n",
    "\n",
    "# save\n",
    "test_x.to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_test_X.csv\")\n",
    "test_y.to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_test_Y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit([X['user'], X['book']], y, batch_size=64, epochs=50, validation_split=0.1)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the ratings\n",
    "y_pred = model.predict([test_x['user'], test_x['book']])\n",
    "\n",
    "# Rescale the predictions back to the 1 - 5 range\n",
    "y_pred = y_pred * 4 + 1\n",
    "\n",
    "# see predictions\n",
    "print(\"Predictions:\")\n",
    "display(y_pred)\n",
    "\n",
    "# save\n",
    "pd.DataFrame(y_pred).to_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_y_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set predictions and actual ratings to variables\n",
    "hidden_ratings_array = (np.array(test_y)*4 + 1)\n",
    "predicted_ratings_array = np.array(y_pred).flatten()\n",
    "\n",
    "# see\n",
    "print(\"Hidden Ratings:\")\n",
    "display(hidden_ratings_array)\n",
    "\n",
    "print(\"Predicted Ratings:\")\n",
    "display(predicted_ratings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MAE, MSE and RMSE (take the hidden ratings as the true values and the predicted ratings as the predicted values) ==========================================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# calculate MAE, MSE and RMSE\n",
    "print(\"Using sklearn\")\n",
    "mae = mean_absolute_error(hidden_ratings_array, predicted_ratings_array)\n",
    "mse = mean_squared_error(hidden_ratings_array, predicted_ratings_array)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "\n",
    "# Manually\n",
    "print(\"\\n\\nManually\")\n",
    "mae = np.mean(np.abs(hidden_ratings_array - predicted_ratings_array)) # Calculate Mean Absolute Error (MAE)\n",
    "mse = np.mean((hidden_ratings_array - predicted_ratings_array) ** 2) # Calculate Mean Squared Error (MSE)\n",
    "rmse = np.sqrt(mse) # Calculate Root Mean Squared Error (RMSE)\n",
    "\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: calculate Classification Metrics (take the hidden ratings and the predicted ratings and binarise them) ==========================================================================\n",
    "\n",
    "# Binarise the hidden ratings and predicted ratings\n",
    "threshold = 3.5\n",
    "binary_prediction_ratings = (predicted_ratings_array >= threshold).astype(int) \n",
    "print(f\"If predicted rating is greater than or equal to {threshold}, then 1, else 0\\n\")\n",
    "print(\"Predicted Ratings:\", predicted_ratings_array)\n",
    "print(\"Binary Predictions:\", binary_prediction_ratings)\n",
    "binary_hidden_ratings = (hidden_ratings_array >= threshold).astype(int)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Hidden Ratings:\", hidden_ratings_array)\n",
    "print(\"Binary Hidden Ratings:\", binary_hidden_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy using sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# calculate accuracy using sklearn\n",
    "print(\"Using sklearn\")\n",
    "accuracy = accuracy_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "precision = precision_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "recall = recall_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "f1 = f1_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# calculate accuracy manually\n",
    "print(\"\\n\\nManually\")\n",
    "true_positives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 1))\n",
    "true_negatives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 0))\n",
    "false_positives = np.sum((binary_hidden_ratings == 0) & (binary_prediction_ratings == 1))\n",
    "false_negatives = np.sum((binary_hidden_ratings == 1) & (binary_prediction_ratings == 0))\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Experimenting with different parameters and visualizing the training process can provide valuable insights into how well our model is performing and where improvements can be made.\n",
    "\n",
    "- ***Plot Training and Validation Loss:*** This can help you understand if your model is overfitting or underfitting. If your training loss is much lower than your validation loss, your model might be overfitting. If both losses are high, your model might be underfitting.\n",
    "\n",
    "- ***Experiment with Different Architectures:*** Try adding more layers to your model or increasing the number of neurons in each layer. You could also experiment with different types of layers (e.g., convolutional layers, recurrent layers) and different activation functions.\n",
    "\n",
    "- ***Tune Hyperparameters***: This includes the learning rate, batch size, number of epochs, and regularization parameters. You could use techniques like grid search or random search to systematically explore different combinations of hyperparameters.\n",
    "\n",
    "- ***Use Early Stopping:*** This technique allows you to stop training once the models performance on a validation set stops improving, which can be useful to prevent overfitting.\n",
    "\n",
    "- ***Try Different Optimization Algorithms:*** In addition to Adam, there are many other optimization algorithms available in TensorFlow, such as SGD, RMSprop, and Adagrad. Different optimizers might lead to different results.\n",
    "\n",
    "- ***Regularization:*** If your model is overfitting, you might want to add some form of regularization, such as L1 or L2 regularization, or dropout.\n",
    "\n",
    "- ***Data Augmentation***: If you have a small dataset, you could artificially increase its size by creating modified versions of your existing data. For example, you could add small amounts of noise to your input data.\n",
    "\n",
    "- ***Learning Rate Scheduling:*** Instead of using a fixed learning rate, you could decrease it over time or in response to the models performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset space\n",
    "%reset -f\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# tensorflow libraries\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten, Concatenate, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADING ====================================\n",
    "\n",
    "# get original data\n",
    "data = pd.read_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data.csv\", index_col=0)\n",
    "\n",
    "# create a copy of the original matrix to store hidden ratings\n",
    "data_hidden = data.copy()\n",
    "\n",
    "# identifies rated books and randomly selects 2 books to hide ratings for each user\n",
    "np.random.seed(10)  # You can use any integer value as the seed\n",
    "for user_id in range(data_hidden.shape[0]):\n",
    "    rated_books = np.where(data_hidden.iloc[user_id, :] > 0)[0]\n",
    "    hidden_indices = np.random.choice(rated_books, min(2, len(rated_books)), replace=False)\n",
    "    data_hidden.iloc[user_id, hidden_indices] = 'Hidden'\n",
    "\n",
    "# get indices of hidden ratings\n",
    "test_data = data_hidden.copy()\n",
    "test_data = test_data.reset_index()\n",
    "test_data = test_data.melt(id_vars=test_data.columns[0], var_name='book', value_name='rating')\n",
    "test_data.columns = ['user', 'book', 'rating']\n",
    "indices_hidden = test_data[test_data['rating'] == 'Hidden'].index\n",
    "\n",
    "# Melt the DataFrame into a format where each row is a user-item interaction\n",
    "data_hidden = data_hidden.reset_index()\n",
    "data_hidden = data_hidden.melt(id_vars=data_hidden.columns[0], var_name='book', value_name='rating')\n",
    "\n",
    "# change rows with hidden ratings to 0\n",
    "data_hidden.iloc[indices_hidden, 2] = 0\n",
    "\n",
    "# rename columns\n",
    "data_hidden.columns = ['user', 'book', 'rating']\n",
    "\n",
    "# Filter out the rows where rating is 0\n",
    "data_hidden = data_hidden[data_hidden['rating'] != 0]\n",
    "\n",
    "# Convert user and book to categorical\n",
    "data_hidden['user'] = data_hidden['user'].astype('category')\n",
    "data_hidden['book'] = data_hidden['book'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST AND TRAIN DATA ====================================\n",
    "\n",
    "# Prepare the data - trining\n",
    "train_x = data_hidden[['user', 'book']].apply(lambda x: x.cat.codes)\n",
    "train_y = data_hidden['rating'].astype(np.float64)\n",
    "train_y = (train_y - 1) / 4\n",
    "\n",
    "# Prepare the data - testing\n",
    "copy = data.copy()\n",
    "copy = copy.reset_index()\n",
    "copy = copy.melt(id_vars=copy.columns[0], var_name='book', value_name='rating')\n",
    "copy.columns = ['user', 'book', 'rating']\n",
    "test_x = copy.iloc[indices_hidden, 0:2]\n",
    "test_x['user'] = test_x['user'].astype('category')\n",
    "test_x['book'] = test_x['book'].astype('category')\n",
    "test_x = test_x.apply(lambda x: x.cat.codes)\n",
    "test_y = copy.iloc[indices_hidden, 2].astype(np.float64)\n",
    "test_y = (test_y - 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a neural network model for collaborative filtering\n",
    "def train_model(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, train_x, train_y, seed=10, train_plot=True):\n",
    "    \"\"\"\n",
    "    Function to train a neural network model for collaborative filtering.\n",
    "    :param n_layers: Number of dense layers in the model\n",
    "    :param n_nodes: Number of nodes in each dense layer\n",
    "    :param optimizer: Optimizer to use for training\n",
    "    :param epochs: Number of epochs to train for\n",
    "    :param learning_rate: Learning rate for the optimizer\n",
    "    :param train_x: Training data\n",
    "    :param train_y: Training labels\n",
    "    :param seed: Random seed\n",
    "    :return: Trained model and training history\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create user and book embedding layers\n",
    "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    book_input = Input(shape=(1,), dtype='int32', name='book_input')\n",
    "\n",
    "    user_embedding = Embedding(input_dim=len(data_hidden['user'].cat.categories), output_dim=50, name='user_embedding')(user_input)\n",
    "    book_embedding = Embedding(input_dim=len(data_hidden['book'].cat.categories), output_dim=50, name='book_embedding')(book_input)\n",
    "\n",
    "    # Flatten the embedding vectors\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    book_vecs = Flatten()(book_embedding)\n",
    "\n",
    "    # Concatenate the embedding vectors\n",
    "    input_vecs = Concatenate()([user_vecs, book_vecs])\n",
    "\n",
    "    # Add dense layers\n",
    "    x = input_vecs\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            x = Dense(n_nodes, activation='relu')(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "        else:\n",
    "            n_nodes = n_nodes/2\n",
    "            x = Dense(n_nodes, activation='relu')(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(inputs=[user_input, book_input], outputs=y)\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([train_x['user'], train_x['book']], train_y, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    if train_plot:\n",
    "        # Plot training & validation loss values\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "        plt.title(f'Model loss for Architecture: {optimizer} optimizer, {n_layers} layers, {n_nodes} nodes, {epochs} epochs, {learning_rate} learning rate, {batch_size} batch size')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.savefig(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\ncf_training.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model, history = train_model(n_layers=2, n_nodes=256, optimizer='adam', epochs=100, learning_rate=0.001, batch_size=64, train_x=train_x, train_y=train_y, seed=10, train_plot=False)\n",
    "\n",
    "# Model 2 - 3 layers, 256 nodes, adam, 100 epochs, 0.001 learning rate, 64 batch size\n",
    "model2, history2 = train_model(n_layers=3, n_nodes=256, optimizer='adam', epochs=100, learning_rate=0.001, batch_size=64, train_x=train_x, train_y=train_y, seed=10, train_plot=False)\n",
    "\n",
    "# Model 3 - 2 layers, 256 nodes, sgd, 100 epochs, 0.001 learning rate, 64 batch size\n",
    "model3, history3 = train_model(n_layers=2, n_nodes=256, optimizer='sgd', epochs=100, learning_rate=0.001, batch_size=64, train_x=train_x, train_y=train_y, seed=10, train_plot=False)\n",
    "\n",
    "# Model 4 - 3 layers, 256 nodes, sgd, 100 epochs, 0.001 learning rate, 64 batch size\n",
    "model4, history4 = train_model(n_layers=3, n_nodes=256, optimizer='sgd', epochs=100, learning_rate=0.001, batch_size=64, train_x=train_x, train_y=train_y, seed=10, train_plot=False)\n",
    "\n",
    "# Model 5 - 2 layers, 256 nodes, rmsprop, 100 epochs, 0.001 learning rate, 64 batch size\n",
    "model5, history5 = train_model(n_layers=2, n_nodes=256, optimizer='rmsprop', epochs=100, learning_rate=0.001, batch_size=64, train_x=train_x, train_y=train_y, seed=10, train_plot=False)\n",
    "\n",
    "# Model 6 - 3 layers, 256 nodes, rmsprop, 100 epochs, 0.001 learning rate, 64 batch size\n",
    "model6, history6 = train_model(n_layers=3, n_nodes=256, optimizer='rmsprop', epochs=100, learning_rate=0.001, batch_size=64, train_x=train_x, train_y=train_y, seed=10, train_plot=False)\n",
    "\n",
    "# Model 7 - 2 layers, 256 nodes, adam, 100 epochs, 0.01 learning rate, 64 batch size\n",
    "model7, history7 = train_model(n_layers=2, n_nodes=256, optimizer='adam', epochs=100, learning_rate=0.01, batch_size=64, train_x=train_x, train_y=train_y, seed=10, train_plot=False)\n",
    "\n",
    "# Model 8 - 3 layers, 256 nodes, adam, 100 epochs, 0.01 learning rate, 64 batch size\n",
    "model8, history8 = train_model(n_layers=3, n_nodes=256, optimizer='adam', epochs=100, learning_rate=0.01, batch_size=64, train_x=train_x, train_y=train_y, seed=10, train_plot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise training and validation loss for all models\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Training Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Model 1', marker='o')\n",
    "plt.plot(history2.history['loss'], label='Model 2', marker='o')\n",
    "plt.plot(history3.history['loss'], label='Model 3', marker='o')\n",
    "plt.plot(history4.history['loss'], label='Model 4', marker='o')\n",
    "plt.plot(history5.history['loss'], label='Model 5', marker='o')\n",
    "plt.plot(history6.history['loss'], label='Model 6', marker='o')\n",
    "plt.plot(history7.history['loss'], label='Model 7', marker='o')\n",
    "plt.plot(history8.history['loss'], label='Model 8', marker='o')\n",
    "plt.title('Model Training Loss for different architectures')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Validation Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['val_loss'], label='Model 1', marker='o')\n",
    "plt.plot(history2.history['val_loss'], label='Model 2', marker='o')\n",
    "plt.plot(history3.history['val_loss'], label='Model 3', marker='o')\n",
    "plt.plot(history4.history['val_loss'], label='Model 4', marker='o')\n",
    "plt.plot(history5.history['val_loss'], label='Model 5', marker='o')\n",
    "plt.plot(history6.history['val_loss'], label='Model 6', marker='o')\n",
    "plt.plot(history7.history['val_loss'], label='Model 7', marker='o')\n",
    "plt.plot(history8.history['val_loss'], label='Model 8', marker='o')\n",
    "plt.title('Model Validation Loss for different architectures')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\ncf_training_all.png\")\n",
    "plt.show()\n",
    "\n",
    "# print models\n",
    "print(\"Model 1: 2 layers, 256 nodes, adam, 100 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 2: 3 layers, 256 nodes, adam, 100 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 3: 2 layers, 256 nodes, sgd, 100 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 4: 3 layers, 256 nodes, sgd, 100 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 5: 2 layers, 256 nodes, rmsprop, 100 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 6: 3 layers, 256 nodes, rmsprop, 100 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 7: 2 layers, 256 nodes, adam, 100 epochs, 0.01 learning rate, 64 batch size\")\n",
    "print(\"Model 8: 3 layers, 256 nodes, adam, 100 epochs, 0.01 learning rate, 64 batch size\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATION ====================================\n",
    "# Predict the ratings\n",
    "y_pred = model.predict([test_x['user'], test_x['book']])\n",
    "\n",
    "# Rescale the predictions back to the 1-5 range\n",
    "y_pred = y_pred * 4 + 1\n",
    "\n",
    "# set predictions and actual ratings to variables\n",
    "hidden_ratings_array = (np.array(test_y)*4 + 1)\n",
    "predicted_ratings_array = np.array(y_pred).flatten()\n",
    "\n",
    "# Rating predictions\n",
    "mae = mean_absolute_error(hidden_ratings_array, predicted_ratings_array)\n",
    "mse = mean_squared_error(hidden_ratings_array, predicted_ratings_array)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Binarise the hidden ratings and predicted ratings\n",
    "threshold = 3.5\n",
    "binary_prediction_ratings = (predicted_ratings_array >= threshold).astype(int) \n",
    "binary_hidden_ratings = (hidden_ratings_array >= threshold).astype(int)\n",
    "\n",
    "# Classification predictions\n",
    "accuracy = accuracy_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "precision = precision_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "recall = recall_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "f1 = f1_score(binary_hidden_ratings, binary_prediction_ratings)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
