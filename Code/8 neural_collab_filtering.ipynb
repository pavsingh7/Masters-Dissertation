{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Based Filtering\n",
    "\n",
    "- using neural network to learn the user-item interaction\n",
    "\n",
    "This piece is a TensorFlow implementation of Neural Collaborative Filtering (NCF) from the paper [He et al. (2017)](https://arxiv.org/pdf/1708.05031.pdf).\n",
    "\n",
    "## Summary\n",
    "\n",
    "NCF uses neural networks to model the interactions between users and items. NCF replaces the inner product (used in ordinary MF methods) with a neural architecture that can learn an arbitrary function from data. This allows NCF to express and generalize matrix factorization under its framework. Essentially, it uses a neural network to learn the user-item interaction function, and uses the learned function to predict the corresponding rating. A multi-layer perceptron is used to learn the user-item interaction function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Ratings Only\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. Read in Original Data\n",
    "2. Remove some ratings to create the test set\n",
    "3. With remaining ratings, create training set\n",
    "4. Preprocess the data (melt the data, create user and item indices, normalize the ratings)\n",
    "5. Create neural network model (NCF)\n",
    "6. Train the model\n",
    "7. Hyperparameter tuning\n",
    "8. Evaluate the model on the test set\n",
    "9. Gather all ratings prediction metrics (MAE, MSE, RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset space\n",
    "%reset -f\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# tensorflow libraries load\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten, Concatenate, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# amz_data = pd.read_csv(r'C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\Data\\set2_data_modelling.csv')\n",
    "amz_data = pd.read_csv('/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/set3_data_modelling.csv')\n",
    "display(amz_data.head())\n",
    "\n",
    "# print details\n",
    "print('Number of Rows: ', amz_data.shape[0])\n",
    "print('Number of Columns: ', amz_data.shape[1])\n",
    "print('Number of Unique Users: ', len(amz_data['reviewerID'].unique()))\n",
    "print('Number of Unique Products: ', len(amz_data['asin'].unique()))\n",
    "print('Min number of ratings per user: ', amz_data['reviewerID'].value_counts().min())\n",
    "print('Max number of ratings per user: ', amz_data['reviewerID'].value_counts().max())\n",
    "print('Min number of ratings per product: ', amz_data['asin'].value_counts().min())\n",
    "print('Max number of ratings per product: ', amz_data['asin'].value_counts().max())\n",
    "\n",
    "\n",
    "\n",
    "# Creating User Item Matrix =====================================================\n",
    "# create user-item matrix\n",
    "data = amz_data.pivot_table(index='reviewerID', columns='asin', values='overall')\n",
    "print(\"\\n\\nUser-Item Matrix\")\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREP ====================================\n",
    "\n",
    "# create a copy of the original matrix to store hidden ratings\n",
    "x_hidden = data.copy()\n",
    "indices_tracker = []\n",
    "\n",
    "# number of products to hide for each user\n",
    "N = 3\n",
    "\n",
    "# identifies rated items and randomly selects N products to hide ratings for each user\n",
    "np.random.seed(2207)  # You can use any integer value as the seed\n",
    "for user_id in range(x_hidden.shape[0]):\n",
    "    rated_products = np.where(x_hidden.iloc[user_id, :] > 0)[0]\n",
    "    hidden_indices = np.random.choice(rated_products, N, replace=False)\n",
    "    indices_tracker.append(hidden_indices)\n",
    "    x_hidden.iloc[user_id, hidden_indices] = 'Hidden'\n",
    "\n",
    "# get indices of hidden ratings\n",
    "test_data = x_hidden.copy()\n",
    "test_data = test_data.reset_index()\n",
    "test_data = test_data.melt(id_vars=test_data.columns[0], var_name='book', value_name='rating')\n",
    "test_data.columns = ['user', 'product', 'rating']\n",
    "indices_hidden = test_data[test_data['rating'] == 'Hidden'].index\n",
    "\n",
    "# Melt the DataFrame into a format where each row is a user-item interaction\n",
    "data_hidden = x_hidden.reset_index()\n",
    "data_hidden = data_hidden.melt(id_vars=data_hidden.columns[0], var_name='product', value_name='rating')\n",
    "\n",
    "# change rows with hidden ratings to NaN\n",
    "data_hidden.iloc[indices_hidden, 2] = np.nan\n",
    "\n",
    "# rename columns\n",
    "data_hidden.columns = ['user', 'product', 'rating']\n",
    "\n",
    "# Filter out the rows where rating is NaN\n",
    "data_hidden = data_hidden[data_hidden['rating'].notna()]\n",
    "\n",
    "# Convert user and item to categorical\n",
    "data_hidden['user'] = data_hidden['user'].astype('category')\n",
    "data_hidden['product'] = data_hidden['product'].astype('category')\n",
    "\n",
    "# see what the data looks like\n",
    "display(data_hidden.head(4))\n",
    "print(\"Data is in format: user, product, rating.\\nIt is ready to be partitioned into training and testing sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # validation data (take 2 more random ratings)\n",
    "# x_validation = data_hidden.copy()\n",
    "# indices_tracker_val = []\n",
    "\n",
    "# # number of products to hide for each user\n",
    "# N = 2\n",
    "\n",
    "# # identifies rated items and randomly selects N products to hide ratings for each user\n",
    "# np.random.seed(2207)  # You can use any integer value as the seed\n",
    "# for user_id in range(x_validation.shape[0]):\n",
    "#     rated_products = np.where(x_validation.iloc[user_id, :] > 0)[0]\n",
    "#     hidden_indices = np.random.choice(rated_products, N, replace=False)\n",
    "#     indices_tracker_val.append(hidden_indices)\n",
    "#     x_validation.iloc[user_id, hidden_indices] = 'Hidden'\n",
    "\n",
    "# # get indices of hidden ratings\n",
    "# val_data = x_validation.copy()\n",
    "# val_data = val_data.reset_index()\n",
    "# val_data = val_data.melt(id_vars=val_data.columns[0], var_name='book', value_name='rating')\n",
    "# val_data.columns = ['user', 'product', 'rating']\n",
    "# indices_hidden_val = val_data[val_data['rating'] == 'Hidden'].index\n",
    "\n",
    "# # Melt the DataFrame into a format where each row is a user-item interaction\n",
    "# data_hidden_val = x_validation.reset_index()\n",
    "# data_hidden_val = data_hidden_val.melt(id_vars=data_hidden_val.columns[0], var_name='product', value_name='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST AND TRAIN DATA ====================================\n",
    "\n",
    "# Prepare the data - trining\n",
    "train_x = data_hidden[['user', 'product']].apply(lambda x: x.cat.codes)\n",
    "train_y = data_hidden['rating'].astype(np.float64)\n",
    "train_y = (train_y - 1) / 4\n",
    "\n",
    "# Prepare the data - testing\n",
    "copy = data.copy()\n",
    "copy = copy.reset_index()\n",
    "copy = copy.melt(id_vars=copy.columns[0], var_name='product', value_name='rating')\n",
    "copy.columns = ['user', 'product', 'rating']\n",
    "test_x = copy.iloc[indices_hidden, 0:2]\n",
    "test_x['user'] = test_x['user'].astype('category')\n",
    "test_x['product'] = test_x['product'].astype('category')\n",
    "test_x = test_x.apply(lambda x: x.cat.codes)\n",
    "test_y = copy.iloc[indices_hidden, 2].astype(np.float64)\n",
    "test_y = (test_y - 1) / 4\n",
    "\n",
    "# show the data\n",
    "print(\"Training Data\")\n",
    "display(train_x.head(3))\n",
    "\n",
    "print(\"\\nTesting Data\")\n",
    "display(test_x.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating NCF Model\n",
    "\n",
    "Inputs:\n",
    "user_input and product_input: These are integer inputs representing user and product IDs.\n",
    "user_embedding and product_embedding: These layers create dense embeddings for users and products based on their IDs.\n",
    "user_vecs and product_vecs: These flatten the embeddings to create feature vectors.\n",
    "input_vecs: The concatenated feature vectors serve as the input to the neural network.\n",
    "Neural Network Architecture:\n",
    "You’ve designed a feedforward neural network with multiple layers.\n",
    "The first layer (i == 0) has n_nodes neurons, followed by dropout regularization.\n",
    "Subsequent layers reduce the number of neurons by halving n_nodes.\n",
    "The final output layer predicts the rating (regression task).\n",
    "Optimizers:\n",
    "You’ve implemented three optimizers: Adam, SGD, and RMSprop.\n",
    "The choice of optimizer affects how the model updates its weights during training.\n",
    "Loss Function:\n",
    "The mean squared error (MSE) loss is used for regression tasks.\n",
    "The model aims to minimize the difference between predicted and actual ratings.\n",
    "Training:\n",
    "The model is trained using user and product IDs as input features.\n",
    "The train_x dictionary contains user and product data.\n",
    "The train_y array holds the corresponding ratings.\n",
    "You’ve split the data into training and validation sets (10% validation split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a neural network model for collaborative filtering\n",
    "def train_model_1(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout, l2_reg, train_x, train_y, seed=2207, train_plot=True, callback=True):\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create user and product embedding layers\n",
    "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    product_input = Input(shape=(1,), dtype='int32', name='product_input')\n",
    "\n",
    "    user_embedding = Embedding(input_dim=len(data_hidden['user'].cat.categories), output_dim=50, name='user_embedding')(user_input)\n",
    "    product_embedding = Embedding(input_dim=len(data_hidden['product'].cat.categories), output_dim=50, name='product_embedding')(product_input)\n",
    "\n",
    "    # Flatten the embedding vectors\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    product_vecs = Flatten()(product_embedding)\n",
    "\n",
    "    # Concatenate the embedding vectors\n",
    "    input_vecs = Concatenate()([user_vecs, product_vecs])\n",
    "\n",
    "    # Add dense layers\n",
    "    x = input_vecs\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            x = Dense(n_nodes, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "        else:\n",
    "            n_nodes = n_nodes/2\n",
    "            x = Dense(n_nodes, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(inputs=[user_input, product_input], outputs=y)\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "    # Define early stopping\n",
    "    if callback:\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    if callback:\n",
    "        history = model.fit([train_x['user'], train_x['product']], train_y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[early_stopping])\n",
    "    else:\n",
    "        history = model.fit([train_x['user'], train_x['product']], train_y, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    if train_plot:\n",
    "        # Plot training & validation loss values\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "        # plt.title(f'Model loss for Architecture: {optimizer} optimizer, {n_layers} layers, {n_nodes} nodes, {epochs} epochs, {learning_rate} learning rate, {batch_size} batch size')\n",
    "        plt.ylabel('Loss', fontsize=40)\n",
    "        plt.xlabel('Epoch', fontsize = 40)\n",
    "        plt.xticks(fontsize=36)\n",
    "        plt.yticks(fontsize=36)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Final Writing/Figures/ncf_training_1.pdf\")\n",
    "        plt.show()\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training NCF Model\n",
    "\n",
    "- increasing batch size is good:128\n",
    "- increase nodes\n",
    "- change optimizer\n",
    "- change learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model with validation data\n",
    "# model, history = train_model_1(n_layers=2, n_nodes=64, optimizer='adam', epochs=100, learning_rate=0.001, batch_size=64, dropout=0.2, l2_reg=0.01, train_x=train_x, train_y=train_y, seed=2207, train_plot=True, callback=True, validation_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 \n",
    "model, history = train_model_1(n_layers=2, n_nodes=512, optimizer='adam', epochs=50, learning_rate=0.001, batch_size=128, dropout=0.5, l2_reg=0.01, train_x=train_x, train_y=train_y, seed=10, train_plot=False, callback=True)\n",
    "\n",
    "# # Model 2 \n",
    "model2, history2 = train_model_1(n_layers=3, n_nodes=1024, optimizer='sgd', epochs=1000, learning_rate=0.001, batch_size=128, dropout=0.5, l2_reg=0.01, train_x=train_x, train_y=train_y, seed=10, train_plot=False, callback=True)\n",
    "\n",
    "# # Model 3 \n",
    "model3, history3 = train_model_1(n_layers=3, n_nodes=1024, optimizer='rmsprop', epochs=200, learning_rate=0.001, batch_size=128, dropout=0.5, l2_reg=0.01, train_x=train_x, train_y=train_y, seed=10, train_plot=False, callback=True)\n",
    "\n",
    "# # Model 4\n",
    "model4, history4 = train_model_1(n_layers=8, n_nodes=1024, optimizer='adam', epochs=450, learning_rate=0.001, batch_size=128, dropout=0.5, l2_reg=0.01, train_x=train_x, train_y=train_y, seed=10, train_plot=False, callback=True)\n",
    "\n",
    "# # Model 5 \n",
    "model5, history5 = train_model_1(n_layers=12, n_nodes=2048, optimizer='sgd', epochs=1000, learning_rate=0.001, batch_size=128, dropout=0.5, l2_reg=0.01, train_x=train_x, train_y=train_y, seed=10, train_plot=False, callback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model had lowest validation loss?\n",
    "print(\"Model 1 Validation Loss: \", min(history.history['val_loss']))\n",
    "print(\"Model 2 Validation Loss: \", min(history2.history['val_loss']))\n",
    "print(\"Model 3 Validation Loss: \", min(history3.history['val_loss']))\n",
    "print(\"Model 4 Validation Loss: \", min(history4.history['val_loss']))\n",
    "print(\"Model 5 Validation Loss: \", min(history5.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training and validation loss for all models\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Training Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Model 1', marker='o', color = 'b')\n",
    "plt.plot(history2.history['loss'], label='Model 2', marker='o', color = 'g')\n",
    "plt.plot(history3.history['loss'], label='Model 3', marker='o', color = 'r')\n",
    "plt.plot(history4.history['loss'], label='Model 4', marker='o', color = 'y')\n",
    "plt.plot(history5.history['loss'], label='Model 5', marker='o', color = 'c')\n",
    "plt.title('Model Training Loss for different architectures', weight='bold', size=12)\n",
    "plt.ylabel('Loss', weight='bold', size=12)\n",
    "plt.xlabel('Epoch', weight='bold', size=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Validation Loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['val_loss'], label='Model 1', marker='o', color = 'b')\n",
    "plt.plot(history2.history['val_loss'], label='Model 2', marker='o', color = 'g')\n",
    "plt.plot(history3.history['val_loss'], label='Model 3', marker='o', color = 'r')\n",
    "plt.plot(history4.history['val_loss'], label='Model 4', marker='o', color = 'y')\n",
    "plt.plot(history5.history['val_loss'], label='Model 5', marker='o', color = 'c')\n",
    "plt.title('Model Validation Loss for different architectures', weight='bold', size=12)\n",
    "plt.ylabel('Loss', weight='bold', size=12)\n",
    "plt.xlabel('Epoch', weight='bold', size=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot validation and training loss on same plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['loss'], label='Model 1 Train', marker='o', color = 'b')\n",
    "plt.plot(history.history['val_loss'], label='Model 1 Validation', marker='o', color = 'b')\n",
    "plt.plot(history2.history['loss'], label='Model 2 Train', marker='o', color = 'g')\n",
    "plt.plot(history2.history['val_loss'], label='Model 2 Validation', marker='o', color = 'g')\n",
    "plt.plot(history3.history['loss'], label='Model 3 Train', marker='o', color = 'r')\n",
    "plt.plot(history3.history['val_loss'], label='Model 3 Validation', marker='o', color = 'r')\n",
    "plt.plot(history4.history['loss'], label='Model 4 Train', marker='o', color = 'y')\n",
    "plt.plot(history4.history['val_loss'], label='Model 4 Validation', marker='o', color = 'y')\n",
    "plt.plot(history5.history['loss'], label='Model 5 Train', marker='o', color = 'c')\n",
    "plt.plot(history5.history['val_loss'], label='Model 5 Validation', marker='o', color = 'c')\n",
    "plt.title('Model Training and Validation Loss', weight='bold', size=12)\n",
    "plt.ylabel('Loss', weight='bold', size=12)\n",
    "plt.xlabel('Epoch', weight='bold', size=12)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print models details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning / Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Grid Search Parameters\n",
    "n_layers = [1,2,3,6,8] \n",
    "n_nodes = [128,256,512,1024] \n",
    "optimizer = ['adam', 'sgd']\n",
    "epochs = [50,150,300] \n",
    "learning_rate = [0.001, 0.01,  0.0001] \n",
    "batch_size = [32,64,128] \n",
    "dropout = [0, 0.01, 0.05, 0.08]\n",
    "l2 = [0.01, 0.001, 0.0001]\n",
    "\n",
    "print(f\"Number of combinations: {len(n_layers) * len(n_nodes) * len(optimizer) * len(epochs) * len(learning_rate) * len(batch_size)* len(dropout)* len(l2)}\")\n",
    "\n",
    "def grid_search(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout, l2, train_x, train_y):\n",
    "    # Initialize best parameters and best model variables\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    best_score = None\n",
    "\n",
    "    # Generate all possible combinations of hyperparameters\n",
    "    param_combinations = itertools.product(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout, l2)\n",
    "\n",
    "    # Loop through all combinations\n",
    "    for combination in param_combinations:\n",
    "        # Unpack the combination\n",
    "        n_layer, n_node, opt, epoch, lr, bs, dropout, l2 = combination\n",
    "\n",
    "        # Train the model\n",
    "        model, history = train_model_1(n_layer, n_node, opt, epoch, lr, bs, dropout, l2, train_x, train_y, seed=10, train_plot=False, callback=True)\n",
    "\n",
    "        # Evaluate the model\n",
    "        min_loss = min(history.history['val_loss'])\n",
    "\n",
    "        # Check if this model is better than the previous best\n",
    "        if best_score is None or min_loss < best_score:\n",
    "            best_score = min_loss\n",
    "            best_params = combination\n",
    "            best_model = model\n",
    "\n",
    "    return best_params, best_model\n",
    "\n",
    "\n",
    "# run grid search\n",
    "best_params, best_model = grid_search(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout, l2, train_x, train_y)\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit best model \n",
    "best_model, history = train_model_1(n_layers=best_params[0], n_nodes=best_params[1], optimizer=best_params[2], epochs=best_params[3], learning_rate=best_params[4], batch_size=best_params[5], dropout=best_params[6], l2_reg=best_params[7], train_x=train_x, train_y=train_y, train_plot=True, callback=True, seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating NCF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATION ====================================\n",
    "# Predict the ratings\n",
    "# y_pred = best_model.predict([test_x['user'], test_x['product']])\n",
    "y_pred = model.predict([test_x['user'], test_x['product']])\n",
    "\n",
    "# Rescale the predictions back to the 1-5 range\n",
    "y_pred = y_pred * 4 + 1\n",
    "\n",
    "# set predictions and actual ratings to variables\n",
    "hidden_ratings_array = (np.array(test_y)*4 + 1)\n",
    "predicted_ratings_array = np.array(y_pred).flatten()\n",
    "\n",
    "# Rating predictions\n",
    "mae = mean_absolute_error(hidden_ratings_array, predicted_ratings_array)\n",
    "mse = mean_squared_error(hidden_ratings_array, predicted_ratings_array)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"\\nRating Metrics\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# save results to csv\n",
    "results = pd.DataFrame({'MAE': [mae.round(3)], 'MSE': [mse.round(3)], 'RMSE': [rmse.round(3)]})\n",
    "# results.to_csv(r\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/Results/NCF_results_1.csv\", index=False)\n",
    "results.index = ['NCF']\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Accuracy Insights\n",
    "\n",
    "1. Want to see if accuracy is better for users who have rated more items. (i.e., for users who have rated more items, is the accuracy of the model better?)\n",
    "\n",
    "2. Want to see if accuracy is better for items that have been rated more times. (i.e., for items that have been rated more times, is the accuracy of the model better?)\n",
    "\n",
    "3. Want to see if accuracy is better for some product categories. (i.e., for some product categories, is the accuracy of the model better?)\n",
    "- TOO FEW REVIEWS PER CATEGORY\n",
    "- RESULTS WOULD NOT OFFER MUCH INSIGHT\n",
    "\n",
    "4. Want to see if accuracy is better for reviews that are longer. (i.e., for reviews that are longer, is the accuracy of the model better?)\n",
    "\n",
    "\n",
    "Effectively, we want to see if accuracy varies according to some variables X or Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### QUESTION 1: PROCESS\n",
    "# 1. Group Users by the Number of Rated Items: Count the number of rated items for each user in your dataset.\n",
    "\n",
    "# Count the number of rated items for each user\n",
    "user_ratings = train_x.groupby('user')['product'].count().reset_index()\n",
    "user_ratings.columns = ['user', 'n_rated_items']\n",
    "\n",
    "# 2. Divide Users into Groups: Divide users into groups based on the number of rated items. You can define these groups based on quartiles, for example, or any other criteria that make sense for your dataset.\n",
    "\n",
    "# Divide users into groups based on the number of rated items\n",
    "user_ratings['group'] = pd.qcut(user_ratings['n_rated_items'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "display(user_ratings)\n",
    "\n",
    "# what are the number of users in each group?\n",
    "print(f'Number of Users in Low Group: {user_ratings[user_ratings[\"group\"] == \"Low\"].shape[0]}')\n",
    "print(f'Number of Users in Medium Group: {user_ratings[user_ratings[\"group\"] == \"Medium\"].shape[0]}')\n",
    "print(f'Number of Users in High Group: {user_ratings[user_ratings[\"group\"] == \"High\"].shape[0]}')\n",
    "print(f'Number of Users in Very High Group: {user_ratings[user_ratings[\"group\"] == \"Very High\"].shape[0]}')\n",
    "\n",
    "# 3. Evaluate the Model for Each Group: Evaluate the model for each group of users. You can use the same metrics you used in the previous question.\n",
    "low_group = user_ratings[user_ratings['group'] == 'Low']\n",
    "medium_group = user_ratings[user_ratings['group'] == 'Medium']\n",
    "high_group = user_ratings[user_ratings['group'] == 'High']\n",
    "very_high_group = user_ratings[user_ratings['group'] == 'Very High']\n",
    "\n",
    "# get test set items for these groups\n",
    "low_group_test = test_x[test_x['user'].isin(low_group['user'])]\n",
    "medium_group_test = test_x[test_x['user'].isin(medium_group['user'])]\n",
    "high_group_test = test_x[test_x['user'].isin(high_group['user'])]\n",
    "very_high_group_test = test_x[test_x['user'].isin(very_high_group['user'])]\n",
    "\n",
    "# get test set ratings for these groups\n",
    "low_group_ratings = test_y[test_x['user'].isin(low_group['user'])]\n",
    "medium_group_ratings = test_y[test_x['user'].isin(medium_group['user'])]\n",
    "high_group_ratings = test_y[test_x['user'].isin(high_group['user'])]\n",
    "very_high_group_ratings = test_y[test_x['user'].isin(very_high_group['user'])]\n",
    "\n",
    "# get predictions for these groups\n",
    "low_group_pred = y_pred[test_x['user'].isin(low_group['user'])]\n",
    "medium_group_pred = y_pred[test_x['user'].isin(medium_group['user'])]\n",
    "high_group_pred = y_pred[test_x['user'].isin(high_group['user'])]\n",
    "very_high_group_pred = y_pred[test_x['user'].isin(very_high_group['user'])]\n",
    "\n",
    "# set predictions and actual ratings to variables\n",
    "low_group_ratings_array = (np.array(low_group_ratings)*4 + 1)\n",
    "low_group_pred_array = np.array(low_group_pred).flatten()\n",
    "\n",
    "medium_group_ratings_array = (np.array(medium_group_ratings)*4 + 1)\n",
    "medium_group_pred_array = np.array(medium_group_pred).flatten()\n",
    "\n",
    "high_group_ratings_array = (np.array(high_group_ratings)*4 + 1)\n",
    "high_group_pred_array = np.array(high_group_pred).flatten()\n",
    "\n",
    "very_high_group_ratings_array = (np.array(very_high_group_ratings)*4 + 1)\n",
    "very_high_group_pred_array = np.array(very_high_group_pred).flatten()\n",
    "\n",
    "# Rating predictions\n",
    "low_group_mae = mean_absolute_error(low_group_ratings_array, low_group_pred_array)\n",
    "low_group_mse = mean_squared_error(low_group_ratings_array, low_group_pred_array)\n",
    "low_group_rmse = np.sqrt(low_group_mse)\n",
    "\n",
    "medium_group_mae = mean_absolute_error(medium_group_ratings_array, medium_group_pred_array)\n",
    "medium_group_mse = mean_squared_error(medium_group_ratings_array, medium_group_pred_array)\n",
    "medium_group_rmse = np.sqrt(medium_group_mse)\n",
    "\n",
    "high_group_mae = mean_absolute_error(high_group_ratings_array, high_group_pred_array)\n",
    "high_group_mse = mean_squared_error(high_group_ratings_array, high_group_pred_array)    \n",
    "high_group_rmse = np.sqrt(high_group_mse)\n",
    "\n",
    "very_high_group_mae = mean_absolute_error(very_high_group_ratings_array, very_high_group_pred_array)\n",
    "very_high_group_mse = mean_squared_error(very_high_group_ratings_array, very_high_group_pred_array)\n",
    "very_high_group_rmse = np.sqrt(very_high_group_mse)\n",
    "\n",
    "# display results\n",
    "print(\"Checking if the number of reviews impact the model performance.\")\n",
    "results = pd.DataFrame({'MAE': [low_group_mae.round(3), medium_group_mae.round(3), high_group_mae.round(3), very_high_group_mae.round(3)], 'MSE': [low_group_mse.round(3), medium_group_mse.round(3), high_group_mse.round(3), very_high_group_mse.round(3)], 'RMSE': [low_group_rmse.round(3), medium_group_rmse.round(3), high_group_rmse.round(3), very_high_group_rmse.round(3)]})\n",
    "results.index = ['Low', 'Medium', 'High', 'Very High']\n",
    "print(f'Number of Users in Low Group: {low_group.shape[0]}')\n",
    "print(f'Number of Users in Medium Group: {medium_group.shape[0]}')\n",
    "print(f'Number of Users in High Group: {high_group.shape[0]}')\n",
    "print(f'Number of Users in Very High Group: {very_high_group.shape[0]}')\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance of model for each user\n",
    "for user in range(user_ratings.shape[0]):\n",
    "    user = user_ratings['user'][user]\n",
    "    # get test set items for user\n",
    "    user_test = test_x[test_x['user'] == user]\n",
    "    # get test set ratings for user\n",
    "    users_ratings = test_y[test_x['user'] == user]\n",
    "    # get predictions for user\n",
    "    user_pred = y_pred[test_x['user'] == user]\n",
    "    # set predictions and actual ratings to variables\n",
    "    user_ratings_array = (np.array(users_ratings)*4 + 1)\n",
    "    user_pred_array = np.array(user_pred).flatten()\n",
    "    # Rating predictions\n",
    "    user_mae = mean_absolute_error(user_ratings_array, user_pred_array)\n",
    "    user_mse = mean_squared_error(user_ratings_array, user_pred_array)\n",
    "    user_rmse = np.sqrt(user_mse)\n",
    "    # assing results to user_ratings\n",
    "    user_ratings.loc[user, 'MAE'] = user_mae\n",
    "    user_ratings.loc[user, 'MSE'] = user_mse\n",
    "    user_ratings.loc[user, 'RMSE'] = user_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize the Results: Plot the accuracy metrics (RMSE, MSE, MAE) against the number of rated items for each group. This will help you visualize any patterns or trends in the accuracy of your model based on the number of rated items.\n",
    "## plot number of rated items vs MAE, MSE, RMSE scatter plot\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(user_ratings['n_rated_items'], user_ratings['MAE'])\n",
    "plt.title('Number of Rated Items vs MAE')\n",
    "plt.xlabel('Number of Rated Items')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(user_ratings['n_rated_items'], user_ratings['MSE'])\n",
    "plt.title('Number of Rated Items vs MSE')\n",
    "plt.xlabel('Number of Rated Items')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(user_ratings['n_rated_items'], user_ratings['RMSE'])\n",
    "plt.title('Number of Rated Items vs RMSE')\n",
    "plt.xlabel('Number of Rated Items')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see summary statistics for each group\n",
    "display(user_ratings.groupby('group').agg({'MAE': ['mean', 'std'], 'MSE': ['mean', 'std'], 'RMSE': ['mean', 'std']}))\n",
    "\n",
    "# apply anova test\n",
    "import scipy.stats as stats\n",
    "f_val, p_val = stats.f_oneway(user_ratings[user_ratings['group'] == 'Low']['RMSE'], user_ratings[user_ratings['group'] == 'Medium']['RMSE'], user_ratings[user_ratings['group'] == 'High']['RMSE'], user_ratings[user_ratings['group'] == 'Very High']['RMSE'])\n",
    "print(f'F-Value: {f_val}')\n",
    "print(f'P-Value: {p_val}')\n",
    "\n",
    "# apply post-hoc test\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "mc = MultiComparison(user_ratings['RMSE'], user_ratings['group'])\n",
    "result = mc.tukeyhsd()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 2: Want to see if accuracy is better for items that have been rated more times. (i.e., for items that have been rated more times, is the accuracy of the model better?)\n",
    "\n",
    "# Count the number of ratings for each item\n",
    "item_ratings = train_x.groupby('product')['user'].count().reset_index()\n",
    "item_ratings.columns = ['product', 'n_ratings']\n",
    "\n",
    "# Divide items into groups based on the number of ratings\n",
    "item_ratings['group'] = pd.qcut(item_ratings['n_ratings'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Evaluate the model for each group of items\n",
    "low_group = item_ratings[item_ratings['group'] == 'Low']\n",
    "medium_group = item_ratings[item_ratings['group'] == 'Medium']\n",
    "high_group = item_ratings[item_ratings['group'] == 'High']\n",
    "very_high_group = item_ratings[item_ratings['group'] == 'Very High']\n",
    "\n",
    "# get test set items for these groups\n",
    "low_group_test = test_x[test_x['product'].isin(low_group['product'])]\n",
    "medium_group_test = test_x[test_x['product'].isin(medium_group['product'])]\n",
    "high_group_test = test_x[test_x['product'].isin(high_group['product'])]\n",
    "very_high_group_test = test_x[test_x['product'].isin(very_high_group['product'])]\n",
    "\n",
    "# get test set ratings for these groups\n",
    "low_group_ratings = test_y[test_x['product'].isin(low_group['product'])]\n",
    "medium_group_ratings = test_y[test_x['product'].isin(medium_group['product'])]\n",
    "high_group_ratings = test_y[test_x['product'].isin(high_group['product'])]\n",
    "very_high_group_ratings = test_y[test_x['product'].isin(very_high_group['product'])]\n",
    "\n",
    "# get predictions for these groups\n",
    "low_group_pred = y_pred[test_x['product'].isin(low_group['product'])]\n",
    "medium_group_pred = y_pred[test_x['product'].isin(medium_group['product'])]\n",
    "high_group_pred = y_pred[test_x['product'].isin(high_group['product'])]\n",
    "very_high_group_pred = y_pred[test_x['product'].isin(very_high_group['product'])]\n",
    "\n",
    "# set predictions and actual ratings to variables\n",
    "low_group_ratings_array = (np.array(low_group_ratings)*4 + 1)\n",
    "low_group_pred_array = np.array(low_group_pred).flatten()\n",
    "\n",
    "medium_group_ratings_array = (np.array(medium_group_ratings)*4 + 1)\n",
    "medium_group_pred_array = np.array(medium_group_pred).flatten()\n",
    "\n",
    "high_group_ratings_array = (np.array(high_group_ratings)*4 + 1)\n",
    "high_group_pred_array = np.array(high_group_pred).flatten()\n",
    "\n",
    "very_high_group_ratings_array = (np.array(very_high_group_ratings)*4 + 1)\n",
    "very_high_group_pred_array = np.array(very_high_group_pred).flatten()\n",
    "\n",
    "# Rating predictions\n",
    "low_group_mae = mean_absolute_error(low_group_ratings_array, low_group_pred_array)\n",
    "low_group_mse = mean_squared_error(low_group_ratings_array, low_group_pred_array)\n",
    "low_group_rmse = np.sqrt(low_group_mse)\n",
    "\n",
    "medium_group_mae = mean_absolute_error(medium_group_ratings_array, medium_group_pred_array)\n",
    "medium_group_mse = mean_squared_error(medium_group_ratings_array, medium_group_pred_array)\n",
    "medium_group_rmse = np.sqrt(medium_group_mse)\n",
    "\n",
    "high_group_mae = mean_absolute_error(high_group_ratings_array, high_group_pred_array)\n",
    "high_group_mse = mean_squared_error(high_group_ratings_array, high_group_pred_array)\n",
    "high_group_rmse = np.sqrt(high_group_mse)\n",
    "\n",
    "very_high_group_mae = mean_absolute_error(very_high_group_ratings_array, very_high_group_pred_array)\n",
    "very_high_group_mse = mean_squared_error(very_high_group_ratings_array, very_high_group_pred_array)\n",
    "very_high_group_rmse = np.sqrt(very_high_group_mse)\n",
    "\n",
    "# display results\n",
    "print(\"Checking if the number of reviews of an impact the model performance for items.\")\n",
    "results = pd.DataFrame({'MAE': [low_group_mae.round(3), medium_group_mae.round(3), high_group_mae.round(3), very_high_group_mae.round(3)], 'MSE': [low_group_mse.round(3), medium_group_mse.round(3), high_group_mse.round(3), very_high_group_mse.round(3)], 'RMSE': [low_group_rmse.round(3), medium_group_rmse.round(3), high_group_rmse.round(3), very_high_group_rmse.round(3)]})\n",
    "results.index = ['Low', 'Medium', 'High', 'Very High']\n",
    "print(f'Number of Items in Low Group: {low_group.shape[0]}')\n",
    "print(f'Number of Items in Medium Group: {medium_group.shape[0]}')\n",
    "print(f'Number of Items in High Group: {high_group.shape[0]}')\n",
    "print(f'Number of Items in Very High Group: {very_high_group.shape[0]}')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance of model for each item\n",
    "for item in item_ratings['product']:\n",
    "    # Filter test set data for the current item\n",
    "    item_test = test_x[test_x['product'] == item]\n",
    "    if not item_test.empty:  # Check if there are samples available\n",
    "        # Get test set ratings for the current item\n",
    "        items_ratings = test_y[test_x['product'] == item]\n",
    "        # Get predictions for the current item\n",
    "        item_pred = y_pred[test_x['product'] == item]\n",
    "        # Set predictions and actual ratings to variables\n",
    "        item_ratings_array = (np.array(items_ratings) * 4 + 1)\n",
    "        item_pred_array = np.array(item_pred).flatten()\n",
    "        # Rating predictions\n",
    "        item_mae = mean_absolute_error(item_ratings_array, item_pred_array)\n",
    "        item_mse = mean_squared_error(item_ratings_array, item_pred_array)\n",
    "        item_rmse = np.sqrt(item_mse)\n",
    "        # Assign results to item_ratings\n",
    "        item_ratings.loc[item, 'MAE'] = item_mae\n",
    "        item_ratings.loc[item, 'MSE'] = item_mse\n",
    "        item_ratings.loc[item, 'RMSE'] = item_rmse\n",
    "    else:\n",
    "        # No samples available for this item\n",
    "        print(f\"No test set data available for item {item}. Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many nas in item_ratings\n",
    "item_ratings.isna().sum()\n",
    "\n",
    "#the test set items were randomly selected from the users' rated items list, there's a possibility that certain items may not have been included in the test set due to the random sampling process. As a result, when we attempt to evaluate the model's performance for each item using the test set, some items may not have any corresponding test set data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize the Results: Plot the accuracy metrics (RMSE, MSE, MAE) against the number of reviews for each group. This will help you visualize any patterns or trends in the accuracy of your model based on the number of rated items.¸\n",
    "## plot number of rated items vs MAE, MSE, RMSE scatter plot\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(item_ratings['n_ratings'], item_ratings['MAE'])\n",
    "plt.title('Number of Ratings vs MAE')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(item_ratings['n_ratings'], item_ratings['MSE'])\n",
    "plt.title('Number of Ratings vs MSE')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(item_ratings['n_ratings'], item_ratings['RMSE'])\n",
    "plt.title('Number of Ratings vs RMSE')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see summary statistics for each group\n",
    "display(item_ratings.groupby('group').agg({'MAE': ['mean', 'std'], 'MSE': ['mean', 'std'], 'RMSE': ['mean', 'std']}))\n",
    "\n",
    "# apply anova test\n",
    "item_ratings.dropna(inplace=True)\n",
    "f_val, p_val = stats.f_oneway(item_ratings[item_ratings['group'] == 'Low']['RMSE'], item_ratings[item_ratings['group'] == 'Medium']['RMSE'], item_ratings[item_ratings['group'] == 'High']['RMSE'], item_ratings[item_ratings['group'] == 'Very High']['RMSE'])\n",
    "print(f'F-Value: {f_val}')\n",
    "print(f'P-Value: {p_val}')\n",
    "\n",
    "# apply post-hoc test\n",
    "item_ratings['RMSE'] = pd.to_numeric(item_ratings['RMSE'], errors='coerce')  # coerce errors to NaN if conversion fails\n",
    "mc = MultiComparison(item_ratings['RMSE'], item_ratings['group'])\n",
    "result = mc.tukeyhsd()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-N Recommendations\n",
    "\n",
    "#### ***Process***\n",
    "\n",
    "***TLDR***: adjust  test setit to only contain items that a user liked (above a threshold). Then use my predictions in completed matrix, to get a list of top-N list of items to recommend to the user. If the user has already rated the item, then I will not recommend it. See how many of the recommended items are in the test set.\n",
    "\n",
    "- By adjusting your test set to only include items that meet or exceed a certain threshold (for example, ratings of 4 or above), we can evaluate our model's performance specifically on predicting items that the user likes or interacts with positively\n",
    "-  if an item receives a low rating from a user, it is unlikely to be recommended by your model or to be of interest to the user in the future. Therefore, you can focus your evaluation on how well your model predicts high-quality recommendations, which are more likely to lead to user satisfaction and engagement\n",
    "\n",
    "Here's a step-by-step breakdown:\n",
    "\n",
    "1. Test Set Adjustment: Modify your test set to only include items that the user liked, typically by setting a threshold for ratings (e.g., only items rated 4 or 5).\n",
    "2. Predictions: Generate predictions for each user-item pair using your model. For pairs where a user has not rated an item yet, your model should predict a rating.\n",
    "3. Top Recommendations: Rank the predicted ratings for each user's unrated items and select the top recommendations (e.g., top 100) based on these predicted ratings.\n",
    "4. Avoiding Already Rated Items: Check if the recommended items are already rated by the user (in your modified test set). If an item is already rated, you should not recommend it.\n",
    "5. Evaluation: Assess the performance of your recommender system using metrics like Precision and Recall, which evaluate how well your recommendations align with the user's preferences (as captured by the test set).\n",
    "\n",
    "\n",
    "#### ***Metrics***\n",
    "\n",
    "**Precision@K:**\n",
    "- Precision@K measures the proportion of relevant items among the top-K recommended items.\n",
    "- It answers the question: “Out of the top-K recommendations, how many are actually relevant?”\n",
    "\n",
    "\n",
    "**Recall@K:**\n",
    "- It answers the question: “Out of all relevant items, how many were included in the top-K recommendations?”\n",
    "- Recall@K (also known as Hit Rate@K) measures how well you capture relevant items among all the relevant items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a dataframe with interactions and ratings\n",
    "data_mat = data.copy()\n",
    "data_mat = data_mat.reset_index()\n",
    "data_mat = data_mat.melt(id_vars=data_mat.columns[0], var_name='product', value_name='rating')\n",
    "data_mat.columns = ['user', 'product', 'rating']\n",
    "data_mat['user'] = data_mat['user'].astype('category')\n",
    "data_mat['product'] = data_mat['product'].astype('category')\n",
    "data_mat['user'] = data_mat['user'].cat.codes\n",
    "data_mat['product'] = data_mat['product'].cat.codes\n",
    "display(data_mat.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill NaN ratings with predictions in the user-item dataframe\n",
    "def fill_nan_ratings_with_predictions(model, data):\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    completed = data.copy()\n",
    "\n",
    "    # Find rows with NaN ratings\n",
    "    nan_rows = completed[completed['rating'].isna()]\n",
    "\n",
    "    # Predict the ratings for these rows\n",
    "    predictions = model.predict([nan_rows['user'], nan_rows['product']])\n",
    "    predictions = predictions * 4 + 1\n",
    "\n",
    "    # Fill in the predictions\n",
    "    completed.loc[nan_rows.index, 'rating'] = predictions.flatten()\n",
    "\n",
    "    return completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill NaN ratings with predictions\n",
    "completed = fill_nan_ratings_with_predictions(model=best_model, data=data_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see original data with user item interactions\n",
    "print(\"User Item Interactions with Ratings\")\n",
    "display(data_mat.head(3))\n",
    "\n",
    "# see data with predictions\n",
    "print(\"\\nUser Item Interactions with Predicted Ratings\")\n",
    "display(completed.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details on completed dataframe\n",
    "print('Number of Rows: ', completed.shape[0])\n",
    "print('Number of Columns: ', completed.shape[1])\n",
    "print('Number of Unique Users: ', len(completed['user'].unique()))\n",
    "print('Number of Unique Products: ', len(completed['product'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Data: X\")\n",
    "display(test_x.head(3))\n",
    "print('Shape of Test Data: ', test_x.shape)\n",
    "\n",
    "# Define the threshold for positive interaction\n",
    "test_y_top_n = test_y.copy()\n",
    "test_y_top_n = pd.DataFrame(test_y_top_n)\n",
    "test_y_top_n = test_y_top_n* 4 + 1\n",
    "\n",
    "# Now, test_y will have a 'label' column with 0 for negative interactions and 1 for positive interactions\n",
    "print(\"\\nTest Data: Y\")\n",
    "display(test_y_top_n.head(3))\n",
    "print('Shape of Test Data: ', test_y_top_n.shape)\n",
    "\n",
    "# predicted data\n",
    "print(\"\\nPredicted Data\")\n",
    "predicted_rats = pd.Series(predicted_ratings_array)\n",
    "predicted_rats.index = test_y.index\n",
    "display(predicted_rats.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute for One User\n",
    "\n",
    "1. Test Set Adjustment: Modify your test set to only include items that the user liked, typically by setting a threshold for ratings (e.g., only items rated 4 or 5).\n",
    "2. Predictions: Generate predictions for each user-item pair using your model. For pairs where a user has not rated an item yet, your model should predict a rating.\n",
    "3. Top Recommendations: Rank the predicted ratings for each user's unrated items and select the top recommendations (e.g., top 100) based on these predicted ratings.\n",
    "4. Avoiding Already Rated Items: Check if the recommended items are already rated by the user (in your modified test set). If an item is already rated, you should not recommend it.\n",
    "5. Evaluation: Assess the performance of your recommender system using metrics like Precision and Recall, which evaluate how well your recommendations align with the user's preferences (as captured by the test set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set N - number of recommendations\n",
    "N = 1000\n",
    "\n",
    "# Get interactions for User 1 (including ratings)\n",
    "user_1 = completed[completed['user'] == 0]\n",
    "print(\"Number of Interactions for User 1: \", user_1.shape[0])\n",
    "\n",
    "# Identify liked items for User 1 (above a threshold, e.g., rating > 3)\n",
    "liked_items = user_1[user_1['rating'] > 3.5]\n",
    "print(\"Number of Liked Items for User 1: \", liked_items.shape[0])\n",
    "\n",
    "# get test set for user 1, including actual ratings and predicted ratings\n",
    "user_1_test = test_x[test_x['user'] == 0]\n",
    "user_1_test['actual_rating'] = test_y_top_n\n",
    "user_1_test['predicted_rating'] = predicted_rats[user_1_test.index]\n",
    "print(\"Number of Test Interactions for User 1: \", user_1_test.shape[0])\n",
    "\n",
    "\n",
    "# threshold for positive interaction\n",
    "threshold = 3.5\n",
    "print(\"Number of Test Interactions that the User Liked: \", user_1_test[user_1_test['actual_rating'] > threshold].shape[0])\n",
    "\n",
    "# adjust test set to include a label column using the threshold\n",
    "user_1_test['label'] = user_1_test['actual_rating'].apply(lambda x: 1 if x > threshold else 0)\n",
    "user_1_test\n",
    "\n",
    "# get predictions for user \n",
    "completed_user_1 = completed[completed['user'] == 0]\n",
    "\n",
    "\n",
    "# add label for used interactions (add 1 to all interactions that exist in train_x)\n",
    "train_x_user_1 = train_x[train_x['user'] == 0]\n",
    "\n",
    "# for each user interaction, check if it exists in train_x\n",
    "completed_user_1['used_ind'] = 0\n",
    "for i in range(completed_user_1.shape[0]):\n",
    "    if completed_user_1.iloc[i, 1] in list(train_x_user_1['product']):\n",
    "        completed_user_1.iloc[i, 3] = 1\n",
    "\n",
    "\n",
    "# count how many interactions are in train_x\n",
    "print(\"Number of Interactions in Train Set for User 1: \", train_x_user_1.shape[0])\n",
    "\n",
    "# count how many 1 in completed_user_1\n",
    "print(\"Number of Interactions in Completed User 1: \", completed_user_1[completed_user_1['used_ind'] == 1].shape[0])\n",
    "\n",
    "# add label liked for completed_user_1\n",
    "completed_user_1['liked'] = completed_user_1['rating'].apply(lambda x: 1 if x > threshold else 0)\n",
    "\n",
    "\n",
    "# get top N recommendations for user 1 - exclude items where used_ind = 1\n",
    "user_1_top_n = completed_user_1[completed_user_1['used_ind'] == 0]\n",
    "user_1_top_n = user_1_top_n.sort_values(by='rating', ascending=False)\n",
    "user_1_top_n = user_1_top_n.head(N)\n",
    "\n",
    "\n",
    "# add a label column to user_1_top_n: test_ind\n",
    "user_1_top_n['test_ind'] = 0\n",
    "for i in range(user_1_top_n.shape[0]):\n",
    "    if user_1_top_n.iloc[i, 1] in list(user_1_test[user_1_test['label'] == 1]['product']):\n",
    "        user_1_top_n.iloc[i, 5] = 1\n",
    "\n",
    "\n",
    "# count how many 1 in user_1_top_n\n",
    "print(\"Number of Items in Top N for User 1 that Were Used and Liked: \", user_1_top_n[user_1_top_n['test_ind'] == 1].shape[0])\n",
    "\n",
    "# see top N recommendations for user 1\n",
    "print(\"\\n\\nTop N Recommendations for User 1\")\n",
    "display(user_1_top_n)\n",
    "\n",
    "\n",
    "# Calculate precision@K (top N recommendations)\n",
    "precision_at_N = user_1_top_n['test_ind'].sum() / N\n",
    "\n",
    "# Calculate recall@K\n",
    "recall_at_N = user_1_top_n['test_ind'].sum() / liked_items.shape[0]\n",
    "\n",
    "# calculate F1 score\n",
    "f1_at_N = 2 * (precision_at_N * recall_at_N) / (precision_at_N + recall_at_N)\n",
    "\n",
    "print(f\"Precision@{N}: {precision_at_N:.4f}\")\n",
    "print(f\"Recall@{N}: {recall_at_N:.4f}\")\n",
    "print(f\"F1@{N}: {f1_at_N:.4f}\")\n",
    "\n",
    "# save results to csv\n",
    "results = pd.DataFrame({'Precision@N': [precision_at_N], 'Recall@N': [recall_at_N], 'F1@N': [f1_at_N]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topN_user(user, threshold, N, data):\n",
    "    # Get interactions for the specified user (including ratings)\n",
    "    user_interactions = data[data['user'] == user]\n",
    "    print(\"\\nNumber of Interactions for User {}: {}\".format(user, user_interactions.shape[0]))\n",
    "\n",
    "    # Identify liked items for the specified user (above the specified threshold)\n",
    "    liked_items = user_interactions[user_interactions['rating'] >= threshold]\n",
    "    print(\"Number of Liked Items for User {}: {}\".format(user, liked_items.shape[0]))\n",
    "\n",
    "    # Get the test set for the specified user, including actual ratings and predicted ratings\n",
    "    user_test_set = test_x[test_x['user'] == user]\n",
    "    user_test_set['actual_rating'] = test_y_top_n\n",
    "    user_test_set['predicted_rating'] = predicted_rats[user_test_set.index]\n",
    "    print(\"Number of Test Interactions for User {}: {}\".format(user, user_test_set.shape[0]))\n",
    "\n",
    "    # Adjust the test set to include a label column using the specified threshold\n",
    "    user_test_set['label'] = user_test_set['actual_rating'].apply(lambda x: 1 if x > threshold else 0)\n",
    "\n",
    "    # Count the number of interactions in the train set for the specified user\n",
    "    train_x_user = train_x[train_x['user'] == user]\n",
    "    print(\"Number of Interactions in Train Set for User {}: {}\\n\\n\".format(user, train_x_user.shape[0]))\n",
    "\n",
    "    # Add a label for used interactions (add 1 to all interactions that exist in train_x)\n",
    "    user_interactions['used_ind'] = 0\n",
    "    for i in range(user_interactions.shape[0]):\n",
    "        if user_interactions.iloc[i, 1] in list(train_x_user['product']):\n",
    "            user_interactions.iloc[i, 3] = 1\n",
    "\n",
    "    # Add a label for liked items\n",
    "    user_interactions['liked'] = user_interactions['rating'].apply(lambda x: 1 if x > threshold else 0)\n",
    "\n",
    "    # Get the top N recommendations for the specified user (excluding items where used_ind = 1)\n",
    "    user_top_n = user_interactions[user_interactions['used_ind'] == 0]\n",
    "    user_top_n = user_top_n.sort_values(by='rating', ascending=False)\n",
    "    user_top_n = user_top_n.head(N)\n",
    "\n",
    "    # Add a label column to user_top_n: test_ind\n",
    "    user_top_n['test_ind'] = 0\n",
    "    for i in range(user_top_n.shape[0]):\n",
    "        if user_top_n.iloc[i, 1] in list(user_test_set[user_test_set['label'] == 1]['product']):\n",
    "            user_top_n.iloc[i, 5] = 1\n",
    "\n",
    "    # Calculate Precision@N, Recall@N, and F1@N\n",
    "    precision_at_N = user_top_n['test_ind'].sum() / N\n",
    "    recall_at_N = user_top_n['test_ind'].sum() / liked_items.shape[0]\n",
    "    if precision_at_N + recall_at_N == 0:\n",
    "        f1_at_N = 0\n",
    "    else: f1_at_N = 2 * (precision_at_N * recall_at_N) / (precision_at_N + recall_at_N)\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Results for User {user} with Threshold {threshold} and Top {N} Recommendations!\")\n",
    "    print(f\"Precision@{N}: {precision_at_N:.4f}\")\n",
    "    print(f\"Recall@{N}: {recall_at_N:.4f}\")\n",
    "    print(f\"F1@{N}: {f1_at_N:.4f}\")\n",
    "\n",
    "    # Save the results to a dataframe and return it\n",
    "    results = pd.DataFrame({'Precision@N': [precision_at_N], 'Recall@N': [recall_at_N], 'F1@N': [f1_at_N]})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topN_user(user, threshold, N, data):\n",
    "    # Get interactions for the specified user (including ratings)\n",
    "    user_interactions = data[data['user'] == user]\n",
    "    print(\"\\nNumber of Interactions for User {}: {}\".format(user, user_interactions.shape[0]))\n",
    "\n",
    "    # Identify liked items for the specified user (above the specified threshold)\n",
    "    liked_items = user_interactions[user_interactions['rating'] >= threshold]\n",
    "    print(\"Number of Liked Items for User {}: {}\".format(user, liked_items.shape[0]))\n",
    "\n",
    "    # Get the test set for the specified user, including actual ratings and predicted ratings\n",
    "    user_test_set = test_x[test_x['user'] == user]\n",
    "    user_test_set['actual_rating'] = test_y_top_n\n",
    "    user_test_set['predicted_rating'] = predicted_rats[user_test_set.index]\n",
    "    print(\"Number of Test Interactions for User {}: {}\".format(user, user_test_set.shape[0]))\n",
    "\n",
    "    # Adjust the test set to include a label column using the specified threshold\n",
    "    user_test_set['label'] = user_test_set['actual_rating'].apply(lambda x: 1 if x > threshold else 0)\n",
    "\n",
    "    # Count the number of interactions in the train set for the specified user\n",
    "    train_x_user = train_x[train_x['user'] == user]\n",
    "    print(\"Number of Interactions in Train Set for User {}: {}\\n\\n\".format(user, train_x_user.shape[0]))\n",
    "\n",
    "    # Add a label for used interactions (add 1 to all interactions that exist in train_x)\n",
    "    user_interactions['used_ind'] = 0\n",
    "    for i in range(user_interactions.shape[0]):\n",
    "        if user_interactions.iloc[i, 1] in list(train_x_user['product']):\n",
    "            user_interactions.iloc[i, 3] = 1\n",
    "\n",
    "    # Add a label for liked items\n",
    "    user_interactions['liked'] = user_interactions['rating'].apply(lambda x: 1 if x > threshold else 0)\n",
    "\n",
    "    # Get the top N recommendations for the specified user (excluding items where used_ind = 1)\n",
    "    user_top_n = user_interactions[user_interactions['used_ind'] == 0]\n",
    "    user_top_n = user_top_n.sort_values(by='rating', ascending=False)\n",
    "    user_top_n = user_top_n.head(N)\n",
    "\n",
    "    # Add a label column to user_top_n: test_ind\n",
    "    user_top_n['test_ind'] = 0\n",
    "    for i in range(user_top_n.shape[0]):\n",
    "        if user_top_n.iloc[i, 1] in list(user_test_set['product']):\n",
    "            user_top_n.iloc[i, 5] = 1\n",
    "\n",
    "    # Calculate Precision@N, Recall@N, and F1@N\n",
    "    precision_at_N = user_top_n['test_ind'].sum() / N\n",
    "    recall_at_N = user_top_n['test_ind'].sum() / liked_items.shape[0]\n",
    "    if precision_at_N + recall_at_N == 0:\n",
    "        f1_at_N = 0\n",
    "    else: f1_at_N = 2 * (precision_at_N * recall_at_N) / (precision_at_N + recall_at_N)\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Results for User {user} with Threshold {threshold} and Top {N} Recommendations!\")\n",
    "    print(f\"Precision@{N}: {precision_at_N:.4f}\")\n",
    "    print(f\"Recall@{N}: {recall_at_N:.4f}\")\n",
    "    print(f\"F1@{N}: {f1_at_N:.4f}\")\n",
    "\n",
    "    # Save the results to a dataframe and return it\n",
    "    results = pd.DataFrame({'Precision@N': [precision_at_N], 'Recall@N': [recall_at_N], 'F1@N': [f1_at_N]})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use function\n",
    "results = evaluate_topN_user(user=0, threshold=3, N=100, data=completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get for All Users\n",
    "\n",
    "It defines a function `evaluate_topN_user` that calculates the metrics for a specified `user`, `threshold`, and `N`. The function returns a dataframe with the results.\n",
    "\n",
    "It then loops through each user in the '`completed`' dataframe and calls the `evaluate_topN_user` function to calculate the metrics for each user. The results for each user are appended to the results dataframe.\n",
    "\n",
    "Finally, it **calculates the average of the metrics across all users and displays the aggregate metrics.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through users to get results for each user and save to a dataframe\n",
    "results = pd.DataFrame()\n",
    "for user in range(len(completed['user'].unique())):\n",
    "    user_results = evaluate_topN_user(user=user, threshold=3, N=10000, data=completed)\n",
    "    results = pd.concat([results, user_results])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average results for all users\n",
    "average_results = results.mean()\n",
    "average_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'Precision@N': [precision_at_N], 'Recall@N': [recall_at_N], 'F1@N': [f1_at_N]})\n",
    "results.to_csv('/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/Results/NCF_results_1_topN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Model 2: Ratings + Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# amz_data = pd.read_csv(r'C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\Data\\set3_data_modelling.csv')\n",
    "amz_data = pd.read_csv('/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/set3_data_modelling.csv')\n",
    "text_embeddings = pd.read_csv(r'/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/NCF Data/text_embeddings.csv')\n",
    "display(amz_data.head())\n",
    "\n",
    "# print details\n",
    "print('Number of Rows: ', amz_data.shape[0])\n",
    "print('Number of Columns: ', amz_data.shape[1])\n",
    "print('Number of Unique Users: ', len(amz_data['reviewerID'].unique()))\n",
    "print('Number of Unique Products: ', len(amz_data['asin'].unique()))\n",
    "\n",
    "\n",
    "# Creating User Item Matrix =====================================================\n",
    "# create user-item matrix\n",
    "data = amz_data.pivot_table(index='reviewerID', columns='asin', values='overall')\n",
    "print(\"\\n\\nUser-Item Matrix\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "- https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/tensorFlow2/variations/universal-sentence-encoder/versions/2?tfhub-redirect=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "# # load the model for sentence embeddings\n",
    "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "# sent_model = hub.load(module_url)\n",
    "# print(f\"Module {module_url} loaded\")\n",
    "\n",
    "# # Embedding review text\n",
    "# print(\"Applying the Universal Sentence Encoder on the review text...\")\n",
    "# review_text = amz_data['reviewText']  # Replace with your actual column name\n",
    "# text_embeddings = sent_model(review_text)\n",
    "# print(\"Review text embeddings generated!\")\n",
    "# print(f\"Shape of Text Embeddings: {text_embeddings.shape}\")\n",
    "\n",
    "# # attach embeddings to dataframe\n",
    "# text_embeddings = text_embeddings.numpy()\n",
    "# text_embeddings = pd.DataFrame(text_embeddings)\n",
    "# text_embeddings['revText'] = amz_data['reviewText']\n",
    "# text_embeddings['asin'] = amz_data['asin']\n",
    "# text_embeddings['reviewerID'] = amz_data['reviewerID']\n",
    "display(text_embeddings.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREP ====================================\n",
    "\n",
    "# create a copy of the original matrix to store hidden ratings\n",
    "x_hidden = data.copy()\n",
    "indices_tracker = []\n",
    "\n",
    "# number of products to hide for each user\n",
    "N = 3\n",
    "\n",
    "# identifies rated items and randomly selects N products to hide ratings for each user\n",
    "np.random.seed(2207)  # You can use any integer value as the seed\n",
    "for user_id in range(x_hidden.shape[0]):\n",
    "    rated_products = np.where(x_hidden.iloc[user_id, :] > 0)[0]\n",
    "    hidden_indices = np.random.choice(rated_products, N, replace=False)\n",
    "    indices_tracker.append(hidden_indices)\n",
    "    x_hidden.iloc[user_id, hidden_indices] = 'Hidden'\n",
    "\n",
    "# get indices of hidden ratings\n",
    "test_data = x_hidden.copy()\n",
    "test_data = test_data.reset_index()\n",
    "test_data = test_data.melt(id_vars=test_data.columns[0], var_name='book', value_name='rating')\n",
    "test_data.columns = ['user', 'product', 'rating']\n",
    "indices_hidden = test_data[test_data['rating'] == 'Hidden'].index\n",
    "\n",
    "# Melt the DataFrame into a format where each row is a user-item interaction\n",
    "data_hidden = x_hidden.reset_index()\n",
    "data_hidden = data_hidden.melt(id_vars=data_hidden.columns[0], var_name='product', value_name='rating')\n",
    "\n",
    "# change rows with hidden ratings to NaN\n",
    "data_hidden.iloc[indices_hidden, 2] = np.nan\n",
    "\n",
    "# rename columns\n",
    "data_hidden.columns = ['user', 'product', 'rating']\n",
    "\n",
    "# Filter out the rows where rating is NaN\n",
    "data_hidden = data_hidden[data_hidden['rating'].notna()]\n",
    "\n",
    "# add text embeddings to the data (match user and product to the embeddings)\n",
    "data_hidden = pd.merge(data_hidden, text_embeddings, how='outer', left_on=['user', 'product'], right_on=['reviewerID', 'asin'])\n",
    "data_hidden.drop(['revText', 'asin','reviewerID'], axis=1, inplace=True)\n",
    "\n",
    "# Filter out the rows where rating is NaN\n",
    "data_hidden = data_hidden[data_hidden['rating'].notna()]\n",
    "\n",
    "# Convert user and item to categorical\n",
    "data_hidden['user'] = data_hidden['user'].astype('category')\n",
    "data_hidden['product'] = data_hidden['product'].astype('category')\n",
    "\n",
    "# see what the data looks like\n",
    "display(data_hidden.head(4))\n",
    "print(\"Data is in format: user, product, rating, text embeddings.\\nIt is ready to be partitioned into training and testing sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save data \n",
    "# data_hidden.to_csv('/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/NCF Data/data_hidden.csv', index=False)    \n",
    "# text_embeddings.to_csv('/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/NCF Data/text_embeddings.csv', index=False)\n",
    "\n",
    "# load data\n",
    "# data_hidden = pd.read_csv(r'/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/NCF Data/data_hidden.csv')\n",
    "# text_embeddings = pd.read_csv(r'/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/NCF Data/text_embeddings.csv')\n",
    "\n",
    "# data_hidden['user'] = data_hidden['user'].astype('category')\n",
    "# data_hidden['product'] = data_hidden['product'].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST AND TRAIN DATA ====================================\n",
    "\n",
    "# Prepare the data - trining\n",
    "train_x = data_hidden[['user', 'product']].apply(lambda x: x.cat.codes)\n",
    "train_y = data_hidden['rating'].astype(np.float64)\n",
    "train_y = (train_y - 1) / 4\n",
    "\n",
    "# add text embeddings to the training data (merge on index)\n",
    "train_x = pd.merge(train_x, data_hidden, how='outer', left_index=True, right_index=True)\n",
    "train_x.drop(['user_y', 'product_y', 'rating'], axis=1, inplace=True)\n",
    "train_x.rename(columns={'user_x': 'user', 'product_x': 'product'}, inplace=True)\n",
    "\n",
    "# Prepare the data - testing\n",
    "copy = data.copy()\n",
    "copy = copy.reset_index()\n",
    "copy = copy.melt(id_vars=copy.columns[0], var_name='product', value_name='rating')\n",
    "copy.columns = ['user', 'product', 'rating']\n",
    "test_x = copy.iloc[indices_hidden, 0:2]\n",
    "\n",
    "\n",
    "# add text embeddings to the testing data (merge on user and product)\n",
    "test_x = pd.merge(test_x, text_embeddings, how='left', left_on=['user', 'product'], right_on=['reviewerID', 'asin'])\n",
    "test_x.drop(['revText', 'asin','reviewerID'], axis=1, inplace=True)\n",
    "test_x['user'] = test_x['user'].astype('category')\n",
    "test_x['product'] = test_x['product'].astype('category')\n",
    "\n",
    "# use cat codes to convert to numerical (for user and product)\n",
    "test_x['user'] = test_x['user'].cat.codes\n",
    "test_x['product'] = test_x['product'].cat.codes\n",
    "test_y = copy.iloc[indices_hidden, 2].astype(np.float64)\n",
    "test_y = (test_y - 1) / 4\n",
    "\n",
    "# show the data\n",
    "print(\"Training Data\")\n",
    "display(train_x.head(3))\n",
    "\n",
    "print(\"\\nTesting Data\")\n",
    "display(test_x.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NCF Model with Reviews\n",
    "\n",
    "\n",
    "**Inputs:**\n",
    "You have three input layers:\n",
    "- `user_input`: Represents the user ID (integer).\n",
    "- `product_input`: Represents the product ID (integer).\n",
    "- `text_input`: Represents the text embeddings of user reviews (float32).\n",
    "\n",
    "**Embeddings:**\n",
    "You create embeddings for users and products using the Embedding layer. These embeddings are essential for capturing latent features.\n",
    "- `user_embedding`: Embedding for user IDs.\n",
    "- `product_embedding`: Embedding for product IDs.\n",
    "\n",
    "**Flattening and Concatenation**:\n",
    "You flatten the user and product embeddings to create vectors (`user_vecs` and `product_vecs`).\n",
    "Then, you concatenate these vectors with the text embeddings (`text_input`) to form the combined input vector (`input_vecs`).\n",
    "\n",
    "**Hidden Layers:**\n",
    "You use a loop to create hidden layers:\n",
    "For the first layer (i == 0), you apply a Dense layer with ReLU activation and dropout.\n",
    "For subsequent layers, you reduce the number of nodes by half and apply the same architecture.\n",
    "\n",
    "**Output Layer**:\n",
    "The final output layer (*y*) predicts the user-item interaction (*rating*).\n",
    "\n",
    "**Model Compilation**:\n",
    "You compile the model using the specified optimizer (Adam, SGD, or RMSprop) and the mean squared error (MSE) loss.\n",
    "\n",
    "**Training:**\n",
    "The model is trained using user and product data (`train_x['user']` and `train_x['product']`) along with the target variable (`train_y`).\n",
    "You split the data into training and validation sets (10% validation split).\n",
    "\n",
    "**Return:**\n",
    "The function returns the trained model and training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a neural network model for collaborative filtering with text embeddings\n",
    "def train_model_2(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout, l2_reg, train_x, train_y, text_embedding_dim, seed=2207, train_plot=True, callback=True):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Inputs\n",
    "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    product_input = Input(shape=(1,), dtype='int32', name='product_input')\n",
    "    text_input = Input(shape=(text_embedding_dim,), dtype='float32', name='text_input') \n",
    "\n",
    "    # Embeddings\n",
    "    user_embedding = Embedding(input_dim=len(data_hidden['user'].cat.categories), output_dim=50, name='user_embedding')(user_input)\n",
    "    product_embedding = Embedding(input_dim=len(data_hidden['product'].cat.categories), output_dim=50, name='product_embedding')(product_input)\n",
    "\n",
    "    # Flatten\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    product_vecs = Flatten()(product_embedding)\n",
    "\n",
    "    # Concatenate user, product, and text embeddings\n",
    "    input_vecs = Concatenate()([user_vecs, product_vecs, text_input])\n",
    "\n",
    "    # Add dense layers\n",
    "    x = input_vecs\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            x = Dense(n_nodes, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "        else:\n",
    "            n_nodes = n_nodes/2\n",
    "            x = Dense(n_nodes, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    # Compile and train the model\n",
    "    model = Model(inputs=[user_input, product_input, text_input], outputs=y)\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "    # Define early stopping\n",
    "    if callback:\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    if callback:\n",
    "        history = model.fit([train_x['user'], train_x['product'], train_x.iloc[:, 2:]], train_y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[early_stopping])\n",
    "    else:\n",
    "        history = model.fit([train_x['user'], train_x['product'], train_x.iloc[:, 2:]], train_y, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    # history = model.fit([train_x['user'], train_x['product'], train_x.iloc[:, 2:]], train_y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    if train_plot:\n",
    "        # Plot training & validation loss values\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "        # plt.title(f'Model loss for Architecture: {optimizer} optimizer, {n_layers} layers, {n_nodes} nodes, {epochs} epochs, {learning_rate} learning rate, {batch_size} batch size')\n",
    "        plt.ylabel('Loss', fontsize=40)\n",
    "        plt.xlabel('Epoch', fontsize = 40)\n",
    "        plt.xticks(fontsize=36)\n",
    "        plt.yticks(fontsize=36)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Final Writing/Figures/ncf_training_2.pdf\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 - 2 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\n",
    "# print(\"Model 1 - 2 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "# model, history = train_model_2(n_layers=2, n_nodes=512, optimizer='adam', epochs=200, learning_rate=0.001, batch_size=128, train_x=train_x, train_y=train_y, text_embedding_dim = text_embeddings.shape[1]-3, dropout=0.5, l2_reg=0.01,  seed=10, train_plot=False, callback = True)\n",
    "\n",
    "model, history = train_model_2(n_layers=2, n_nodes=512, optimizer='adam', epochs=50, learning_rate=0.001, batch_size=128, dropout=0.5, l2_reg=0.01, train_x=train_x, train_y=train_y, seed=2207, train_plot=False, callback=True, text_embedding_dim = text_embeddings.shape[1]-3)\n",
    "\n",
    "# model, history = train_model_1(n_layers=2, n_nodes=512, optimizer='adam', epochs=50, learning_rate=0.001, batch_size=128, dropout=0.5, l2_reg=0.01, train_x=train_x, train_y=train_y, seed=10, train_plot=True, callback=True)\n",
    "\n",
    "# # Model 2 - 3 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\n",
    "# print(\"Model 2 - 3 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "# model2, history2 = train_model_2(n_layers=3, n_nodes=512, optimizer='adam', epochs=200, learning_rate=0.001, batch_size=128, train_x=train_x, train_y=train_y, text_embedding_dim = text_embeddings.shape[1]-3, dropout=0.5, l2_reg=0.01,seed=10, train_plot=False, callback = True)\n",
    "\n",
    "# # Model 3 - 4 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\n",
    "# print(\"Model 3 - 4 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "# model3, history3 = train_model_2(n_layers=4, n_nodes=512, optimizer='adam', epochs=200, learning_rate=0.001, batch_size=128, train_x=train_x, train_y=train_y, text_embedding_dim = text_embeddings.shape[1]-3, dropout=0.5, l2_reg=0.01,seed=10, train_plot=False, callback = True)\n",
    "\n",
    "# # Model 4 - 5 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\n",
    "# print(\"Model 4 - 5 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "# model4, history4 = train_model_2(n_layers=5, n_nodes=512, optimizer='adam', epochs=200, learning_rate=0.001, batch_size=128, train_x=train_x, train_y=train_y, text_embedding_dim = text_embeddings.shape[1]-3, dropout=0.5, l2_reg=0.01,seed=10, train_plot=False, callback = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model had lowest validation loss?\n",
    "print(\"Model 1 Validation Loss: \", min(history.history['val_loss']))\n",
    "# print(\"Model 2 Validation Loss: \", min(history2.history['val_loss']))\n",
    "# print(\"Model 3 Validation Loss: \", min(history3.history['val_loss']))\n",
    "# print(\"Model 4 Validation Loss: \", min(history4.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training and validation loss for all models\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Training Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Model 1', marker='o', color = 'b')\n",
    "plt.plot(history2.history['loss'], label='Model 2', marker='o', color = 'g')\n",
    "plt.plot(history3.history['loss'], label='Model 3', marker='o', color = 'r')\n",
    "plt.plot(history4.history['loss'], label='Model 4', marker='o', color = 'y')\n",
    "plt.title('Model Training Loss for different architectures', weight='bold', size=12)\n",
    "plt.ylabel('Loss', weight='bold', size=12)\n",
    "plt.xlabel('Epoch', weight='bold', size=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Validation Loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['val_loss'], label='Model 1', marker='o', color = 'b')\n",
    "plt.plot(history2.history['val_loss'], label='Model 2', marker='o', color = 'g')\n",
    "plt.plot(history3.history['val_loss'], label='Model 3', marker='o', color = 'r')\n",
    "plt.plot(history4.history['val_loss'], label='Model 4', marker='o', color = 'y')\n",
    "plt.title('Model Validation Loss for different architectures', weight='bold', size=12)\n",
    "plt.ylabel('Loss', weight='bold', size=12)\n",
    "plt.xlabel('Epoch', weight='bold', size=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot validation and training loss on same plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['loss'], label='Model 1 Train', marker='o', color = 'b')\n",
    "plt.plot(history.history['val_loss'], label='Model 1 Validation', marker='o', color = 'b')\n",
    "plt.plot(history2.history['loss'], label='Model 2 Train', marker='o', color = 'g')\n",
    "plt.plot(history2.history['val_loss'], label='Model 2 Validation', marker='o', color = 'g')\n",
    "plt.plot(history3.history['loss'], label='Model 3 Train', marker='o', color = 'r')\n",
    "plt.plot(history3.history['val_loss'], label='Model 3 Validation', marker='o', color = 'r')\n",
    "plt.plot(history4.history['loss'], label='Model 4 Train', marker='o', color = 'y')\n",
    "plt.plot(history4.history['val_loss'], label='Model 4 Validation', marker='o', color = 'y')\n",
    "plt.title('Model Training and Validation Loss', weight='bold', size=12)\n",
    "plt.ylabel('Loss', weight='bold', size=12)\n",
    "plt.xlabel('Epoch', weight='bold', size=12)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print models\n",
    "print(\"Model 1: 2 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 2: 3 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 3: 4 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 4: 5 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning / Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Grid Search Parameters\n",
    "n_layers = [1,2,3,6,8] \n",
    "n_nodes = [128,256,512,1024] \n",
    "optimizer = ['adam', 'sgd']\n",
    "epochs = [50,150,300] \n",
    "learning_rate = [0.001, 0.01,  0.0001] \n",
    "batch_size = [32,64,128] \n",
    "dropout = [0, 0.01, 0.05, 0.08]\n",
    "l2 = [0.01, 0.001, 0.0001]\n",
    "\n",
    "# print(f\"Number of combinations: {len(n_layers) * len(n_nodes) * len(optimizer) * len(epochs) * len(learning_rate) * len(batch_size)* len(dropout)* len(l2)}\")\n",
    "\n",
    "def grid_search(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout_val, l2_val, train_x, train_y):\n",
    "    # Initialize best parameters and best model variables\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    best_score = None\n",
    "\n",
    "    # Generate all possible combinations of hyperparameters\n",
    "    param_combinations = itertools.product(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout_val, l2_val)\n",
    "\n",
    "    # Loop through all combinations\n",
    "    for combination in param_combinations:\n",
    "        # Unpack the combination\n",
    "        n_layer, n_node, opt, epoch, lr, bs, dropout_val, l2_val = combination\n",
    "        print(combination)\n",
    "\n",
    "        # Train the model\n",
    "        model, history = train_model_2(n_layers=n_layer, n_nodes=n_node, optimizer=opt, epochs=epoch, learning_rate=lr, batch_size=bs, dropout=dropout_val, l2_reg=l2_val, train_x=train_x, train_y=train_y, train_plot=False, seed=10, text_embedding_dim = text_embeddings.shape[1]-3, callback=True)\n",
    "\n",
    "        # Evaluate the model\n",
    "        min_loss = min(history.history['val_loss'])\n",
    "\n",
    "        # Check if this model is better than the previous best\n",
    "        if best_score is None or min_loss < best_score:\n",
    "            best_score = min_loss\n",
    "            best_params = combination\n",
    "            best_model = model\n",
    "\n",
    "    return best_params, best_model\n",
    "\n",
    "\n",
    "# run grid search\n",
    "best_params, best_model = grid_search(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout_val, l2_val, train_x, train_y)\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit best model \n",
    "best_model, history = train_model_2(n_layers=best_params[0], n_nodes=best_params[1], optimizer=best_params[2], epochs=best_params[3], learning_rate=best_params[4], batch_size=best_params[5], dropout=best_params[6], l2_reg=best_params[7], train_x=train_x, train_y=train_y, text_embedding_dim = text_embeddings.shape[1]-3, train_plot=True, callback=True, seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATION ====================================\n",
    "# Predict the ratings\n",
    "# y_pred = best_model.predict([test_x['user'], test_x['product'], test_x.iloc[:, 2:]])\n",
    "y_pred = model.predict([test_x['user'], test_x['product'], test_x.iloc[:, 2:]])\n",
    "\n",
    "# Rescale the predictions back to the 1-5 range\n",
    "y_pred = y_pred * 4 + 1\n",
    "\n",
    "# set predictions and actual ratings to variables\n",
    "hidden_ratings_array = (np.array(test_y)*4 + 1)\n",
    "predicted_ratings_array = np.array(y_pred).flatten()\n",
    "\n",
    "# Rating predictions\n",
    "mae = mean_absolute_error(hidden_ratings_array, predicted_ratings_array)\n",
    "mse = mean_squared_error(hidden_ratings_array, predicted_ratings_array)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"\\nRating Metrics\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# save results to csv\n",
    "results = pd.DataFrame({'MAE': [mae.round(3)], 'MSE': [mse.round(3)], 'RMSE': [rmse.round(3)]}) \n",
    "# results.to_csv(r\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/Results/NCF_results_2.csv\", index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming hidden_ratings_array and predicted_ratings_array are NumPy arrays containing the respective data\n",
    "\n",
    "# Get differences\n",
    "differences = hidden_ratings_array - predicted_ratings_array\n",
    "\n",
    "# Check for normality using Shapiro-Wilk test\n",
    "normality_test_statistic, p_value = stats.shapiro(differences)\n",
    "print(\"Shapiro-Wilk test statistic:\", normality_test_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Visualize distribution of differences\n",
    "sns.histplot(differences, kde=True)\n",
    "plt.title('Distribution of Differences')\n",
    "plt.xlabel('Differences')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "rmse_values = {\n",
    "    'Item-Based Collaborative Filtering': 921323.861,\n",
    "    'User-Based Collaborative Filtering': 2.992,\n",
    "    'Non-Negative Matrix Factorisation': 100.862,\n",
    "    'Neural Collaborative Filtering (Ratings Only)': 21212.903,\n",
    "    'Neural Collaborative Filtering (Ratings & Reviews)': 0.01,\n",
    "    'Neural Collaborative Filtering (Ratings, Reviews, Sentiments)': 33233.779,\n",
    "    'Baseline Model': 1.311\n",
    "}\n",
    "\n",
    "# Perform Kruskal-Wallis test\n",
    "h_statistic, p_value_kruskal = kruskal(*rmse_values.values())\n",
    "\n",
    "print(\"Kruskal-Wallis p-value:\", p_value_kruskal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Accuracy Insights\n",
    "\n",
    "1. Want to see if accuracy is better for users who have rated more items. (i.e., for users who have rated more items, is the accuracy of the model better?)\n",
    "\n",
    "2. Want to see if accuracy is better for items that have been rated more times. (i.e., for items that have been rated more times, is the accuracy of the model better?)\n",
    "\n",
    "3. Want to see if accuracy is better for some product categories. (i.e., for some product categories, is the accuracy of the model better?)\n",
    "- TOO FEW REVIEWS PER CATEGORY\n",
    "- RESULTS WOULD NOT OFFER MUCH INSIGHT\n",
    "\n",
    "4. Want to see if accuracy is better for reviews that are longer. (i.e., for reviews that are longer, is the accuracy of the model better?)\n",
    "\n",
    "\n",
    "Effectively, we want to see if accuracy varies according to some variables X or Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### QUESTION 1: PROCESS\n",
    "# 1. Group Users by the Number of Rated Items: Count the number of rated items for each user in your dataset.\n",
    "\n",
    "# Count the number of rated items for each user\n",
    "user_ratings = train_x.groupby('user')['product'].count().reset_index()\n",
    "user_ratings.columns = ['user', 'n_rated_items']\n",
    "\n",
    "# 2. Divide Users into Groups: Divide users into groups based on the number of rated items. You can define these groups based on quartiles, for example, or any other criteria that make sense for your dataset.\n",
    "\n",
    "# Divide users into groups based on the number of rated items\n",
    "user_ratings['group'] = pd.qcut(user_ratings['n_rated_items'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "display(user_ratings)\n",
    "\n",
    "# what are the number of users in each group?\n",
    "print(f'Number of Users in Low Group: {user_ratings[user_ratings[\"group\"] == \"Low\"].shape[0]}')\n",
    "print(f'Number of Users in Medium Group: {user_ratings[user_ratings[\"group\"] == \"Medium\"].shape[0]}')\n",
    "print(f'Number of Users in High Group: {user_ratings[user_ratings[\"group\"] == \"High\"].shape[0]}')\n",
    "print(f'Number of Users in Very High Group: {user_ratings[user_ratings[\"group\"] == \"Very High\"].shape[0]}')\n",
    "\n",
    "# 3. Evaluate the Model for Each Group: Evaluate the model for each group of users. You can use the same metrics you used in the previous question.\n",
    "low_group = user_ratings[user_ratings['group'] == 'Low']\n",
    "medium_group = user_ratings[user_ratings['group'] == 'Medium']\n",
    "high_group = user_ratings[user_ratings['group'] == 'High']\n",
    "very_high_group = user_ratings[user_ratings['group'] == 'Very High']\n",
    "\n",
    "# get test set items for these groups\n",
    "low_group_test = test_x[test_x['user'].isin(low_group['user'])]\n",
    "medium_group_test = test_x[test_x['user'].isin(medium_group['user'])]\n",
    "high_group_test = test_x[test_x['user'].isin(high_group['user'])]\n",
    "very_high_group_test = test_x[test_x['user'].isin(very_high_group['user'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test set ratings for these groups\n",
    "test_y_reset = test_y.reset_index(drop=True)\n",
    "low_group_ratings = test_y_reset[test_x['user'].isin(low_group['user'])]\n",
    "medium_group_ratings = test_y_reset[test_x['user'].isin(medium_group['user'])]\n",
    "high_group_ratings = test_y_reset[test_x['user'].isin(high_group['user'])]\n",
    "very_high_group_ratings = test_y_reset[test_x['user'].isin(very_high_group['user'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get predictions for these groups\n",
    "low_group_pred = y_pred[test_x['user'].isin(low_group['user'])]\n",
    "medium_group_pred = y_pred[test_x['user'].isin(medium_group['user'])]\n",
    "high_group_pred = y_pred[test_x['user'].isin(high_group['user'])]\n",
    "very_high_group_pred = y_pred[test_x['user'].isin(very_high_group['user'])]\n",
    "\n",
    "# set predictions and actual ratings to variables\n",
    "low_group_ratings_array = (np.array(low_group_ratings)*4 + 1)\n",
    "low_group_pred_array = np.array(low_group_pred).flatten()\n",
    "\n",
    "medium_group_ratings_array = (np.array(medium_group_ratings)*4 + 1)\n",
    "medium_group_pred_array = np.array(medium_group_pred).flatten()\n",
    "\n",
    "high_group_ratings_array = (np.array(high_group_ratings)*4 + 1)\n",
    "high_group_pred_array = np.array(high_group_pred).flatten()\n",
    "\n",
    "very_high_group_ratings_array = (np.array(very_high_group_ratings)*4 + 1)\n",
    "very_high_group_pred_array = np.array(very_high_group_pred).flatten()\n",
    "\n",
    "# Rating predictions\n",
    "low_group_mae = mean_absolute_error(low_group_ratings_array, low_group_pred_array)\n",
    "low_group_mse = mean_squared_error(low_group_ratings_array, low_group_pred_array)\n",
    "low_group_rmse = np.sqrt(low_group_mse)\n",
    "\n",
    "medium_group_mae = mean_absolute_error(medium_group_ratings_array, medium_group_pred_array)\n",
    "medium_group_mse = mean_squared_error(medium_group_ratings_array, medium_group_pred_array)\n",
    "medium_group_rmse = np.sqrt(medium_group_mse)\n",
    "\n",
    "high_group_mae = mean_absolute_error(high_group_ratings_array, high_group_pred_array)\n",
    "high_group_mse = mean_squared_error(high_group_ratings_array, high_group_pred_array)    \n",
    "high_group_rmse = np.sqrt(high_group_mse)\n",
    "\n",
    "very_high_group_mae = mean_absolute_error(very_high_group_ratings_array, very_high_group_pred_array)\n",
    "very_high_group_mse = mean_squared_error(very_high_group_ratings_array, very_high_group_pred_array)\n",
    "very_high_group_rmse = np.sqrt(very_high_group_mse)\n",
    "\n",
    "# display results\n",
    "print(\"Checking if the number of reviews impact the model performance.\")\n",
    "results = pd.DataFrame({'MAE': [low_group_mae.round(3), medium_group_mae.round(3), high_group_mae.round(3), very_high_group_mae.round(3)], 'MSE': [low_group_mse.round(3), medium_group_mse.round(3), high_group_mse.round(3), very_high_group_mse.round(3)], 'RMSE': [low_group_rmse.round(3), medium_group_rmse.round(3), high_group_rmse.round(3), very_high_group_rmse.round(3)]})\n",
    "results.index = ['Low', 'Medium', 'High', 'Very High']\n",
    "print(f'Number of Users in Low Group: {low_group.shape[0]}')\n",
    "print(f'Number of Users in Medium Group: {medium_group.shape[0]}')\n",
    "print(f'Number of Users in High Group: {high_group.shape[0]}')\n",
    "print(f'Number of Users in Very High Group: {very_high_group.shape[0]}')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance of model for each user\n",
    "for user in range(user_ratings.shape[0]):\n",
    "    user = user_ratings['user'][user]\n",
    "    # get test set items for user\n",
    "    user_test = test_x[test_x['user'] == user]\n",
    "    # get test set ratings for user\n",
    "    users_ratings = test_y_reset[test_x['user'] == user]\n",
    "    # get predictions for user\n",
    "    user_pred = y_pred[test_x['user'] == user]\n",
    "    # set predictions and actual ratings to variables\n",
    "    user_ratings_array = (np.array(users_ratings)*4 + 1)\n",
    "    user_pred_array = np.array(user_pred).flatten()\n",
    "    # Rating predictions\n",
    "    user_mae = mean_absolute_error(user_ratings_array, user_pred_array)\n",
    "    user_mse = mean_squared_error(user_ratings_array, user_pred_array)\n",
    "    user_rmse = np.sqrt(user_mse)\n",
    "    # assing results to user_ratings\n",
    "    user_ratings.loc[user, 'MAE'] = user_mae\n",
    "    user_ratings.loc[user, 'MSE'] = user_mse\n",
    "    user_ratings.loc[user, 'RMSE'] = user_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize the Results: Plot the accuracy metrics (RMSE, MSE, MAE) against the number of rated items for each group. This will help you visualize any patterns or trends in the accuracy of your model based on the number of rated items.\n",
    "## plot number of rated items vs MAE, MSE, RMSE scatter plot\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(user_ratings['n_rated_items'], user_ratings['MAE'])\n",
    "plt.title('Number of Rated Items vs MAE')\n",
    "plt.xlabel('Number of Rated Items')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(user_ratings['n_rated_items'], user_ratings['MSE'])\n",
    "plt.title('Number of Rated Items vs MSE')\n",
    "plt.xlabel('Number of Rated Items')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(user_ratings['n_rated_items'], user_ratings['RMSE'])\n",
    "plt.title('Number of Rated Items vs RMSE')\n",
    "plt.xlabel('Number of Rated Items')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a colormap for different accuracy metrics\n",
    "colors = {'RMSE': 'seagreen'}\n",
    "\n",
    "# 4. Visualize the Results: Plot the accuracy metrics (RMSE, MSE, MAE) against the number of rated items for each group.\n",
    "# Plot number of rated items vs MAE, MSE, RMSE scatter plot\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "# Iterate over each accuracy metric\n",
    "for metric in ['RMSE']:\n",
    "    plt.scatter(user_ratings['n_rated_items'], user_ratings[metric], c=colors[metric], label=metric)\n",
    "\n",
    "plt.xlabel('Number of Reviews', fontsize=38)\n",
    "plt.ylabel('Accuracy Metric Scores', fontsize=38)\n",
    "plt.xticks(fontsize=34)\n",
    "plt.yticks(fontsize=34)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Final Writing/Figures/ncf_user_ratings_metrics.pdf\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a colormap for different groups\n",
    "palette = sns.color_palette(\"hsv\", len(user_ratings['group'].unique()))\n",
    "\n",
    "# 4. Visualize the Results: Plot the RMSE against the number of rated items for each group.\n",
    "# Plot number of rated items vs RMSE scatter plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate over each group\n",
    "for i, group in enumerate(user_ratings['group'].unique()):\n",
    "    group_data = user_ratings[user_ratings['group'] == group]\n",
    "    plt.scatter(group_data['n_rated_items'], group_data['RMSE'], color=palette[i], label=group)\n",
    "\n",
    "plt.xlabel('Number of Rated Items', fontsize=14)\n",
    "plt.ylabel('RMSE', fontsize=14)\n",
    "plt.legend(title='Group', fontsize=12)\n",
    "plt.title('', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the quartiles \n",
    "user_ratings['n_rated_items'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see summary statistics for each group\n",
    "tab = user_ratings.groupby('group').agg({'MAE': ['mean'], 'MSE': ['mean'], 'RMSE': ['mean',]}).round(3)\n",
    "display(tab)\n",
    "tab['quartiles'] = ['0-12', '13-15', '16-21', '21-' ]\n",
    "tab = tab[['quartiles', 'MAE', 'MSE', 'RMSE']]\n",
    "\n",
    "import tabulate\n",
    "latex_table = tabulate.tabulate(tab, headers='keys', tablefmt='latex_raw', showindex=False)\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kruskal wallis test\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "# Perform Kruskal-Wallis test\n",
    "h_statistic, p_value_kruskal = kruskal(low_group_rmse, medium_group_rmse, high_group_rmse, very_high_group_rmse)\n",
    "print(\"Kruskal-Wallis p-value:\", p_value_kruskal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# apply anova test\n",
    "import scipy.stats as stats\n",
    "f_val, p_val = stats.f_oneway(user_ratings[user_ratings['group'] == 'Low']['RMSE'], user_ratings[user_ratings['group'] == 'Medium']['RMSE'], user_ratings[user_ratings['group'] == 'High']['RMSE'], user_ratings[user_ratings['group'] == 'Very High']['RMSE'])\n",
    "print(f'F-Value: {f_val}')\n",
    "print(f'P-Value: {p_val}')\n",
    "\n",
    "# apply post-hoc test\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "mc = MultiComparison(user_ratings['RMSE'], user_ratings['group'])\n",
    "result = mc.tukeayhsd()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a boxplot to visualize the results\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(x='group', y='RMSE', data=user_ratings, color = 'steelblue')\n",
    "plt.xlabel('', fontsize=1)\n",
    "plt.ylabel('RMSE', fontsize=20)\n",
    "plt.title('', fontsize=16)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.savefig(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Final Writing/Figures/ncf_groupUser_ratings_metrics.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 2: Want to see if accuracy is better for items that have been rated more times. (i.e., for items that have been rated more times, is the accuracy of the model better?)\n",
    "\n",
    "# Count the number of ratings for each item\n",
    "item_ratings = train_x.groupby('product')['user'].count().reset_index()\n",
    "item_ratings.columns = ['product', 'n_ratings']\n",
    "\n",
    "# Divide items into groups based on the number of ratings\n",
    "item_ratings['group'] = pd.qcut(item_ratings['n_ratings'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Evaluate the model for each group of items\n",
    "low_group = item_ratings[item_ratings['group'] == 'Low']\n",
    "medium_group = item_ratings[item_ratings['group'] == 'Medium']\n",
    "high_group = item_ratings[item_ratings['group'] == 'High']\n",
    "very_high_group = item_ratings[item_ratings['group'] == 'Very High']\n",
    "\n",
    "# get test set items for these groups\n",
    "low_group_test = test_x[test_x['product'].isin(low_group['product'])]\n",
    "medium_group_test = test_x[test_x['product'].isin(medium_group['product'])]\n",
    "high_group_test = test_x[test_x['product'].isin(high_group['product'])]\n",
    "very_high_group_test = test_x[test_x['product'].isin(very_high_group['product'])]\n",
    "\n",
    "# get test set ratings for these groups\n",
    "low_group_ratings = test_y_reset[test_x['product'].isin(low_group['product'])]\n",
    "medium_group_ratings = test_y_reset[test_x['product'].isin(medium_group['product'])]\n",
    "high_group_ratings = test_y_reset[test_x['product'].isin(high_group['product'])]\n",
    "very_high_group_ratings = test_y_reset[test_x['product'].isin(very_high_group['product'])]\n",
    "\n",
    "# get predictions for these groups\n",
    "low_group_pred = y_pred[test_x['product'].isin(low_group['product'])]\n",
    "medium_group_pred = y_pred[test_x['product'].isin(medium_group['product'])]\n",
    "high_group_pred = y_pred[test_x['product'].isin(high_group['product'])]\n",
    "very_high_group_pred = y_pred[test_x['product'].isin(very_high_group['product'])]\n",
    "\n",
    "# set predictions and actual ratings to variables\n",
    "low_group_ratings_array = (np.array(low_group_ratings)*4 + 1)\n",
    "low_group_pred_array = np.array(low_group_pred).flatten()\n",
    "\n",
    "medium_group_ratings_array = (np.array(medium_group_ratings)*4 + 1)\n",
    "medium_group_pred_array = np.array(medium_group_pred).flatten()\n",
    "\n",
    "high_group_ratings_array = (np.array(high_group_ratings)*4 + 1)\n",
    "high_group_pred_array = np.array(high_group_pred).flatten()\n",
    "\n",
    "very_high_group_ratings_array = (np.array(very_high_group_ratings)*4 + 1)\n",
    "very_high_group_pred_array = np.array(very_high_group_pred).flatten()\n",
    "\n",
    "# Rating predictions\n",
    "low_group_mae = mean_absolute_error(low_group_ratings_array, low_group_pred_array)\n",
    "low_group_mse = mean_squared_error(low_group_ratings_array, low_group_pred_array)\n",
    "low_group_rmse = np.sqrt(low_group_mse)\n",
    "\n",
    "medium_group_mae = mean_absolute_error(medium_group_ratings_array, medium_group_pred_array)\n",
    "medium_group_mse = mean_squared_error(medium_group_ratings_array, medium_group_pred_array)\n",
    "medium_group_rmse = np.sqrt(medium_group_mse)\n",
    "\n",
    "high_group_mae = mean_absolute_error(high_group_ratings_array, high_group_pred_array)\n",
    "high_group_mse = mean_squared_error(high_group_ratings_array, high_group_pred_array)\n",
    "high_group_rmse = np.sqrt(high_group_mse)\n",
    "\n",
    "very_high_group_mae = mean_absolute_error(very_high_group_ratings_array, very_high_group_pred_array)\n",
    "very_high_group_mse = mean_squared_error(very_high_group_ratings_array, very_high_group_pred_array)\n",
    "very_high_group_rmse = np.sqrt(very_high_group_mse)\n",
    "\n",
    "# display results\n",
    "print(\"Checking if the number of reviews of an impact the model performance for items.\")\n",
    "results = pd.DataFrame({'MAE': [low_group_mae.round(3), medium_group_mae.round(3), high_group_mae.round(3), very_high_group_mae.round(3)], 'MSE': [low_group_mse.round(3), medium_group_mse.round(3), high_group_mse.round(3), very_high_group_mse.round(3)], 'RMSE': [low_group_rmse.round(3), medium_group_rmse.round(3), high_group_rmse.round(3), very_high_group_rmse.round(3)]})\n",
    "results.index = ['Low', 'Medium', 'High', 'Very High']\n",
    "print(f'Number of Items in Low Group: {low_group.shape[0]}')\n",
    "print(f'Number of Items in Medium Group: {medium_group.shape[0]}')\n",
    "print(f'Number of Items in High Group: {high_group.shape[0]}')\n",
    "print(f'Number of Items in Very High Group: {very_high_group.shape[0]}')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance of model for each item\n",
    "for item in item_ratings['product']:\n",
    "    # Filter test set data for the current item\n",
    "    item_test = test_x[test_x['product'] == item]\n",
    "    if not item_test.empty:  # Check if there are samples available\n",
    "        # Get test set ratings for the current item\n",
    "        items_ratings = test_y_reset[test_x['product'] == item]\n",
    "        # Get predictions for the current item\n",
    "        item_pred = y_pred[test_x['product'] == item]\n",
    "        # Set predictions and actual ratings to variables\n",
    "        item_ratings_array = (np.array(items_ratings) * 4 + 1)\n",
    "        item_pred_array = np.array(item_pred).flatten()\n",
    "        # Rating predictions\n",
    "        item_mae = mean_absolute_error(item_ratings_array, item_pred_array)\n",
    "        item_mse = mean_squared_error(item_ratings_array, item_pred_array)\n",
    "        item_rmse = np.sqrt(item_mse)\n",
    "        # Assign results to item_ratings\n",
    "        item_ratings.loc[item, 'MAE'] = item_mae\n",
    "        item_ratings.loc[item, 'MSE'] = item_mse\n",
    "        item_ratings.loc[item, 'RMSE'] = item_rmse\n",
    "    else:\n",
    "        # No samples available for this item\n",
    "        print(f\"No test set data available for item {item}. Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many nas in item_ratings\n",
    "item_ratings.isna().sum()\n",
    "\n",
    "#the test set items were randomly selected from the users' rated items list, there's a possibility that certain items may not have been included in the test set due to the random sampling process. As a result, when we attempt to evaluate the model's performance for each item using the test set, some items may not have any corresponding test set data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize the Results: Plot the accuracy metrics (RMSE, MSE, MAE) against the number of reviews for each group. This will help you visualize any patterns or trends in the accuracy of your model based on the number of rated items.¸\n",
    "## plot number of rated items vs MAE, MSE, RMSE scatter plot\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(item_ratings['n_ratings'], item_ratings['MAE'])\n",
    "plt.title('Number of Ratings vs MAE')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(item_ratings['n_ratings'], item_ratings['MSE'])\n",
    "plt.title('Number of Ratings vs MSE')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(item_ratings['n_ratings'], item_ratings['RMSE'])\n",
    "plt.title('Number of Ratings vs RMSE')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a colormap for different accuracy metrics\n",
    "colors = {'RMSE': 'seagreen'}\n",
    "\n",
    "# 4. Visualize the Results: Plot the accuracy metrics (RMSE, MSE, MAE) against the number of rated items for each group.\n",
    "# Plot number of rated items vs MAE, MSE, RMSE scatter plot\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "# Iterate over each accuracy metric\n",
    "for metric in ['RMSE']:\n",
    "    plt.scatter(item_ratings['n_ratings'], item_ratings[metric], c=colors[metric], label=metric)\n",
    "\n",
    "plt.xlabel('Number of Reviews', fontsize=38)\n",
    "plt.ylabel('Accuracy Metric Scores', fontsize=38)\n",
    "plt.legend(fontsize=38)\n",
    "plt.xticks(fontsize=34)\n",
    "plt.yticks(fontsize=34)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Final Writing/Figures/ncf_item_ratings_metrics.pdf\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ratings['n_ratings'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see summary statistics for each group\n",
    "tab = item_ratings.groupby('group').agg({'MAE': ['mean'], 'MSE': ['mean'], 'RMSE': ['mean',]}).round(3)\n",
    "display(tab)\n",
    "tab['quartiles'] = ['0-13', '14-16', '6-25', '26-' ]\n",
    "tab = tab[['quartiles', 'MAE', 'MSE', 'RMSE']]\n",
    "\n",
    "import tabulate\n",
    "latex_table = tabulate.tabulate(tab, headers='keys', tablefmt='latex_raw', showindex=False)\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see summary statistics for each group\n",
    "display(item_ratings.groupby('group').agg({'MAE': ['mean', 'std'], 'MSE': ['mean', 'std'], 'RMSE': ['mean', 'std']}))\n",
    "\n",
    "# apply anova test\n",
    "item_ratings.dropna(inplace=True)\n",
    "f_val, p_val = stats.f_oneway(item_ratings[item_ratings['group'] == 'Low']['RMSE'], item_ratings[item_ratings['group'] == 'Medium']['RMSE'], item_ratings[item_ratings['group'] == 'High']['RMSE'], item_ratings[item_ratings['group'] == 'Very High']['RMSE'])\n",
    "print(f'F-Value: {f_val}')\n",
    "print(f'P-Value: {p_val}')\n",
    "\n",
    "# apply post-hoc test\n",
    "item_ratings['RMSE'] = pd.to_numeric(item_ratings['RMSE'], errors='coerce')  # coerce errors to NaN if conversion fails\n",
    "mc = MultiComparison(item_ratings['RMSE'], item_ratings['group'])\n",
    "result = mc.tukeyhsd()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Want to see if accuracy is better for reviews that are longer. (i.e., for reviews that are longer, is the accuracy of the model better?)\n",
    "train_x_extd = pd.merge(data_hidden, text_embeddings[['revText', 'reviewerID', 'asin']], how='outer', left_on=['user', 'product'], right_on=['reviewerID', 'asin'])\n",
    "train_x_extd.drop(['asin','reviewerID'], axis=1, inplace=True)\n",
    "train_x_extd = train_x_extd[train_x_extd['rating'].notna()]\n",
    "train_x_extd['user'] = train_x_extd['user'].astype('category')\n",
    "train_x_extd['product'] = train_x_extd['product'].astype('category')\n",
    "train_x_extd.set_index(data_hidden.index, inplace=True)\n",
    "train_x_extd[['user', 'product']] = train_x_extd[['user', 'product']].apply(lambda x: x.cat.codes)\n",
    "train_x_extd['review_length'] = train_x_extd['revText'].apply(lambda x: len(x.split()))\n",
    "train_x_extd.drop('revText', axis=1, inplace=True)\n",
    "train_x_extd.drop('rating', axis=1, inplace=True)\n",
    "train_x_extd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test_x_extd\n",
    "test_x_extd = copy.iloc[indices_hidden, 0:2]\n",
    "\n",
    "\n",
    "# add review length to test_x_extd\n",
    "test_x_extd = pd.merge(test_x_extd, text_embeddings[['revText', 'reviewerID', 'asin']], how='outer', left_on=['user', 'product'], right_on=['reviewerID', 'asin'])\n",
    "test_x_extd.drop(['asin','reviewerID'], axis=1, inplace=True)\n",
    "test_x_extd = test_x_extd[test_x_extd['user'].notna()]\n",
    "test_x_extd['user'] = test_x_extd['user'].astype('category')\n",
    "test_x_extd['product'] = test_x_extd['product'].astype('category')\n",
    "test_x_extd['user'] = test_x_extd['user'].cat.codes\n",
    "test_x_extd['product'] = test_x_extd['product'].cat.codes\n",
    "test_x_extd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to 1-dimensional arrays\n",
    "actual_ratings = (test_y.values.flatten() * 4) + 1\n",
    "predicted_ratings = (y_pred.flatten())\n",
    "\n",
    "# Create the final DataFrame\n",
    "final_df = pd.DataFrame({'user': test_x['user'], 'product': test_x['product'], 'actual_rating': actual_ratings, 'predicted_rating': predicted_ratings})\n",
    "print(final_df.shape)\n",
    "\n",
    "# change user and product back to original values (not cat.code)\n",
    "final_df\n",
    "\n",
    "# merge to get review length\n",
    "merged=final_df.merge(test_x_extd, on=['user', 'product'], how='left')\n",
    "merged\n",
    "\n",
    "# get review length for each review\n",
    "merged['review_length'] = merged['revText'].apply(lambda x: len(x.split()))\n",
    "merged = merged.drop('revText', axis=1)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide reviews into groups based on the review length\n",
    "merged['group'] = pd.qcut(merged['review_length'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "merged\n",
    "\n",
    "\n",
    "# see numbers in each group\n",
    "print(f'Number of Reviews in Low Group: {merged[merged[\"group\"] == \"Low\"].shape[0]}')\n",
    "print(f'Number of Reviews in Medium Group: {merged[merged[\"group\"] == \"Medium\"].shape[0]}')\n",
    "print(f'Number of Reviews in High Group: {merged[merged[\"group\"] == \"High\"].shape[0]}')\n",
    "print(f'Number of Reviews in Very High Group: {merged[merged[\"group\"] == \"Very High\"].shape[0]}')\n",
    "\n",
    "# get metrics for row\n",
    "merged['MAE'] = abs(merged['actual_rating'] - merged['predicted_rating'])\n",
    "merged['MSE'] = (merged['actual_rating'] - merged['predicted_rating'])**2\n",
    "merged['RMSE'] = np.sqrt((merged['actual_rating'] - merged['predicted_rating'])**2)\n",
    "display(merged)\n",
    "\n",
    "#drop std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a colormap for different accuracy metrics\n",
    "colors = {'RMSE': 'seagreen'}\n",
    "\n",
    "# 4. Visualize the Results: Plot the accuracy metrics (RMSE, MSE, MAE) against the number of rated items for each group.\n",
    "# Plot number of rated items vs MAE, MSE, RMSE scatter plot\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Iterate over each accuracy metric\n",
    "for metric in ['RMSE']:\n",
    "    plt.scatter(merged['review_length'], merged[metric], c=colors[metric], label=metric)\n",
    "\n",
    "plt.xlabel('Review Length', fontsize=24)\n",
    "plt.ylabel('Accuracy Metric Scores', fontsize=24)\n",
    "plt.legend(fontsize=24)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Final Writing/Figures/ncf_length_ratings_metrics.pdf\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['review_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see summary statistics for each group\n",
    "tab = merged.groupby('group').agg({'MAE': ['mean'], 'MSE': ['mean'], 'RMSE': ['mean',]}).round(3)\n",
    "display(tab)\n",
    "tab['quartiles'] = ['0-6', '7-31', '32-118', '118-' ]\n",
    "tab = tab[['quartiles', 'MAE', 'MSE', 'RMSE']]\n",
    "\n",
    "import tabulate\n",
    "latex_table = tabulate.tabulate(tab, headers='keys', tablefmt='latex_raw', showindex=False)\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean rmse for each group\n",
    "display(merged.groupby('group').agg({'MAE': ['mean'], 'MSE': ['mean'], 'RMSE': ['mean']}))\n",
    "\n",
    "# apply anova test\n",
    "import scipy.stats as stats\n",
    "f_val, p_val = stats.f_oneway(merged[merged['group'] == 'Low']['RMSE'], merged[merged['group'] == 'Medium']['RMSE'], merged[merged['group'] == 'High']['RMSE'], merged[merged['group'] == 'Very High']['RMSE'])\n",
    "print(f'F-Value: {f_val}')\n",
    "print(f'P-Value: {p_val}')\n",
    "\n",
    "# apply post-hoc test\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "mc = MultiComparison(merged['RMSE'], merged['group'])\n",
    "result = mc.tukeyhsd()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Ratings + Reviews + Sentiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# amz_data = pd.read_csv(r'C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\Data\\set2_data_modelling.csv')\n",
    "amz_data = pd.read_csv('/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/set3_data_modelling.csv')\n",
    "text_embeddings = pd.read_csv(r'/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/NCF Data/text_embeddings.csv')\n",
    "display(amz_data.head())\n",
    "\n",
    "# print details\n",
    "print('Number of Rows: ', amz_data.shape[0])\n",
    "print('Number of Columns: ', amz_data.shape[1])\n",
    "print('Number of Unique Users: ', len(amz_data['reviewerID'].unique()))\n",
    "print('Number of Unique Products: ', len(amz_data['asin'].unique()))\n",
    "\n",
    "\n",
    "# Creating User Item Matrix =====================================================\n",
    "# create user-item matrix\n",
    "data = amz_data.pivot_table(index='reviewerID', columns='asin', values='overall')\n",
    "print(\"\\n\\nUser-Item Matrix\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "# # load the model for sentence embeddings\n",
    "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "# sent_model = hub.load(module_url)\n",
    "# print(f\"Module {module_url} loaded\")\n",
    "\n",
    "# # Embedding review text\n",
    "# print(\"Applying the Universal Sentence Encoder on the review text...\")\n",
    "# review_text = amz_data['reviewText']  # Replace with your actual column name\n",
    "# text_embeddings = sent_model(review_text)\n",
    "# print(\"Review text embeddings generated!\")\n",
    "# print(f\"Shape of Text Embeddings: {text_embeddings.shape}\")\n",
    "\n",
    "# # attach embeddings to dataframe\n",
    "# text_embeddings = text_embeddings.numpy()\n",
    "# text_embeddings = pd.DataFrame(text_embeddings)\n",
    "# text_embeddings['revText'] = amz_data['reviewText']\n",
    "# text_embeddings['asin'] = amz_data['asin']\n",
    "# text_embeddings['reviewerID'] = amz_data['reviewerID']\n",
    "display(text_embeddings.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = amz_data[['reviewerID', 'asin','sentiments_vader']]\n",
    "sentiments.columns = ['reviewerID', 'asin', 'sentiments']\n",
    "display(sentiments.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREP ====================================\n",
    "\n",
    "# create a copy of the original matrix to store hidden ratings\n",
    "x_hidden = data.copy()\n",
    "indices_tracker = []\n",
    "\n",
    "# number of products to hide for each user\n",
    "N = 3\n",
    "\n",
    "# identifies rated items and randomly selects N products to hide ratings for each user\n",
    "np.random.seed(2207)  # You can use any integer value as the seed\n",
    "for user_id in range(x_hidden.shape[0]):\n",
    "    rated_products = np.where(x_hidden.iloc[user_id, :] > 0)[0]\n",
    "    hidden_indices = np.random.choice(rated_products, N, replace=False)\n",
    "    indices_tracker.append(hidden_indices)\n",
    "    x_hidden.iloc[user_id, hidden_indices] = 'Hidden'\n",
    "\n",
    "# get indices of hidden ratings\n",
    "test_data = x_hidden.copy()\n",
    "test_data = test_data.reset_index()\n",
    "test_data = test_data.melt(id_vars=test_data.columns[0], var_name='book', value_name='rating')\n",
    "test_data.columns = ['user', 'product', 'rating']\n",
    "indices_hidden = test_data[test_data['rating'] == 'Hidden'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame into a format where each row is a user-item interaction\n",
    "data_hidden = x_hidden.reset_index()\n",
    "data_hidden = data_hidden.melt(id_vars=data_hidden.columns[0], var_name='product', value_name='rating')\n",
    "\n",
    "\n",
    "# change rows with hidden ratings to NaN\n",
    "data_hidden.iloc[indices_hidden, 2] = np.nan\n",
    "\n",
    "# rename columns\n",
    "data_hidden.columns = ['user', 'product', 'rating']\n",
    "\n",
    "# Filter out the rows where rating is NaN\n",
    "data_hidden = data_hidden[data_hidden['rating'].notna()]\n",
    "\n",
    "# add sentiments\n",
    "data_hidden = pd.merge(data_hidden, sentiments, how='outer', left_on=['user', 'product'], right_on=['reviewerID', 'asin'])\n",
    "data_hidden.drop(['asin','reviewerID'], axis=1, inplace=True)\n",
    "\n",
    "# add text embeddings to the data (match user and product to the embeddings)\n",
    "data_hidden = pd.merge(data_hidden, text_embeddings, how='outer', left_on=['user', 'product'], right_on=['reviewerID', 'asin'])\n",
    "data_hidden.drop(['revText', 'asin','reviewerID'], axis=1, inplace=True)\n",
    "\n",
    "# Filter out the rows where rating is NaN\n",
    "data_hidden = data_hidden[data_hidden['rating'].notna()]\n",
    "\n",
    "# Convert user and item to categorical\n",
    "data_hidden['user'] = data_hidden['user'].astype('category')\n",
    "data_hidden['product'] = data_hidden['product'].astype('category')\n",
    "\n",
    "# see what the data looks like\n",
    "display(data_hidden.head(4))\n",
    "print(\"Data is in format: user, product, rating, sentiments, text embeddings.\\nIt is ready to be partitioned into training and testing sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST AND TRAIN DATA ====================================\n",
    "\n",
    "# Prepare the data - trining\n",
    "train_x = data_hidden[['user', 'product']].apply(lambda x: x.cat.codes)\n",
    "train_y = data_hidden['rating'].astype(np.float64)\n",
    "train_y = (train_y - 1) / 4\n",
    "\n",
    "# add text embeddings to the training data (merge on index)\n",
    "train_x = pd.merge(train_x, data_hidden, how='outer', left_index=True, right_index=True)\n",
    "train_x.drop(['user_y', 'product_y', 'rating'], axis=1, inplace=True)\n",
    "train_x.rename(columns={'user_x': 'user', 'product_x': 'product'}, inplace=True)\n",
    "train_x\n",
    "\n",
    "\n",
    "# Prepare the data - testing\n",
    "copy = data.copy()\n",
    "copy = copy.reset_index()\n",
    "copy = copy.melt(id_vars=copy.columns[0], var_name='product', value_name='rating')\n",
    "copy.columns = ['user', 'product', 'rating']\n",
    "test_x = copy.iloc[indices_hidden, 0:2]\n",
    "\n",
    "# add sentiments to the testing data\n",
    "test_x = pd.merge(test_x, sentiments, how='left', left_on=['user', 'product'], right_on=['reviewerID', 'asin'])\n",
    "test_x.drop(['asin','reviewerID'], axis=1, inplace=True)\n",
    "\n",
    "# add text embeddings to the testing data (merge on user and product)\n",
    "test_x = pd.merge(test_x, text_embeddings, how='left', left_on=['user', 'product'], right_on=['reviewerID', 'asin'])\n",
    "test_x.drop(['revText', 'asin','reviewerID'], axis=1, inplace=True)\n",
    "test_x['user'] = test_x['user'].astype('category')\n",
    "test_x['product'] = test_x['product'].astype('category')\n",
    "\n",
    "# use cat codes to convert to numerical (for user and product)\n",
    "test_x['user'] = test_x['user'].cat.codes\n",
    "test_x['product'] = test_x['product'].cat.codes\n",
    "test_y = copy.iloc[indices_hidden, 2].astype(np.float64)\n",
    "test_y = (test_y - 1) / 4\n",
    "\n",
    "# show the data\n",
    "print(\"Training Data\")\n",
    "display(train_x.head(3))\n",
    "\n",
    "print(\"\\nTesting Data\")\n",
    "display(test_x.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NCF Model with Reviews + Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a neural network model for collaborative filtering with text embeddings\n",
    "def train_model_3(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout, l2_reg, train_x, train_y, text_embedding_dim, seed=2207, train_plot=True, callback=True):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Inputs\n",
    "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    product_input = Input(shape=(1,), dtype='int32', name='product_input')\n",
    "    text_input = Input(shape=(text_embedding_dim,), dtype='float32', name='text_input') \n",
    "    sentiment_input = Input(shape=(1,), dtype='float32', name='sentiment_input')  # Sentiment scores\n",
    "\n",
    "    # Embeddings\n",
    "    user_embedding = Embedding(input_dim=len(data_hidden['user'].cat.categories), output_dim=50, name='user_embedding')(user_input)\n",
    "    product_embedding = Embedding(input_dim=len(data_hidden['product'].cat.categories), output_dim=50, name='product_embedding')(product_input)\n",
    "\n",
    "    # Flatten\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    product_vecs = Flatten()(product_embedding)\n",
    "\n",
    "    # Concatenate user, product, and text embeddings\n",
    "    input_vecs = Concatenate()([user_vecs, product_vecs, sentiment_input, text_input])\n",
    "\n",
    "    # Add dense layers\n",
    "    x = input_vecs\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            x = Dense(n_nodes, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "        else:\n",
    "            n_nodes = n_nodes/2\n",
    "            x = Dense(n_nodes, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    # Compile and train the model\n",
    "    model = Model(inputs=[user_input, product_input, sentiment_input, text_input], outputs=y)\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "    # Define early stopping\n",
    "    if callback:\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    if callback:\n",
    "        history = model.fit([train_x['user'], train_x['product'],  train_x.iloc[:, 3:], train_x['sentiments']], train_y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[early_stopping])\n",
    "    else:\n",
    "        history = model.fit([train_x['user'], train_x['product'],  train_x.iloc[:, 3:], train_x['sentiments']], train_y, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    if train_plot:\n",
    "        # Plot training & validation loss values\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "        # plt.title(f'Model loss for Architecture: {optimizer} optimizer, {n_layers} layers, {n_nodes} nodes, {epochs} epochs, {learning_rate} learning rate, {batch_size} batch size')\n",
    "        plt.ylabel('Loss', fontsize=40)\n",
    "        plt.xlabel('Epoch', fontsize = 40)\n",
    "        plt.xticks(fontsize=36)\n",
    "        plt.yticks(fontsize=36)\n",
    "        plt.legend(loc='upper right', fontsize=40)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Final Writing/Figures/ncf_training_3.pdf\")\n",
    "        plt.show()\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 - 2 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\n",
    "# print(\"Model 1 - 2 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "# model, history = train_model_3(n_layers=2, n_nodes=512, optimizer='adam', epochs=200, learning_rate=0.001, batch_size=128, train_x=train_x, train_y=train_y, text_embedding_dim = text_embeddings.shape[1]-3, dropout=0.5, l2_reg=0.01,  seed=10, train_plot=False, callback=True)\n",
    "\n",
    "model, history = train_model_3(n_layers=3, n_nodes=512, optimizer='adam', epochs=50, learning_rate=0.001, batch_size=128, dropout=0.5, l2_reg=0.01, train_x=train_x, train_y=train_y, seed=10, train_plot=True, callback=False, text_embedding_dim = text_embeddings.shape[1]-3)\n",
    "\n",
    "# # Model 2 - 3 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\n",
    "# print(\"Model 2 - 3 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "# model2, history2 = train_model_3(n_layers=3, n_nodes=512, optimizer='adam', epochs=200, learning_rate=0.001, batch_size=128, train_x=train_x, train_y=train_y, text_embedding_dim = text_embeddings.shape[1]-3, dropout=0.5, l2_reg=0.01, seed=10, train_plot=False, callback=True)\n",
    "\n",
    "# # Model 3 - 4 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\n",
    "# print(\"Model 3 - 4 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "# model3, history3 = train_model_3(n_layers=4, n_nodes=512, optimizer='adam', epochs=200, learning_rate=0.001, batch_size=128, train_x=train_x, train_y=train_y, text_embedding_dim = text_embeddings.shape[1]-3, dropout=0.5, l2_reg=0.01, seed=10, train_plot=False, callback=True)\n",
    "\n",
    "# # Model 4 - 5 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\n",
    "# print(\"Model 4 - 5 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "# model4, history4 = train_model_3(n_layers=5, n_nodes=512, optimizer='adam', epochs=200, learning_rate=0.001, batch_size=128, train_x=train_x, train_y=train_y, text_embedding_dim = text_embeddings.shape[1]-3, dropout=0.5, l2_reg=0.01, seed=10, train_plot=False, callback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model had lowest validation loss?\n",
    "print(\"Model 1 Validation Loss: \", min(history.history['val_loss']))\n",
    "print(\"Model 2 Validation Loss: \", min(history2.history['val_loss']))\n",
    "print(\"Model 3 Validation Loss: \", min(history3.history['val_loss']))\n",
    "print(\"Model 4 Validation Loss: \", min(history4.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training and validation loss for all models\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Training Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Model 1', marker='o', color = 'b')\n",
    "plt.plot(history2.history['loss'], label='Model 2', marker='o', color = 'g')\n",
    "plt.plot(history3.history['loss'], label='Model 3', marker='o', color = 'r')\n",
    "plt.plot(history4.history['loss'], label='Model 4', marker='o', color = 'y')\n",
    "plt.title('Model Training Loss for different architectures', weight='bold', size=12)\n",
    "plt.ylabel('Loss', weight='bold', size=12)\n",
    "plt.xlabel('Epoch', weight='bold', size=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Validation Loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['val_loss'], label='Model 1', marker='o', color = 'b')\n",
    "plt.plot(history2.history['val_loss'], label='Model 2', marker='o', color = 'g')\n",
    "plt.plot(history3.history['val_loss'], label='Model 3', marker='o', color = 'r')\n",
    "plt.plot(history4.history['val_loss'], label='Model 4', marker='o', color = 'y')\n",
    "plt.title('Model Validation Loss for different architectures', weight='bold', size=12)\n",
    "plt.ylabel('Loss', weight='bold', size=12)\n",
    "plt.xlabel('Epoch', weight='bold', size=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot validation and training loss on same plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['loss'], label='Model 1 Train', marker='o', color = 'b')\n",
    "plt.plot(history.history['val_loss'], label='Model 1 Validation', marker='o', color = 'b')\n",
    "plt.plot(history2.history['loss'], label='Model 2 Train', marker='o', color = 'g')\n",
    "plt.plot(history2.history['val_loss'], label='Model 2 Validation', marker='o', color = 'g')\n",
    "plt.plot(history3.history['loss'], label='Model 3 Train', marker='o', color = 'r')\n",
    "plt.plot(history3.history['val_loss'], label='Model 3 Validation', marker='o', color = 'r')\n",
    "plt.plot(history4.history['loss'], label='Model 4 Train', marker='o', color = 'y')\n",
    "plt.plot(history4.history['val_loss'], label='Model 4 Validation', marker='o', color = 'y')\n",
    "plt.title('Model Training and Validation Loss', weight='bold', size=12)\n",
    "plt.ylabel('Loss', weight='bold', size=12)\n",
    "plt.xlabel('Epoch', weight='bold', size=12)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print models\n",
    "print(\"Model 1: 2 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 2: 3 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 3: 4 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")\n",
    "print(\"Model 4: 5 layers, 512 nodes, adam, 50 epochs, 0.001 learning rate, 64 batch size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamaeter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Grid Search Parameters\n",
    "n_layers = [1,2,3,6,8] \n",
    "n_nodes = [128,256,512,1024] \n",
    "optimizer = ['adam', 'sgd']\n",
    "epochs = [50,150,300] \n",
    "learning_rate = [0.001, 0.01,  0.0001] \n",
    "batch_size = [32,64,128] \n",
    "dropout = [0, 0.01, 0.05, 0.08]\n",
    "l2 = [0.01, 0.001, 0.0001]\n",
    "print(f\"Number of combinations: {len(n_layers) * len(n_nodes) * len(optimizer) * len(epochs) * len(learning_rate) * len(batch_size)* len(dropout)* len(l2)}\")\n",
    "\n",
    "def grid_search(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout, l2, train_x, train_y):\n",
    "    # Initialize best parameters and best model variables\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    best_score = None\n",
    "\n",
    "    # Generate all possible combinations of hyperparameters\n",
    "    param_combinations = itertools.product(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout, l2)\n",
    "\n",
    "    # Loop through all combinations\n",
    "    for combination in param_combinations:\n",
    "        # Unpack the combination\n",
    "        n_layer, n_node, opt, epoch, lr, bs, dropout, l2 = combination\n",
    "\n",
    "        # Train the model\n",
    "        model, history = train_model_2(n_layer, n_node, opt, epoch, lr, bs, dropout, l2, train_x, train_y, train_plot=False, seed=10, text_embedding_dim = text_embeddings.shape[1]-3, callback=True)\n",
    "\n",
    "        # Evaluate the model - min val loss\n",
    "        min_loss = min(history.history['val_loss'])\n",
    "        \n",
    "        # Check if this model is better than the previous best\n",
    "        if best_score is None or min_loss < best_score:\n",
    "            best_score = min_loss\n",
    "            best_params = combination\n",
    "            best_model = model\n",
    "\n",
    "    return best_params, best_model\n",
    "\n",
    "\n",
    "# run grid search\n",
    "best_params, best_model = grid_search(n_layers, n_nodes, optimizer, epochs, learning_rate, batch_size, dropout, l2, train_x, train_y)\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit best model \n",
    "best_model, history = train_model_3(n_layers=best_params[0], n_nodes=best_params[1], optimizer=best_params[2], epochs=best_params[3], learning_rate=best_params[4], batch_size=best_params[5], dropout=best_params[6], l2_reg=best_params[7], train_x=train_x, train_y=train_y, text_embedding_dim = text_embeddings.shape[1]-3, train_plot=True, callback=True, seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATION ====================================\n",
    "# Predict the ratings\n",
    "y_pred = best_model.predict([test_x['user'], test_x['product'], test_x.iloc[:, 3:], test_x['sentiments']])\n",
    "\n",
    "# Rescale the predictions back to the 1-5 range\n",
    "y_pred = y_pred * 4 + 1\n",
    "\n",
    "# set predictions and actual ratings to variables\n",
    "hidden_ratings_array = (np.array(test_y)*4 + 1)\n",
    "predicted_ratings_array = np.array(y_pred).flatten()\n",
    "\n",
    "# Rating predictions\n",
    "mae = mean_absolute_error(hidden_ratings_array, predicted_ratings_array)\n",
    "mse = mean_squared_error(hidden_ratings_array, predicted_ratings_array)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"\\nRating Metrics\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# save results to csv\n",
    "results = pd.DataFrame({'MAE': [mae.round(3)], 'MSE': [mse.round(3)], 'RMSE': [rmse.round(3)]})\n",
    "results.to_csv(\"Data/Results/NCF_results_3.csv\", index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Naive Model\n",
    "\n",
    "We want to build a **naive benchmark model to compare with our NCF model**. The benchmark model will predict the rating of a user-item pair as:\n",
    "\n",
    "1. the most popular rating in the training set. For example, if the most popular rating in the training set is 4, then the benchmark model will predict the rating of all user-item pairs as 4.\n",
    "\n",
    "***TLDR***: The benchmark model, which predicts a constant value (5) for all ratings, outperforms the NCF model in terms of rating metrics and some classification metrics. The NCF model, while providing reasonable results, might need further optimization or tuning to improve its performance, especially in terms of rating prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Model 1 (make it all 5s) ====================================\n",
    "benchmark_results_1 = predicted_ratings_array.copy()\n",
    "benchmark_results_1.fill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate benchmark model 1 ====================================\n",
    "predicted_ratings_array = benchmark_results_1\n",
    "\n",
    "# Rating predictions\n",
    "mae = mean_absolute_error(hidden_ratings_array, predicted_ratings_array)\n",
    "mse = mean_squared_error(hidden_ratings_array, predicted_ratings_array)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"\\nRating Metrics\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# save results to csv\n",
    "results = pd.DataFrame({'MAE': [mae.round(3)], 'MSE': [mse.round(3)], 'RMSE': [rmse.round(3)]})\n",
    "results.to_csv(\"Data/Results/NCF_results_benchmark.csv\", index=False)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
