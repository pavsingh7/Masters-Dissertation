{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Name: Neural Collaborative Based Filtering\n",
    "\n",
    "- using neural network to learn the user-item interaction\n",
    "\n",
    "This piece is a TensorFlow implementation of Neural Collaborative Filtering (NCF) from the paper [He et al. (2017)](https://arxiv.org/pdf/1708.05031.pdf).\n",
    "\n",
    "Summary: NCF uses neural networks to model the interactions between users and items. NCF replaces the inner product (used in ordinary MF methods) with a neural architecture that can learn an arbitrary function from data. This allows NCF to express and generalize matrix factorization under its framework. Essentially, it uses a neural network to learn the user-item interaction function, and uses the learned function to predict the corresponding rating. A multi-layer perceptron is used to learn the user-item interaction function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset space\n",
    "%reset -f\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Example\n",
    "\n",
    "This is a basic implementation of the NCF model. For more advanced features (like adding more layers to the model or using different activation functions), you might need to modify the code accordingly. Also, remember to handle overfitting and underfitting by tuning your model and using techniques like early stopping, regularization, etc.\n",
    "\n",
    "The code below will train a NCF model on your data and then use it to predict the ratings. The predicted ratings will be stored in the y_pred variable. You can adjust the parameters of the model (such as the number of epochs, the batch size, and the dimensions of the embedding layers) to better fit your data\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset space\n",
    "%reset -f\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "# tensorflow libraries\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten, Concatenate, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(r\"C:\\Users\\e1002902\\Documents\\GitHub Repository\\Masters-Dissertation\\Code\\temp_data.csv\")\n",
    "\n",
    "# Melt the DataFrame into a format where each row is a user-item interaction\n",
    "x_melt = x.melt(id_vars=x.columns[0], var_name='book', value_name='rating')\n",
    "\n",
    "# rename columns\n",
    "x_melt.columns = ['user', 'book', 'rating']\n",
    "\n",
    "# Filter out the rows where rating is 0\n",
    "x_melt = x_melt[x_melt['rating'] != 0]\n",
    "x_melt\n",
    "\n",
    "# Convert user and book to categorical\n",
    "x_melt['user'] = x_melt['user'].astype('category')\n",
    "x_melt['book'] = x_melt['book'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user and book embedding layers\n",
    "user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "book_input = Input(shape=(1,), dtype='int32', name='book_input')\n",
    "\n",
    "user_embedding = Embedding(input_dim=len(x_melt['user'].cat.categories), output_dim=50, name='user_embedding')(user_input)\n",
    "book_embedding = Embedding(input_dim=len(x_melt['book'].cat.categories), output_dim=50, name='book_embedding')(book_input)\n",
    "\n",
    "# Flatten the embedding vectors\n",
    "user_vecs = Flatten()(user_embedding)\n",
    "book_vecs = Flatten()(book_embedding)\n",
    "\n",
    "# Concatenate the embedding vectors\n",
    "input_vecs = Concatenate()([user_vecs, book_vecs])\n",
    "\n",
    "# Add dense layers\n",
    "x = Dense(128, activation='relu')(input_vecs)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "y = Dense(1)(x)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[user_input, book_input], outputs=y)\n",
    "model.compile(optimizer=Adam(0.001), loss='mse')\n",
    "\n",
    "# Prepare the data\n",
    "X = x_melt[['user', 'book']].apply(lambda x: x.cat.codes)\n",
    "y = x_melt['rating']\n",
    "\n",
    "# Normalize ratings to be between 0 and 1\n",
    "y = (y - 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.2665 - val_loss: 0.2964\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.2450 - val_loss: 0.2657\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.2045 - val_loss: 0.2354\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1827 - val_loss: 0.2058\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.1603 - val_loss: 0.1771\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.1460 - val_loss: 0.1497\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.1221 - val_loss: 0.1244\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0843 - val_loss: 0.1017\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0952 - val_loss: 0.0824\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0832 - val_loss: 0.0669\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 0.0739 - val_loss: 0.0550\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0868 - val_loss: 0.0466\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.0909 - val_loss: 0.0407\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 0.0994 - val_loss: 0.0366\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0985 - val_loss: 0.0337\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0984 - val_loss: 0.0317\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0866 - val_loss: 0.0307\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0720 - val_loss: 0.0306\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0778 - val_loss: 0.0314\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0799 - val_loss: 0.0334\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0696 - val_loss: 0.0365\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0748 - val_loss: 0.0402\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0653 - val_loss: 0.0442\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0613 - val_loss: 0.0484\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0586 - val_loss: 0.0523\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 0.0546 - val_loss: 0.0555\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0589 - val_loss: 0.0580\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0519 - val_loss: 0.0601\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0566 - val_loss: 0.0614\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0586 - val_loss: 0.0618\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0597 - val_loss: 0.0613\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0541 - val_loss: 0.0601\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0498 - val_loss: 0.0585\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 0.0651 - val_loss: 0.0564\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0501 - val_loss: 0.0536\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0476 - val_loss: 0.0508\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0522 - val_loss: 0.0483\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.0492 - val_loss: 0.0458\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0466 - val_loss: 0.0435\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0505 - val_loss: 0.0414\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0564 - val_loss: 0.0397\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0439 - val_loss: 0.0383\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0419 - val_loss: 0.0375\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0481 - val_loss: 0.0370\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0494 - val_loss: 0.0372\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 0.0574 - val_loss: 0.0378\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0483 - val_loss: 0.0385\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0419 - val_loss: 0.0394\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0419 - val_loss: 0.0403\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0498 - val_loss: 0.0414\n",
      "3/3 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit([X['user'], X['book']], y, batch_size=64, epochs=50, validation_split=0.1)\n",
    "\n",
    "# Predict the ratings\n",
    "y_pred = model.predict([X['user'], X['book']])\n",
    "\n",
    "# Rescale the predictions back to the 1-5 range\n",
    "y_pred = y_pred * 4 + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.665163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.640481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.352252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.186118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.680455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.064956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.895488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "count  68.000000\n",
       "mean    3.665163\n",
       "std     0.640481\n",
       "min     2.352252\n",
       "25%     3.186118\n",
       "50%     3.680455\n",
       "75%     4.064956\n",
       "max     4.895488"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_pred).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiementing\n",
    "\n",
    "Experimenting with different parameters and visualizing the training process can provide valuable insights into how well our model is performing and where improvements can be made.\n",
    "\n",
    "- ***Plot Training and Validation Loss:*** This can help you understand if your model is overfitting or underfitting. If your training loss is much lower than your validation loss, your model might be overfitting. If both losses are high, your model might be underfitting.\n",
    "\n",
    "- ***Experiment with Different Architectures:*** Try adding more layers to your model or increasing the number of neurons in each layer. You could also experiment with different types of layers (e.g., convolutional layers, recurrent layers) and different activation functions.\n",
    "\n",
    "- ***Tune Hyperparameters***: This includes the learning rate, batch size, number of epochs, and regularization parameters. You could use techniques like grid search or random search to systematically explore different combinations of hyperparameters.\n",
    "\n",
    "- ***Use Early Stopping:*** This technique allows you to stop training once the model’s performance on a validation set stops improving, which can be useful to prevent overfitting.\n",
    "\n",
    "- ***Try Different Optimization Algorithms:*** In addition to Adam, there are many other optimization algorithms available in TensorFlow, such as SGD, RMSprop, and Adagrad. Different optimizers might lead to different results.\n",
    "\n",
    "- ***Regularization:*** If your model is overfitting, you might want to add some form of regularization, such as L1 or L2 regularization, or dropout.\n",
    "\n",
    "- ***Data Augmentation***: If you have a small dataset, you could artificially increase its size by creating modified versions of your existing data. For example, you could add small amounts of noise to your input data.\n",
    "\n",
    "- ***Learning Rate Scheduling:*** Instead of using a fixed learning rate, you could decrease it over time or in response to the model’s performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_train['user'], X_train['book']], y_train, batch_size=64, epochs=5, validation_split=0.1)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
