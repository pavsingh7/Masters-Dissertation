{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Data Loading](#toc0_)\n",
    "\n",
    "Here we are looking at taking several samples of the amazon review dataset and loading them into a dataframe.\n",
    "\n",
    "- the key thing to remember is that we need to sample the reviewers and make sure we take all their reviews across all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset (removing all variables, functions, and other objects from memory)\n",
    "%reset -f\n",
    "\n",
    "# get modules in \n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import linecache"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Datasets](#toc0_)\n",
    "\n",
    "We have individual datasets for each category. These data have been reduced to extract the $k$-core, such that each of the remaining users and items have $k$ reviews each.\n",
    "\n",
    "- Amazon Fashion\t\n",
    "- All Beauty\t\n",
    "- Appliances\t\n",
    "- Arts, Crafts and Sewing\t\n",
    "- Automotive\t\n",
    "- Books\t\n",
    "- CDs and Vinyl\t\n",
    "- Cell Phones and Accessories\t\n",
    "- Clothing, Shoes and Jewelry\t\n",
    "- Digital Music\t\n",
    "- Electronics\t\n",
    "- Gift Cards\t\n",
    "- Grocery and Gourmet Food\t\n",
    "- Home and Kitchen\t\n",
    "- Industrial and Scientific\t\n",
    "- Kindle Store\t\n",
    "- Luxury Beauty\t\n",
    "- Magazine Subscriptions\t\n",
    "- Movies and TV\t\n",
    "- Musical Instruments\t\n",
    "- Office Products\t\n",
    "- Patio, Lawn and Garden\t\n",
    "- Pet Supplies\t\n",
    "- Prime Pantry\t\n",
    "- Software\t\n",
    "- Sports and Outdoors\t\n",
    "- Tools and Home Improvement\t\n",
    "- Toys and Games\t\n",
    "- Video Games\t\n",
    "\n",
    "***\n",
    "\n",
    "### <a id='toc2_1_1_'></a>[Review Dataset](#toc0_)\n",
    "Format is one-review-per-line in json. \n",
    "\n",
    "- **overall**: ratings of the product\n",
    "- **reviewerID**: ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- **asin**: ID of the product, e.g. 0000013714\n",
    "- **reviewerName**: name of the reviewer\n",
    "- **vote**: helpful votes of the review\n",
    "- **style**: a disctionary of the product metadata, e.g., \"Format\" is \"Hardcover\"\n",
    "- **reviewText**: text of the review\n",
    "- **summary**: summary of the review\n",
    "- **unixReviewTime**: time of the review (unix time)\n",
    "- **reviewTime**: time of the review (raw)\n",
    "- **image**: images that users post after they have received the product\n",
    "\n",
    "***\n",
    "### <a id='toc2_1_2_'></a>[Product Metadata Dataset](#toc0_)\n",
    "We also have metadata. \n",
    "\n",
    "- **asin**: ID of the product, e.g. 0000031852\n",
    "- **title**: name of the product\n",
    "- **feature**: bullet-point format features of the product\n",
    "- **description**: description of the product\n",
    "- **price**: price in US dollars (at time of crawl)\n",
    "- **imageURL**: url of the product image\n",
    "- **imageURL**: url of the high resolution product image\n",
    "- **related**: related products (also bought, also viewed, bought together, buy after viewing)\n",
    "- **salesRank**: sales rank information\n",
    "- **brand**: brand name\n",
    "- **categories**: list of categories the product belongs to\n",
    "- **tech1**: the first technical detail table of the product\n",
    "- **tech2**: the second technical detail table of the product\n",
    "- **similar**: similar product table\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <a id='toc4_'></a>[The Review Dataset and Metadata Dataset](#toc0_)\n",
    "\n",
    "We have individual datasets for each category. We combine them to generate one larger datasets encompassing all the categories (5-core dataset).\n",
    "\n",
    "The following function is created to read in large JSON files:\n",
    "\n",
    "``` py\n",
    "def read_file(filename, category):\n",
    "    num_lines = sum(1 for line in open(filename))\n",
    "    selected_lines = set()\n",
    "    while len(selected_lines) < min(50000, num_lines):\n",
    "        line_num = random.randint(1, num_lines)\n",
    "        if line_num not in selected_lines:\n",
    "            selected_lines.add(line_num)\n",
    "            line = linecache.getline(filename, line_num)\n",
    "            selected_data = json.loads(line)\n",
    "            selected_data['category'] = category\n",
    "            yield selected_data\n",
    "```\n",
    "\n",
    "1. It calculates the total number of lines in the file using the `sum(1 for line in open(filename))` expression.\n",
    "2. It initializes an empty set called `selected_lines`, which will **store the line numbers that have been selected**.\n",
    "3. It enters a loop that continues until the number of selected lines reaches the minimum value between 500,000 and the total number of lines in the file (`min(500000, num_lines)`).\n",
    "4. Within each iteration of the loop, it generates a random line number using `random.randint(1, num_lines)`.\n",
    "5. If the randomly generated line number is not already in the `selected_lines` set, it adds the line number to the set and proceeds to read that specific line from the file using `linecache.getline(filename, line_num)`.\n",
    "6. The selected line is then parsed as JSON using `json.loads(line)`.\n",
    "7. Additional data, such as the **category**, is added to the selected data object.\n",
    "8. The selected data object is yielded, which means it will be returned as an element of an iterator.\n",
    "9. The loop continues until the desired number of lines is selected.\n",
    "\n",
    "The function defined as:\n",
    "\n",
    "```py\n",
    "def read_matching_metadata(filename, category, product_ids):\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if data['asin'] in product_ids:\n",
    "                data['category'] = category\n",
    "                yield data\n",
    "```\n",
    "\n",
    "Reads a JSON file and yields metadata entries that match a given set of product IDs. \n",
    "- `read_matching_metadata` is a function that takes three parameters: `filename`, `category`, and `product_ids`.\n",
    "- It opens the specified filename (assumed to be a JSON file) in read mode using a with statement, which ensures the file is properly closed after reading.\n",
    "- It iterates over each line in the file using a for loop.\n",
    "- For each line, it loads the line as a JSON object using `json.loads(line)`.\n",
    "- It checks if the value of the '`asin`' key in the loaded JSON data is present in the `product_ids` set.\n",
    "- If there is a match, it adds the '`category`' key to the data dictionary and assigns it the value of the `category` parameter.\n",
    "- Finally, it yields the modified data using the `yield` statement, allowing the caller to iterate over the matching metadata entries one by one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review data\n",
    "def read_file(filename, category):\n",
    "    num_lines = sum(1 for line in open(filename))\n",
    "    selected_lines = set()\n",
    "    while len(selected_lines) < min(500000, num_lines):\n",
    "        line_num = random.randint(1, num_lines)\n",
    "        if line_num not in selected_lines:\n",
    "            selected_lines.add(line_num)\n",
    "            line = linecache.getline(filename, line_num)\n",
    "            selected_data = json.loads(line)\n",
    "            selected_data['category'] = category\n",
    "            yield selected_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Data with Fewer Reviews](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise data list\n",
    "data = []\n",
    "\n",
    "# category files - smaller reviews\n",
    "beauty = \"/Users/pavansingh/Desktop/Amazon Review Data/All_Beauty_5.json\"\n",
    "fashion = \"/Users/pavansingh/Desktop/Amazon Review Data/AMAZON_FASHION_5.json\"\n",
    "appliances = \"/Users/pavansingh/Desktop/Amazon Review Data/Appliances_5.json\"\n",
    "gift_cards = \"/Users/pavansingh/Desktop/Amazon Review Data/Gift_Cards_5.json\"\n",
    "industrial = \"/Users/pavansingh/Desktop/Amazon Review Data/Industrial_and_Scientific_5.json\"\n",
    "luxury_beauty = \"/Users/pavansingh/Desktop/Amazon Review Data/Luxury_Beauty_5.json\"\n",
    "magazine_subscriptions = \"/Users/pavansingh/Desktop/Amazon Review Data/Magazine_Subscriptions_5.json\"\n",
    "software = \"/Users/pavansingh/Desktop/Amazon Review Data/Software_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('beauty', beauty), ('fashion', fashion), ('appliances', appliances), ('gift_cards', gift_cards), ('industrial', industrial), ('luxury_beauty', luxury_beauty), ('magazine_subscriptions', magazine_subscriptions), ('software', software)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data_with_less_reviews = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data_with_less_reviews.shape)\n",
    "display(data_with_less_reviews.head(5))\n",
    "\n",
    "# save data_with_less_reviews to csv called few_revs.csv in folder Data\n",
    "data_with_less_reviews.to_csv('Data/few_revs_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data_with_less_reviews['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Data with A Lot of Reviews](#toc0_)\n",
    "\n",
    "We split this up into 9 batches and load them seperately as the metadata is quite large and takes up a lot of memory.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_2_1_1_'></a>[Batch 1](#toc0_)\n",
    "\n",
    "- arts_crafts_and_sewing\n",
    "- automotive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the review data!\n",
    "\n",
    "data = []\n",
    "\n",
    "arts_crafts = \"/Users/pavansingh/Desktop/Amazon Review Data/Arts_Crafts_and_Sewing_5.json\"\n",
    "automotive = \"/Users/pavansingh/Desktop/Amazon Review Data/Automotive_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('arts_crafts', arts_crafts), ('automotive', automotive)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data.shape)\n",
    "display(data.head(5))\n",
    "\n",
    "# save data_with_less_reviews to csv called few_revs.csv in folder Data\n",
    "data.to_csv('Data/revs_batch1_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <a id='toc4_2_1_2_'></a>[Batch 2](#toc0_)\n",
    "\n",
    "- cds_and_vinyl\n",
    "- cell_phones_and_accessories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the review data!\n",
    "\n",
    "data = []\n",
    "\n",
    "cds_and_vinyl = \"/Users/pavansingh/Desktop/Amazon Review Data/CDs_and_Vinyl_5.json\"\n",
    "cell_phones = \"/Users/pavansingh/Desktop/Amazon Review Data/Cell_Phones_and_Accessories_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('cds_and_vinyl', cds_and_vinyl), ('cell_phones', cell_phones)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data.shape)\n",
    "display(data.head(5))\n",
    "\n",
    "# save data_with_less_reviews to csv called few_revs.csv in folder Data\n",
    "data.to_csv('Data/revs_batch2_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### <a id='toc4_2_1_3_'></a>[Batch 3](#toc0_)\n",
    "\n",
    "- clothing_shoes_and_jewelry\n",
    "- digital_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the review data!\n",
    "\n",
    "data = []\n",
    "\n",
    "clothing_shoes_and_jewelry = \"/Users/pavansingh/Desktop/Amazon Review Data/Clothing_Shoes_and_Jewelry_5.json\"\n",
    "digital_music = \"/Users/pavansingh/Desktop/Amazon Review Data/Digital_Music_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('clothing_shoes_and_jewelry', clothing_shoes_and_jewelry), ('digital_music', digital_music)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data.shape)\n",
    "display(data.head(5))\n",
    "\n",
    "# save data in folder Data\n",
    "data.to_csv('Data/revs_batch3_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc4_2_2_'></a>[Batch 4](#toc0_)\n",
    "\n",
    "- electronics\n",
    "- musical_instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the review data!\n",
    "\n",
    "data = []\n",
    "\n",
    "electronics = \"/Users/pavansingh/Desktop/Amazon Review Data/Electronics_5.json\"\n",
    "musical_instruments = \"/Users/pavansingh/Desktop/Amazon Review Data/Musical_Instruments_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('electronics', electronics), ('musical_instruments', musical_instruments)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data.shape)\n",
    "display(data.head(5))\n",
    "\n",
    "# save data in folder Data\n",
    "data.to_csv('Data/revs_batch4_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc4_2_3_'></a>[Batch 5](#toc0_)\n",
    "\n",
    "- office_products\n",
    "- patio_lawn_and_garden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the review data!\n",
    "\n",
    "data = []\n",
    "\n",
    "office_products = \"/Users/pavansingh/Desktop/Amazon Review Data/Office_Products_5.json\"\n",
    "patio_lawn_and_garden = \"/Users/pavansingh/Desktop/Amazon Review Data/Patio_Lawn_and_Garden_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('office_products', office_products), ('patio_lawn_and_garden', patio_lawn_and_garden)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data.shape)\n",
    "display(data.head(5))\n",
    "\n",
    "# save data in folder Data\n",
    "data.to_csv('Data/revs_batch5_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc4_2_4_'></a>[Batch 6](#toc0_)\n",
    "\n",
    "- sports_and_outdoors\n",
    "- video_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the review data!\n",
    "\n",
    "data = []\n",
    "\n",
    "sports_and_outdoors = \"/Users/pavansingh/Desktop/Amazon Review Data/Sports_and_Outdoors_5.json\"\n",
    "video_games = \"/Users/pavansingh/Desktop/Amazon Review Data/Video_Games_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('sports_and_outdoors', sports_and_outdoors), ('video_games', video_games)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data.shape)\n",
    "display(data.head(5))\n",
    "\n",
    "# save data in folder Data\n",
    "data.to_csv('Data/revs_batch6_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc4_2_5_'></a>[Batch 7](#toc0_)\n",
    "\n",
    "- tools_and_home_improvement\n",
    "- kindle_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the review data!\n",
    "\n",
    "data = []\n",
    "\n",
    "tools_and_home_improvement = \"/Users/pavansingh/Desktop/Amazon Review Data/Tools_and_Home_Improvement_5.json\"\n",
    "kindle_store = \"/Users/pavansingh/Desktop/Amazon Review Data/Kindle_Store_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('tools_and_home_improvement', tools_and_home_improvement), ('kindle_store', kindle_store)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data.shape)\n",
    "display(data.head(5))\n",
    "\n",
    "# save data in folder Data\n",
    "data.to_csv('Data/revs_batch7_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_6_'></a>[Batch 8](#toc0_)\n",
    "\n",
    "- toys_and_games\n",
    "- prime_pantry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the review data!\n",
    "\n",
    "data = []\n",
    "\n",
    "toys_and_games = \"/Users/pavansingh/Desktop/Amazon Review Data/Toys_and_Games_5.json\"\n",
    "prime_pantry = \"/Users/pavansingh/Desktop/Amazon Review Data/Prime_Pantry_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('toys_and_games', toys_and_games), ('prime_pantry', prime_pantry)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data.shape)\n",
    "display(data.head(5))\n",
    "\n",
    "# save data in folder Data\n",
    "data.to_csv('Data/revs_batch8_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_7_'></a>[Batch 9](#toc0_)\n",
    "\n",
    "- home_and_kitchen\n",
    "- movies_and_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the review data!\n",
    "\n",
    "data = []\n",
    "\n",
    "home_and_kitchen = \"/Users/pavansingh/Desktop/Amazon Review Data/Home_and_Kitchen_5.json\"\n",
    "movies_and_tv = \"/Users/pavansingh/Desktop/Amazon Review Data/Movies_and_TV_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('home_and_kitchen', home_and_kitchen), ('movies_and_tv', movies_and_tv)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data.shape)\n",
    "display(data.head(5))\n",
    "\n",
    "# save data in folder Data\n",
    "data.to_csv('Data/revs_batch9_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc4_2_8_'></a>[Batch 10](#toc0_)\n",
    "\n",
    "- pet_supplies\n",
    "- grocery_and_gourmet_food\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the review data!\n",
    "\n",
    "data = []\n",
    "\n",
    "pet_supplies = \"/Users/pavansingh/Desktop/Amazon Review Data/Pet_Supplies_5.json\"\n",
    "grocery_and_gourmet_food = \"/Users/pavansingh/Desktop/Amazon Review Data/Grocery_and_Gourmet_Food_5.json\"\n",
    "\n",
    "# load each file and join into dataframe\n",
    "for category, filename in [('pet_supplies', pet_supplies), ('grocery_and_gourmet_food', grocery_and_gourmet_food)]:\n",
    "    for selected_data in read_file(filename, category):\n",
    "        data.append(selected_data)\n",
    "\n",
    "# make it into a dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# show the dataframe\n",
    "print(\"Shape of all data:\", data.shape)\n",
    "display(data.head(5))\n",
    "\n",
    "# save data in folder Data\n",
    "data.to_csv('Data/revs_batch10_1.csv')\n",
    "\n",
    "# category value counts\n",
    "print(\"Value counts of product reviews per category:\\n\",data['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_9_'></a>[Merge Batches (for large reviews data)](#toc0_)\n",
    "\n",
    "In this section, we merge the batches together to create one large dataset.\n",
    "\n",
    "We use the `pd.concat()` function to merge the batches together. The resulting dataset is saved as a CSV file for use in the next section - **data cleaning**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and merge csv files\n",
    "df1 = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/revs_batch1_1.csv\", low_memory=False)\n",
    "df2 = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/revs_batch2_1.csv\", low_memory=False)\n",
    "df3 = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/revs_batch3_1.csv\", low_memory=False)\n",
    "df4 = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/revs_batch4_1.csv\", low_memory=False)\n",
    "df5 = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/revs_batch5_1.csv\", low_memory=False)\n",
    "df6 = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/revs_batch6_1.csv\", low_memory=False)\n",
    "df7 = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/revs_batch7_1.csv\", low_memory=False)\n",
    "df8 = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/revs_batch8_1.csv\", low_memory=False)\n",
    "df9 = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/revs_batch9_1.csv\", low_memory=False)\n",
    "df10 = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/revs_batch10_1.csv\", low_memory=False)\n",
    "\n",
    "# merge all dataframes\n",
    "frames = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10]\n",
    "lots_revs_meta = pd.concat(frames)\n",
    "\n",
    "# save to csv\n",
    "lots_revs_meta.to_csv('Data/lots_revs_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the data\n",
    "print(\"Shape of all data:\", lots_revs_meta.shape)\n",
    "display(lots_revs_meta.head(3))\n",
    "\n",
    "# value counts\n",
    "print(\"\\nValue counts of product reviews per category:\\n\",lots_revs_meta['category'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <a id='toc4_3_'></a>[Merge Large Reviews with Few Reviews](#toc0_)\n",
    "\n",
    "We now have two CSV files:\n",
    "\n",
    "1. `few_revs_meta.csv`: contains the metadata for products with fewer reviews\n",
    "2. `lots_revs_meta.csv`: contains the metadata for products with a lot of reviews\n",
    "\n",
    "We merge these two datasets together to create one large dataset that we will use for data cleaning and the subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and merge csv files\n",
    "few = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/few_revs_1.csv\", low_memory=False)\n",
    "#lots = pd.read_csv(\"/Users/pavansingh/Library/CloudStorage/GoogleDrive-pavansingho23@gmail.com/My Drive/Portfolio/Masters-Dissertation/Code/Data/lots_revs_1.csv\", low_memory=False)\n",
    "lots = lots_revs_meta\n",
    "\n",
    "# merge all dataframes\n",
    "frames = [few, lots]\n",
    "all_revs_meta = pd.concat(frames)\n",
    "all_revs_meta = all_revs_meta.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "\n",
    "# save to csv\n",
    "all_revs_meta.to_csv('Data/all_revs_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Keep Reviewers with more than X reviews and Products with X reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all_revs\n",
    "all_revs = pd.read_csv(\"/Users/pavansingh/Desktop/all_revs_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the data\n",
    "all_revs.head(3)\n",
    "print(all_revs.shape)\n",
    "print(\"number of unique products:\", all_revs['asin'].nunique())\n",
    "print(\"number of unique reviewers:\", all_revs['reviewerID'].nunique())\n",
    "\n",
    "test = all_revs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates  reviews\n",
    "# ===  # === \n",
    "\n",
    "test = test[test['asin'].isin(test.groupby('asin').size().reset_index(name='counts').query('counts >= 10')['asin'])]\n",
    "test = test[test['reviewerID'].isin(test.groupby('reviewerID').size().reset_index(name='counts').query('counts >= 10')['reviewerID'])]\n",
    "test = test[test['asin'].isin(test.groupby('asin').size().reset_index(name='counts').query('counts >= 10')['asin'])]\n",
    "\n",
    "# shape\n",
    "print(test.shape)\n",
    "\n",
    "# show number of ratings per reviewer in table\n",
    "display(test.groupby('reviewerID').size().reset_index(name='counts').sort_values('counts', ascending=True).head(5))\n",
    "\n",
    "# show number of ratings per product in table\n",
    "display(test.groupby('asin').size().reset_index(name='counts').sort_values('counts', ascending=True).head(5))\n",
    "\n",
    "# save to csv\n",
    "test.to_csv('Data/set1_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates reviews\n",
    "# ===  # === \n",
    "\n",
    "test = test[test['asin'].isin(test.groupby('asin').size().reset_index(name='counts').query('counts >= 12')['asin'])]\n",
    "test = test[test['reviewerID'].isin(test.groupby('reviewerID').size().reset_index(name='counts').query('counts >= 12')['reviewerID'])]\n",
    "test = test[test['asin'].isin(test.groupby('asin').size().reset_index(name='counts').query('counts >= 12')['asin'])]\n",
    "\n",
    "# shape\n",
    "print(test.shape)\n",
    "\n",
    "# show number of ratings per reviewer in table\n",
    "display(test.groupby('reviewerID').size().reset_index(name='counts').sort_values('counts', ascending=True).head(5))\n",
    "\n",
    "# show number of ratings per product in table\n",
    "display(test.groupby('asin').size().reset_index(name='counts').sort_values('counts', ascending=True).head(5))\n",
    "\n",
    "# save to csv\n",
    "test.to_csv('Data/set2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates reviews\n",
    "# ===  # === \n",
    "\n",
    "test = test[test['asin'].isin(test.groupby('asin').size().reset_index(name='counts').query('counts >= 14')['asin'])]\n",
    "test = test[test['reviewerID'].isin(test.groupby('reviewerID').size().reset_index(name='counts').query('counts >= 14')['reviewerID'])]\n",
    "test = test[test['asin'].isin(test.groupby('asin').size().reset_index(name='counts').query('counts >= 14')['asin'])]\n",
    "\n",
    "# shape\n",
    "print(test.shape)\n",
    "\n",
    "# show number of ratings per reviewer in table\n",
    "display(test.groupby('reviewerID').size().reset_index(name='counts').sort_values('counts', ascending=True).head(5))\n",
    "\n",
    "# show number of ratings per product in table\n",
    "display(test.groupby('asin').size().reset_index(name='counts').sort_values('counts', ascending=True).head(5))\n",
    "\n",
    "# save to csv\n",
    "test.to_csv('Data/set3_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates reviews\n",
    "# ===  # === \n",
    "\n",
    "test = test[test['asin'].isin(test.groupby('asin').size().reset_index(name='counts').query('counts >= 20')['asin'])]\n",
    "test = test[test['reviewerID'].isin(test.groupby('reviewerID').size().reset_index(name='counts').query('counts >= 20')['reviewerID'])]\n",
    "test = test[test['asin'].isin(test.groupby('asin').size().reset_index(name='counts').query('counts >= 20')['asin'])]\n",
    "\n",
    "# shape\n",
    "print(test.shape)\n",
    "\n",
    "# show number of ratings per reviewer in table\n",
    "display(test.groupby('reviewerID').size().reset_index(name='counts').sort_values('counts', ascending=True).head(5))\n",
    "\n",
    "# show number of ratings per product in table\n",
    "display(test.groupby('asin').size().reset_index(name='counts').sort_values('counts', ascending=True).head(5))\n",
    "\n",
    "# save to csv\n",
    "test.to_csv('Data/set4_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the data\n",
    "print(\"Shape of all data:\", test.shape) #4 164 059\n",
    "display(test.head(3))\n",
    "\n",
    "# value counts\n",
    "print(\"\\nValue counts of product reviews per category:\\n\",test['category'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
