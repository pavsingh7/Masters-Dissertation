% Chapter Template

\chapter{Model Specification and Analysis} % Main chapter title

\label{Chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}



\begin{figure}[h]
\centering
  \includegraphics[scale =0.9]{/Users/pavansingh/Google Drive (UCT)/STA Honours/Project/Thesis/Python Coding/Figures/Methods/Untitled Diadsdsdsgram.pdf}
  \caption{Diagram illustrating the models and whole experimental procedure followed.}
  \label{}
\end{figure}

All 42 variables, including mix of stock data (open, high, low, close and volume), as well several technical indicators. were used as inputs to our. models. Our feed-forward and long-short term memory neural networks were configured as classification models, while our ensemble trees, where configured as regression problems. As such, since we want to assess these models on their ability to predict the movements of daily log returns of stocks, we had to convert our predictions from our ensemble models to binary output. This was done, by coding returns equal to or greater than zero as one and any negative returns as zero.

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Model Architectures}

Hyper-parameters are the parameters that are set prior to the learning process (Nelson \textit{et al.}, 2017). They are not derived via training. Hyper-parameter optimisation allows for improving and - in some cases - correcting an algorithmâ€™s performance. One of the main characteristics to consider when optimising hyper-parameters is the model's complexity (Nti \textit{et al.}, 2020). If the model is over-complex, it might be prone to overfitting and thus will not generalise well. On the other hand, if it is too simplistic, the model might not fit the data correctly or be able to capture the relationship between input and output sufficiently under-fitting (Nelson \textit{et al.}, 2017).

There are several approaches to find the model hyper-parameters that yield the best score on the validation set metric. In our paper, we opted for using a grid search and in some cases,  manually searching for optimal hyper-parameters. 

\subsection{Long Short-Term Memory Neural Network}

The network is based on a sequential model. We used a cross validation grid search \footnote{Grid Search CV tries all combinations of parameters grid for a model and returns with the best set of parameters having the best performance score} from Scikit-Learn module to try out several values for our hyper-parameters and compare the results. 

Hyper-parameters tuned with grid search, with the accompanied combination of values tried, is shown in the table below.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
Hyper-parameter     &  Value \\
\hline
Batch Size      & 4, 12, 24 \\
Dropout & 0.2, 0.3, 0.4 \\
Learning Rate & 0.1, 0.01, 0.001 \\
LSTM Units & 20, 50, 100 \\
Dense Units & 1, 5, 20\\
\hline
\end{tabular}
\caption{Hyper-Parameters and their corresponding combination of values used in grid search for our LSTM.}
\end{table}

\begin{itemize}
\item  \verb|batch_size|: defines the number of samples that will be propagated through the network
\item  \verb|dropout|: a regularisation technique, which helps with reducing overfitting
\item  \verb|LSTM units|:  number of neurons in a LSTM layer
\item  \verb|Dense units|:  number of neurons in dense layers
\item \verb|Learning Rate|: controls how much to change the model in response to the estimated error each time the model weights are updated
\end{itemize}

Our final model has two LSTM layers, each followed by a dropout layer with dropout value of 0.2. Dropout is a regularisation technique for neural network, where randomly selected neurons are ignored during training. As such, the contribution of those selected neurons to the activation of downstream neurons is temporally removed on the forward pass. Thus any weight updates are not applied to the neuron on the backward pass. Ultimately,  this helps in reducing the possibility of over-fitting the network (Nelson \textit{et al.}, 2017). In addition, we have two dense layers in our model architecture. The dense layer is the most frequently used layer which is basically a layer where each neuron receives input from all neurons in the previous layer (Nelson \textit{et al.}, 2017). 

The loss function used was the binary cross entropy function - as it was appropriate the type of problem we have - supervised binary classification. With regards to the number of neurons (units) in the hidden layers, there is no straightforward approach one should use. We chose to grid search with 20, 50 and 100 units in the LSTM layers. The final model has 100 units in the LSTM layer.  

With respect to the models fit and architecture, we also consider the activation function and choice of optimiser. We used both a rectified linear unit (ReLU) and sigmoid activation function in the final model architecture. The ReLU activation function is specified for all except the output layer. The sigmoid activation function is used in the final output layer, satisfying our binary classification solution. The optimiser used for this model was stochastic gradient descent (SGD). 

Our grid search results suggested we use a learning rate of 0.01. We note that setting a higher learning rate accelerates the learning but the model may not converge. Conversely, a lower rate will slow down the learning drastically as steps towards the minimum of loss function will be tiny, but will allow the model to converge smoothly.  Finally, we specified 100 epochs \footnote{the number of iterations (forward and back propagation) our model needs to make.} and set a stopping method, which stops training once the model performance stops improving by a pre-set threshold on the validation set. The final model architecture is summarised in the table below:

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
Hyper-parameter     &  Value \\
\hline
Epochs     & 100  \\
Batch Size      & 12 \\
Dropout & 0.2 \\
Dropout Layers & 2 \\
Activation Function & ReLU and Sigmoid \\
Learning Rate & 0.01 \\
Optimiser & Binary Cross Entropy \\
LSTM Units & 100\\
LSTM Layers & 2\\
Dense Layers & 2\\
Dense Units & 5 and 1 \\
\hline
\end{tabular}
\caption{Chosen hyper-parameters for LSTM. We used a grid search with cross validation to select the number of units in the LSTM layer, the dropout, learning rate and batch size.}
\end{table}


A rectified linear unit (ReLU) function is written as: 
\begin{equation}
f(x)=\left\{\begin{array}{ll}
0 & \text { if } x \leq 0 \\
x & \text { otherwise }
\end{array}\right.
\end{equation}

The ReLU activates a node with the same input value only when the input is above zero.




\subsection{Feed-Forward Neural Network}

The hyper-parameters for our feed-forward neural network (FFNN), are similar to that of our LSTM model. However, we do not have dropout layer nor do we have LSTM layers in our architecture for the FFNN.  We again used a grid search with cross validation from Scikit-Learn module to try out several values for our hyper-parameters and compare the results. The combination of values in the grid are shown in Table 5.3.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
Hyper-parameter     &  Value \\
\hline
Batch Size      & 4, 12, 24 \\
Learning Rate & 0.1, 0.01, 0.005, 0.001 \\
Dense Units & 1, 10, 20, 50\\
\hline
\end{tabular}
\caption{Hyper-Parameters and their corresponding combination of values used in grid search for our FFNN.}
\end{table}


For our optimisation algorithm, we again opted for SGD. We tuned the learning rate, and found that 0.01 produced the best model performance on the validation set. The selection of the rest of our hyper-parameters, as well as the final model architecture is shown in table 5.4. 



\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
Hyper-parameter     &  Value \\
\hline
Epochs       & 100  \\
Batch Size      & 0.01 \\
Activation Function & 1 (default) \\
Learning Rate & 1 (default) \\
Optimiser & 1 (default) \\
Dense Layers & 2\\
Dense Units & 5, 1\\
\hline
\end{tabular}
\caption{Chosen Hyper-Parameters for FFNN. We used a grid search with cross validation to select the number of units in the dense layers, the learning rate and batch size.}
\end{table}


\subsection{Random Forest}

The hyper-parameters for our random forest were found by implementing a grid search with cross validation as well. The hyper-parameters we are concerned with for this model include:
\begin{itemize}
\item \verb|n_estimators|: the number of decision trees in the forest
\item  \verb|max_features|: max number of features considered for splitting a node
\item  \verb|max_depth|: max number of levels in each decision tree
\item  \verb|min_samples_split|:  number of data points placed in a node before the node is split
\end{itemize}

By building forests with a large number of trees (high number of estimators) we can create a more robust aggregate model with less variance. This comes at the cost of a greater training time (Ampomah \textit{et al.}, 2020). We also note that due to the randomness of Random Forests, if you have a lot of features and a small number of trees some features with high predictive power could get left out of the forest and not be used whatsoever (Ampomah \textit{et al.}, 2020). 

With respect to the maximum features considered at each node split, we have to be careful in deciding upon a value.  A small value (less features considered when splitting at each node) will reduce the variance of the ensemble, at the cost of higher individual tree bias (Nti \textit{et al.}, 2020). Increasing the maximum number of random features considered in a split tends to decrease the bias of the model, as there is a better chance that good features will be included, however this can come at the cost of increased variance. Given that we have many features we chose a combination of values for this parameter, shown in figure 5.2. For max depth of trees, if we increase this value,  this would increase the possible number of feature combinations that are taken into account. The deeper the tree, the more splits it has and the more information about the data it takes into account (Ampomah \textit{et al.}, 2020).

\begin{figure}[h]
\centering
  \includegraphics[scale =0.37]{/Users/pavansingh/Google Drive (UCT)/STA Honours/Project/Thesis/Python Coding/Figures/RF AND BOOST/VAL/RandomForestGRID.pdf}
  \caption{Grid search results using different combination of values for hyper-parameters of Random Forest. We select a max depth of 20 and minimum sample split of 2.}
  \label{}
\end{figure}

Figure 5.2 depicts the optimal hyper-parameters for our Random Forest model on the validation set. It can be observed, that as the maximum depth of the decision tree increases and as minimum sample leaf decreases, the algorithm tend to overfit. This results in an optimistic training-validation set performance and decreased generalisation performance on the test set. Thus, in order to obtain the best performing model, we need to determine both the optimal tree depth as well as the minimum sample leaves at the same time. After hyper-parameter tuning, we were able to obtain the parameters for our final Random Forest model. These can be seen in table 5.5.

 
\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
Hyper-parameter   &  Value \\
\hline
Max Number Estimators       & 250  \\
Max Depth & 20 \\
Min Samples Split & 2 \\
\hline
\end{tabular}
\caption{Chosen model hyper-parameters for Random Forest model. We found the minimum samples split and max depth, by implementing a cross validation grid search.}
\end{table}




\subsection{AdaBoost Model}

Similar to our Random Forest, we need to specify the number of estimators for our AdaBoost algorithm. That is, the maximum number of estimators (trees) at which boosting is terminated. In contrast to Random Forests, we have to specify a learning rate - how fast the tree learns. 

We also have to specify the base estimator. The default value is none, which equates to a decision tree with max depth of 1 (a stump). For the hyper-parameters of our AdaBoost model we applied a grid search over a combination of values. The results of which are shown in figure 5.3. 

\begin{figure}[h]
\centering
  \includegraphics[scale =0.34]{/Users/pavansingh/Google Drive (UCT)/STA Honours/Project/Thesis/Python Coding/Figures/RF AND BOOST/VAL/Best_Model_Boosting_2.pdf}
  \caption{Grid Search Results using Different Combination of Values for Hyper-parameters of Adaboost Model. The chosen hyper-parameters, were a max depth of ten and minimum samples leaf of ten.}
  \label{}
\end{figure}

We chose to vary the learning rate to penalise our model against overfitting. The results of using a learning rate of 0.01 and 1 are shown in the appendix C. After hyper-parameter tuning, we were able to obtain the parameters for our final model. These can be seen in the Table 5.6.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
Hyper-parameter     &  \\
\hline
Max Number Estimators       & 250  \\
Learning Rate        & 0.01 \\
Max Depth & 1 (default) \\
\hline
\end{tabular}
\caption{Chosen Hyper-Parameters for AdaBoost Model}
\end{table}

For both ensemble decision tree models, we constructed regression trees. We predicted the daily log eturns to infer the movement of the stocks. Our binary prediction was set such that zero indicated a negative return and a one indicated a positive return.


\newpage
\subsection{Auto Regressive Integrated Moving Average}

The modelling procedure for fitting our ARIMA model is as follows (): 

\begin{itemize}
\item Plot the data and identify any unusual observations.
\item If necessary, transform the data (using a Box-Cox transformation) to stabilise the variance.
\item If the data are non-stationary, take first differences of the data until the data are stationary.
\item Examine the ACF/PACF: Is an ARIMA $(p, d, 0)$ or ARIMA $(0, d, q)$ model appropriate?
\item Try your chosen model(s), and use the AIC to search for a better model.
\item Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.
\item Once the residuals look like white noise, calculate forecasts.
\end{itemize}

As such, we used AIC to determine the order of the ARIMA model (Binner \textiot{et al.}, 2005). AIC is written as: 

\begin{equation}
\mathrm{AIC}=-2 \log (L)+2(p+q+k+1), 
\end{equation}

where $L$ is the likelihood of the data, $k=1$ if $c \neq 0$ and $k=0$ if $c=0$. Good models are obtained by minimising the AIC. 
We looked at a combination of different p and q values. The results of our analysis are shown in the table below:

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
Model Specification & AIC        \\
\hline
(1, 0, 1)           & -10549.128 \\
(1, 0, 0)           & -10545.228 \\
(2, 0, 0)           & -10551.196 \\
(2, 0, 2)           & -10551.033 \\
(3, 0, 1)           & -10548.880 \\
(2, 0, 1)           & -10554.112 \\
\hline
\end{tabular}
\caption{Combination of different ARIMA model specifications used to find the optimal architecture}
\end{table}

Of these models shown in Table 5.7, the ARIMA(2,0,1) has the smallest AIC value, hence it is our chosen model. Figure 5.3 shows the residual diagnostic plots from the ARIMA(2,0,1) model. 

\begin{figure}[h]
\centering
  \includegraphics[scale =0.34]{/Users/pavansingh/Google Drive (UCT)/STA Honours/Project/Thesis/Python Coding/Figures/ARIMA/DiagnosticsARIMA_301_FORD.pdf
}
  \caption{Model diagnostics for the ARIMA(2,0,1) model.}
  \label{}
\end{figure}

We see from the plot in the top right of figure 5.4, that. the residuals have a mean close to 0.  Pots in figure 5.4 shows that all autocorrelations are within the threshold limits, indicating that the residuals are behaving like white noise. Any autocorrelation would imply that there is some pattern in the residual errors which are not explained in the model. 

We note that the QQ-plot indicates that the distribution of the residuals deviate from normality in the tails.  We apply the Box-Ljung test \footnote{Tests whether any of a group of autocorrelations of a time series are different from zero. Null hypothesis is that the data are random.} to the residuals from the ARIMA(2,0,1) model fit to determine whether residuals are random. We received a p-value of 0.008 indicating that the residuals are random and that the model provides an adequate fit to the data. The top left plot in figure 5.4, illustrates the standardised residuals.  These residual errors seem to fluctuate around a mean of zero and for the most part have a uniform variance. 


%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Validation Analysis}

The validation data set provides an un-biased evaluation of a model fit on the training data set while tuning the model's hyper-parameters. Using our optimised models, we illustrate the validation curves for our LSTM and FFNN models on the Ford Motors stock.

The loss (or cost) measures our model error. Specifically, the training loss indicates how well the model is fitting the training data, while the validation loss indicates how well the model fits new data.  

%\begin{figure}[h]
%\centering
%  \includegraphics[scale =0.30]{/Users/pavansingh/Google Drive (UCT)/STA Honours/Project/Thesis/Python Coding/Figures/ANN/LOSS/ModelLoss_ANN_Google.pdf}
%  \caption{Loss Curve for Tuned LSTM Network on Ford Data.}
  %\label{}
%\end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Multivariate LSTM]{\label{fig:2:a}\includegraphics[width=0.5\textwidth]{/Users/pavansingh/Google Drive (UCT)/STA Honours/Project/Thesis/Python Coding/Figures/ANN/LOSS/ModelLoss_ANN_Google.pdf}}
    \subfloat[Multivariate FFNN]{\label{fig:2:b}\includegraphics[width=0.5\textwidth]{/Users/pavansingh/Google Drive (UCT)/STA Honours/Project/Thesis/Python Coding/Figures/ANN/LOSS/ModelLoss_ANN_Ford.pdf}}    
\label{}
\end{figure}


We note that the loss curve for our training data decreases quickly over each iteration, and flattens out after around 6 iterations. Similarly, we see that our validation curve decreases as we iterate through the data. The validation curve reaches the training loss curve at the 9th iteration. After the 12th iteration the validation curve flattens. 

%\begin{figure}[h]
%\centering
%  \includegraphics[scale =0.30]{/Users/pavansingh/Google Drive (UCT)/STA Honours/Project/Thesis/Python Coding/Figures/ANN/LOSS/ModelLoss_ANN_Ford.pdf}
%    \caption{Loss Curve for Tuned FF Network on Ford Data.}
%  \label{}
%\end{figure}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Variable Feature Importance}

The feature importance captures how much the splits produced by the feature helped to optimise the model's metric used to evaluate the split quality, which in our case is the mean square error (Ampomah \textit{et al.}, 2020). 


\begin{figure}[h]
        \begin{adjustwidth}{-1.2cm}{}

    \subfloat{\label{fig:2:f}\includegraphics[width=0.55\textwidth]{/Users/pavansingh/Google Drive (UCT)/STA Honours/Project/Thesis/Python Coding/Figures/RF AND BOOST/BOOST_importance.pdf}}
     \subfloat{\label{fig:2:f}\includegraphics[width=0.55\textwidth]{//Users/pavansingh/Google Drive (UCT)/STA Honours/Project/Thesis/Python Coding/Figures/RF AND BOOST/RF_importance.pdf}}
    \caption{\small{Variable importance plots from our ensemble decision tree models. }}
\label{}
 \end{adjustwidth}    

\end{figure}



Across both of our models we see that Moving Average Convergence Divergence (MACD) was the most significant predictive variable. Coincidentally, this happens to be one of the most popular technical indicators used in practice. The MACD is a trend following momentum indicator which shows the relationship between two different moving averages on our stocks price. It essentially serves as a signal to indicate when a price has risen/dropped by too much and will correct back to its 26- day moving average. In addition, another strongly predictive feature was the 21-day Exponential Moving Average (EMA). Essentially, an EMA is just a moving average of a stockâ€™s returns where the last few days are given larger weights (Ampomah \textit{et al.}, 2020).


%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

