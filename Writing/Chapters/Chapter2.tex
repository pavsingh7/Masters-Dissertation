% Chapter Template

\chapter{Literature Review} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}


Due to the face that temporal data is created in a variety of circumstances, time series forecasting is a very common data modelling challenge. The financial domain has extensive research into modelling and analysis of time series data. Some of the early approaches to financial forecasting were based upon autoregressive models such as ARIMA (Binner \textit{et al.}, 2005). Such models produce a forecast estimate which is dependent on a linear combination of past values and errors. If the assumption of stationarity is true and the series is generated by a linear process, autoregressive models have been found to perform well (Binner \textit{et al.}, 2005).  The data we are concerned with are stock returns which are generated from a non-linear process (Shivley, 2003). Machine learning models overcomes the shortcoming of linear modelling processes as they makes no assumptions about the prior distribution of the data (Binner \textit{et al.}, 2005). 

In this section we cover a variety of research on financial time series including a comparison on the literature between linear time series and non-linear machine learning methods in forecasting. We begin this review of the relevant literature, by looking at the random walk and efficient market hypothesis. 

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Random Walk and Efficient Market Hypothesis}

It was widely believed, for many years, that stock prices could not be predicted. The random walk theory and the efficient market hypothesis (EMH) gave rise to this notion (Malkiel, 1973).  The Random-walk theory (see Appendix A) states that the price of a stock swings irrespective of its historical performance. That is, stock prices will move in a random walk pattern, and that any prediction of stock movement will be around 50\% accurate. According to EMH, an asset's current price always reflects all previously available information in real time. Hence, it cannot be predicted to consistently earn economic gains greater than the overall market average. As such, the use of prediction algorithms to determine future trends in stock market prices contradicts these two notions.

After exposure to the literature, there appears to exist contention surrounding financial markets and in particular the random walk hypothesis and EMH. Many researchers accept the random walk hypothesis, including Biondo \textit{et al} (2013) who conducted a paper which supported the theory. In studying the performance of technical trading methods, they found that a random strategy outperformed some of the more traditional technical trading methods, such as Moving Average Convergence Divergence (MACD) and Relative Strength Index (RSI), in a series of tests.  Other academics, contend that stock values can be forecasted, at least to some extent (MacKinlay, 1999).
 That is, some researchers have claimed that movements in market prices are not random. Rather, they behave in a highly non-linear, dynamic manner (MacKinlay, 1999). Papers published by Smith (2003), Nofsinger (2005), and Bollen \textit{et al.} (2011), provide. evidence contrary to the notions suggested by EMG and the random. walk. hypothesis.  These studies all conclude that that the stock market can be predicted to some degree and therefore. This is in contrast to and undermines the EMH’s underlying assumptions. In this paper, we are not concerned with proving or disproving the predictability of markets. We are simply comparing models using stock data, however the research mentioned provided some context to the data domain. 

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------
\section{Stock Market Prediction Strategy}

The field of financial forecasting is characterised by noise, non-stationarity, lack of structure and high degree of uncertainty. There are many factors inherent in finance that have an effect on its dynamics and movements, such as political events and traders’ expectations. Aside from the discord surrounding predicting stock markets, there has been a great deal of research on the predictability of stock returns. Current strategies regarding financial investing are broadly classified into two methods; fundamental and technical analysis. 

Fundamental analysis refers to the type of trading analysis which involves the in-depth analysis of a company’s performance and the profitability. This is done so as to measure a company's intrinsic value. Fundamental analysis studies the company in terms of its product sales, man power quality, infrastructure, profitability on investment (Agrawal \textit{et al}, 2013). As such, the stock value of a company is estimated by analysing its margins, earnings, return on equity and profits amongst others as well as other economic factors. It based its principles that the market value of a stock tends to move towards its “intrinsic value”  (Agrawal \textit{et al}, 2013). 

In contrast, technical analysis is an method for forecasting the direction of prices. This is done, not by trying to measure a ciompany's intrinsic value, but by through the study of past market data, primarily price and volume, mostly to trigger the buy or sell rules in the technical analysis (Kirkpatrick II and Dahlquist, 2010). It looks for peaks, bottoms, trends, patterns, and other factors affecting a stock's price movement.

Technical analysis is based on the following assumptions:
\begin{itemize}
\item Prices are solely determined by the supply demand relationship;
\item Prices fluctuate in response to trends;
\item Changes in supply and demand lead trends to reverse;
\item Changes in supply and demand can be graphed;
\item Patterns on charts seem to repeat themselves (Kirkpatrick II and Dahlquist, 2010)
\end{itemize}

Given the above, it is clear that technical analysis methodology ignores external aspects such as political, social, and macroeconomic factors (Cambell, 2012). A paper published in 2004, by Wong \textit{et al} (2010), looked at the role of technical analysis in signalling the timing of stock market entry and exit. Using Singapore data, their results indicated that the indicators can be used to generate positive return from the Singapore Stock Exchange (SES). Moreover, Gao (2018) looked at improving improving stock closing price prediction using recurrent neural network and technical indicators. Gao (2018) employed an LSTM model coupled with technical indicators, to predict the closing price of Apple stock. They compared their model to various other forecasting models, including an ARMA, feed-froward neural network and support vector machine. Their proposed LSTM model with technical indicators outperformed all the other models. 

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{Neural Networks in Financial Forecasting}

Neural networks possess certain characteristics, which deliver very promising potential in prediction problems. Neural networks are particularly suited to settings in which identifying links between data is challenging but there is a large enough training data available. Due to their innate ability to generalise and approximate any complex continuous function, they are able to learn any relationship between a system's input and output  (Binner \textit{et al.}, 2005). This makes them extremely adept in prediction problems, and hence there exists vast research available on using this machine learning architecture on the financial data  (Binner \textit{et al.}, 2005).

The earliest application of neural networks in forecasting was in 1964 by Hu (Chen \textit{et al.}, 2015). Although it did not receive significant attention due to the absence of any learning method for neural networks - thus, making it impossible to apply these networks to complex precision problems. However, in response to the work on error backpropagation learning, which was published by Werbose (1982) and by Rumelhart \textit{et al} (1986), the interest in neural networks have grown rapidly since.

In 1988, White (1988) made the first attempt to model a financial time series using a neural network model. Looking at asset data for IBM, White (1988) tried to model a neural network for decoding the non-linear regularities in IBM's asset price movement. His work, albeit. limited in scope, helped in establishing evidences against EMH (Chen \textit{et al.}, 2015). Moreover, in 1990, Kimoto \text{et al.} (1990), used neural networks to predict the index of the Tokyo stock market, and achieved precision of 63\% precision (Naeini, 2010).

Recurrent neural networks (RNNs) are a class of artificial neural network. In these networks, connections between neurons form a directed cycle. RNNs can be compared to normal feed-forward networks, but with loops. As such, they naturally exhibit dynamic temporal behaviour for a time sequence. This has led to them being studied in the context of time series forecasting extensively. Additionally, RNNs have produced better results with respect to time series problems than other machine learning methods (Chen \textit{et al.}, 2015). More specifically, in this paper, we look at using a long short term memory (LSTM) network, which is a type of RNN model LSTMs were introduced in 1997 by Hochreiter and Schmidhuber, to address certain limitations that standard RNNs had (Chen \textit{et al.}, 2015). 

When compared to other machine learning modelling methods, these networks have often produced some of the best results in the context of sequential data (Graves, 2012). Their impressive performance is particularly renowned in the field of Natural Language Processing (NLP), and in handwriting recognition (Liwicki, 2009). The promising results of LSTMs models in their applications have triggered their use in the financial forecasting. 

From the past literature, LSTM have delivered promising results in forecasting financial assets. A paper published by Chen \textit{et a.l} (2015), used price data to forecast price movement. More, specifically he used historic price data and technical indicators in addiction to stock indexes to predict if a stock’s price would increase or decrease on a given day. Their research looked at different stocks from the Brazilian Stock Exchange. Chen \textit{et a.l} (2015) were able to achieve promising results from their analysis. They obtained an average of 55.9\% of accuracy when predicting if the price of a particular stock is going to go up or not - using their LSTM model. 

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------


\section{Ensemble Decision Trees in Financial Forecasting}

Due to the success of machine learning algorithms in a variety of fields since their inception, ensemble methods have also risen in popularity. Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. Fast algorithms such as decision trees are commonly employed in ensemble methods, however ensemble methods do not only pertain to decision trees.  For the purposes of this study we shall be implementing ensemble decision trees, more specifically a Random Forest and an Adaptive Boosting model. 

Recently there have been several publications regarding the evaluation of tree based ensemble machine learning models in financial forecasting. Ampomah \textit{et al.} (2020) published a paper comparing the effectiveness of several tree based ensemble models. Their paper included a random forest, XGboost model, and even an Adaptive Boosting model. The authors assessed the models on their ability in forecasting the direction of stock price movement, using several performance metrics such as, accuracy, precision and area under receiver operating characteristics curve (AUC ROC). Ampomah \textit{et al.} (2020) found that when using their training set, the AdaBoost model performed the best, while on the test. set, the Extra Trees classifier outperformed the other models. In the study the Adaptive Boosting model was the second best model, followed closely by the Random Forest model.

Moreover, a study by Nti \textit{et al.} (20), compared ensemble learning methods for classification and regression machine learning tasks in stock market prediction. The goal of the paper was to clarify which ensemble strategies are most suited for stock market prediction problems. In the study, a host of methods were used; namely boosting, bagging, blending and stacking. The models used included decision trees, support vector machines and neural networks. In their comparative analysis, they found that stacking and blending ensemble techniques offered higher prediction accuracies compared with bagging and boosting. 

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Comparison of Linear and Non-Linear Forecasting Methods}

The literature argues that neural networks offer significant theoretical advantages over traditional statistical methods. Neural networks have been mathematically shown to be universal approximators of functions (Hornik \textit{et al.}, 1989). Hence, neural networks can - in theory - approximate whatever functional form best characterises a time series. Neural networks are also inherently non-linear and so can estimate nonlinear functions considerably better than linear methods (Rumelhart and McClelland 1986). However, empirically their application and results against traditional methods have been long contested. Several studies have compared neural networks and traditional time series approaches. Most of these studies have used the data from the M-competition (Makridakis \textit{et al.}, 1982), which includes 1001 real time series (Ahmed. \textit{et al.}, 2010). 

Looking at the original M-competition study, various groups of forecasters were given all but the most recent data points in each series. Each group consisted of forecasters who were regarded as experts in a particular technique. The groups were free to apply any approaches in. their. domain of competence to forecast the time series. After each group prepared its forecasts, Makridakis compared the forecasts to the actual values. The specific results of this competition are reported in Makridakis et al. (1982). One of the main conclusions from the competition, were that statistically sophisticated or complex methods do not necessarily produce more accurate forecasts than simpler ones. This inspired our decision to include a simple benchmark model to compare the performance our machine learning models. 

In the years that followed the competition ,the data which contained all 1001 series have been made available. This sparked interest in the community and led to many studies using the data and empirically analysing the conclusions from the M-competition. The results of these studies have contrasted the findings of the competition, by demonstrating that neural network models in some cases appear to outperform simpler methods.  Sharda and Patil (1990) made use of. a subset of the competition data. Their findings suggested that neural network models performed as well as the automatic Box-Jenkins (Auto-box) procedure. Similarly, Tang \textit{et al.} (1990), found that neural network models and Box-Jenkins models produce comparable results, when used on time series with a long history. In 1991 however, Foster \textit{et al.} (1991)  found that neural networks to be inferior to Holt's, Brown's, and the least squares statistical models for time series of yearly data. When comparing the models on quarterly data, they found the models to be comparable, using the competition data. Kang (1991), compared neural networks and Auto-box on the 50 M-competition series and found that it was the most appropriate technique for forecasting. An important finding from Kang's research was that the neural networks often performed better when predicting on a forecasting horizon a few periods ahead. This effect has also been noted when using neural networks to forecast macroeconomic data (White, 1994) and currency exchange rates (Zimmerman, 1994). Finally, Tim Hill \textit{et al} (1996), conducted research on time series forecasts produced by neural networks and compared them with forecasts from six statistical time series methods generated in the M-competition (Makridakis \textit{et al.}, 1982). Across monthly and quarterly time series, the neural networks did significantly better than traditional methods. However, the neural network model and traditional models were comparable on the annual data. 

More recently, a paper published by Binner \textit{et al.} (2005), compared neural networks to traditional time series models; univariate autoregressive integrated moving average (ARIMA) and multivariate vector autoregressive (VAR) models. They did not use the competition data, however they found that the best models for their neural network outperform the traditionally used linear ARIMA and VAR models in macroeconomic forecasting.This is in line with findings from White (1994) and Zimmerman (1994). The authors attributed the gain in forecasting accuracy for their neural network models, to their capability to capture nonlinear relationships between macroeconomic variables (Binner \textit{et al.}, 2005). 

The results from the literature above are encouraging but equivocal; we were thus inspired to include a benchmark model to our comparative analysis of machine learning models - namely, an ARIMA model which, in fact, outperformed neural network models in the M-competition (Makridakis \textit{et al.}, 1982). 


%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Comparison of Machine Learning Forecasting Methods}

Although there has been extensive research into the applications of individual machine learning methods for forecasting financial data, there does not exist as much research into comparisons of these existing methods. One of the few, was published by Ahmed NK \textit{et al.} (2010). Their paper compared empirically the performance of several families of machine learning models for time series forecasting. More specifically, they compared in their study the performance of eight machine learning models with respect to their accuracy applied to a subset (1045 series) of the M3 competition data. The M3 competition is a sequel of M forecasting competitions, organised by Makridakis and Hibon (2000). The data builds on its predecessors and consists of 3003 business-type time series, covering various fields. In the study, they consider only the monthly time series. 

Before computing the forecasts, they pre-processed the series in order to achieve stationarity in their mean and variance. Ahmed NK \textit{et al.} (2010) calculated one-step-ahead forecasts for each one of the time series. To summarise the findings of the paper, the family of models which ranked the best turned out to be a multilayer perceptron, followed closely by a Gaussian processes and Bayesian neural network. The worst performing family of machine learning models was the radial basis functions. Our study extends this analysis, by looking at four different machine learning models, assessed on financial stock data.
