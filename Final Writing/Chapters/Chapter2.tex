% Chapter Template

\chapter{Literature Review and Related Work} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

Chapter \ref{Chapter1} briefly introduced the key concepts around recommender systems, the problem they solve and the main ideas relevant to this paper. Chapter \ref{Chapter2} will build on this by providing historical context through an overview of recommender systems. This overview will focus on e-commerce applications, reviewing literature done on recommenders incorporating similar features and characteristics relevant to the work done in this paper. We divide this overview into several different sections, each addressing key discoveries and background information relevant to this paper. To this end, the sections covered will focus on work which harnessed either collaborative-based filtering, deep learning, text analysis or a combination of these approaches all within the context of recommender systems.

\section{Recommender Systems: history and applications}
\label{sec:2 Recommender Systems: history and applications}

Recommender systems have evolved significantly since their inception, becoming a pivotal component of contemporary information retrieval and personalised content delivery. Due to their success they play a key part in many platforms, offering services or showcasing items to users across various different domains. In this section, we first look at the history and various applications of recommender systems (\ref{subsec:2 History and Applications}) and then we shall focus on the literature behind recommender systems within e-commerce (\ref{subsec:2 Recommenders in e-commerce: Amazon case study}) specifically. 


\subsection{History and Applications}
\label{subsec:2 History and Applications}

The roots of recommender systems can be traced back 1992, where \cite{goldberg1992using} proposed the Tapestry system, which was the first information filtering system based on collaborative filtering through human evaluation. This system laid the groundwork for collaborative recommendation approaches for the years to come. Similarly, the GroupLens project at the University of Minnesota in 1992, introduced a framework for recommender systems which involved generating recommendations by analysing users' historical preferences and behaviours \cite{konstan1997grouplens}. Their work significantly influenced the development of recommendation models and architectures (\cite{konstan1997grouplens}, \cite{huang2004applying}). In 1998, Amazon developed their own version of a collaborative-based filtering recommendation system, coined item-based filtering. This system changed and shaped the landscape for e-commerce \cite{linden2003amazon}. Although it is unclear when content-based filtering was first introduced \cite{balabanovic1997fab}, the method became popularised in 1999 \cite{herlocker1999algorithmic}, where items were recommended based on their inherent attributes and user preferences. In the years that followed, research interest on recommender systems grew significantly, leading to the exploration of diverse methodologies and variations of collaborative-based or content-based (and sometimes both) recommender system methods \cite{burke2002hybrid}. 

An important milestone in recommender systems occurred in 2006 when Netflix initiated an open competition offering a \textdollar1 million prize for an algorithm that could improve on the accuracy of their system, Cinematch\footnote{a recommendation algorithm developed by Netflix in the year 2000, which aimed to provide personalised movie recommendations to users based on their viewing history and preferences}. This competition spurred significant interest and development in recommender system applications and use cases, with the prize claimed in 2009 by a group of three researchers \cite{bennett2007netflix}. Their winning algorithm, an ensemble method that combined various collaborative filtering techniques, improved the accuracy of Cinematch by 10\%. This competition was a significant milestone in the development of recommender systems, as it demonstrated the potential for significant (and lucrative) improvements in recommendation accuracy and the value of collaborative filtering and matrix factorisation in recommendation systems.

Over the next decade, recommender systems have become critical for the success of major Internet companies such as Netflix, Amazon and Facebook, amongst others \cite{chen2012critiquing}. The widespread adoption of internet-based services further fuelled the proliferation of recommender systems, extending their use across diverse domains. In broad terms, when a wide variety of items exist and users differ from each other, personalised recommendations can assist in delivering suitable content to the respective individuals.  Applications of such systems range from tourism, encompassing hotels, restaurants, and parks (\cite{yang2013itravel},\cite{loh2003tourism}); advertising \cite{cheung2003mining}; business and retail \cite{ghani2002building}; medical diagnosis \cite{perez2013collective}; and music selection \cite{bogdanov2013semantic}. 

Table \ref{tab:recommendations} shows some popular e-commerce sites using recommender systems and a high-level description of what these systems recommend to users of these platforms. Streaming platforms like Netflix leverage recommender systems to personalise content recommendations, significantly impacting user engagement and retention \cite{gomez2015netflix}. In fact, Netflix used recommender systems so extensively that their Chief Product Officer, Neil Hunt, indicated that more than 80\% of movies watched on Netflix came through recommendations and placed the value of Netflix recommendations at more than \textdollar1 billion per year \cite{gomez2015netflix}. Social media platforms like Facebook and LinkedIn employ recommender systems to curate users' news feeds and suggest connections \cite{aivazoglou2020fine}. Recommendations were used so extensively by Amazon that a Microsoft research report estimated that 30\% of Amazon’s page views were from recommendations \cite{sharma2015estimating}. 

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
      \hline
      \textbf{Site/Platform} & \textbf{What is Recommended} \\
      \hline
      \text{Netflix} & \text{Movies, TV shows} \\
      \hline
      \text{Amazon} & \text{Books, Fashion, and other products} \\
      \hline
      \text{Facebook} & \text{Friends, posts, articles} \\
      \hline
      \text{LinkedIn} & \text{Posts, articles, jobs} \\
      \hline
      \text{Spotify} & \text{Music, podcasts} \\
      \hline
    \end{tabular}
    \caption{Content Recommendations on Different Platforms}
    \label{tab:recommendations}
  \end{table}
  

  Ultimately, recommender systems continue to evolve and play a crucial role in delivering personalised content across various industries, adapting to the dynamic needs of users and businesses alike. The ongoing development and integration of these systems underscore their enduring significance in shaping the landscape of information retrieval and content delivery. We shall now describe the use of recommender systems in the e-commerce domain specifically.

\subsection{Recommenders in E-commerce: Amazon case study}
\label{subsec:2 Recommenders in E-commerce: Amazon case study}

The exponential growth of the internet has transformed the operations of companies, particularly within the realm of e-commerce. E-commerce, characterised by the buying and selling of goods and services over the internet, has become a platform ubiquitous with consumers exploring and purchasing products \cite{schafer2001commerce}. Users’ navigating through these platforms are confronted with a multitude of decisions, prompting questions such as "Which product should I purchase?" or "What brand should I choose?". For a successful e-commerce platform, efficiency in showcasing relevant products is paramount, and recommender systems have emerged as a pivotal tool in achieving this goal \cite{schafer1999recommender}. Recommender systems have evolved from novelties to indispensable tools that shape the world of e-commerce \cite{schafer1999recommender}. They are proven to have significant impacts on sales, diversity, customer retention, and revenue generation \cite{linden2003amazon}.

Today, prominent e-commerce websites, including Amazon, Netflix, eBay, Alibaba, and Etsy, leverage recommender systems to assist users in discovering products for purchase \cite{aivazoglou2020fine}. Recommenders are identified as able to turn browsers into buyers, addressing the challenge of users abandoning a platform when their desired item is not readily visible. In fact, research has demonstrated that users often exhibit a tendency to abandon a system when their desired item does not appear within the initial 5 or 10 search results and said user tries again on another system \cite{linden2003amazon}. Recommender systems have the capability to assist these customers in discovering the products they wish to buy. Moreover, recommender systems enhance cross-selling efforts by suggesting supplementary products during the checkout process. For example, during the checkout process, a website could suggest additional items based on the products already present in the shopping cart. Moreover, effective recommenders can contribute  to a 10-30\% increase in cross-selling revenue \cite{mckinsey_personalization_2021}. Cultivating customer loyalty in a competitive online landscape is another notable outcome of recommenders, as they establish value-enhanced relationships between platforms (e.g. Amazon) and users \cite{linden2003amazon}. These platforms' investment into recommenders which are able to comprehend their user’s preferences, and then accordingly translate this understanding into action, result in users tending to favour their platform - since they align well with their preferences.



Looking at the history of recommender systems within e-commerce, the paper by \cite{schafer1999recommender} offers a comprehensive exploration of how early e-commerce platforms employ their technologies, using notable businesses like Amazon as a case study. The paper unveils key insights by scrutinising recommender system implementations and their contributions to platform profitability. Several of these recommendation strategies are still used today and stand out as revenue drivers, some of which are discussed below. These include, "Similar Items", "Text Comments", "Average Ratings", and "Top-\textit{N} Recommendations". The "Similar Items" approach prompts customers to explore related products based on their previous purchases. "Text Comments" allow users to read (or leave) impartial comments on a product so as to get a better understanding. And "Average Ratings" provides a numerical rating which gives a convenient gauge of item quality, and "Top-\textit{N} Recommendations" provides lists of top unrated items aligning to a users preferences. These strategies collectively contribute to the effectiveness of recommender systems in optimising the e-commerce landscape and, additionally, many of these strategies are still offered on Amazon’s platform today. 


Looking at Amazon speicfically beyond these strategies employed, the company says their recommender systems play a critical role in leveraging the "long tail" concept, contributing to substantial profits from their products with infrequent purchases \cite{leino2007case}. While individually rare, the collective presence of such items can yield substantial profits. For instance, Amazon attributes a noteworthy portion of its sales, ranging from 20-40\%, to products that do not fall within its top 10 000 best-selling items \cite{brynjolfsson2003consumer}. The e-commerce giants approach involves dynamically personalising the online store for each customer, aligning with the concept of having "2 million stores on the web" as articulated by Jeff Bezos, the CEO of Amazon \cite{linden2003amazon}. Amazon's technical implementation of recommender systems involves applying a variety of recommender algorithms, such as item-based collaborative filtering. These recommender systems extend across various pages on Amazon's platform, including the homepage, search results page, and shopping cart, offering personalised recommendations based on customers' past behaviour and preferences. The paper by \cite{linden2003amazon} discusses the recommender system - item-based collaborative filtering algorithm - in great detail. 

Ultimately, recommender systems serve as communication bridges between products and users, enhancing customer experiences and increasing the likelihood of a sale. Amazon's success stands as a testament to the effectiveness of quality recommender systems, continuously adapting to user preferences and fostering customer loyalty. Within e-commerce, recommenders greatly impact the revenue generation and cross-selling ability of platforms by directly being able to understand customer preferences and show or highlight items aligning well to these user preferences. Having established the background and history of recommender systems (with a focus on e-commerce), we now look toward one of the primary types of recommender systems - collaborative-based filtering.  


\section{Collaborative Filtering}
\label{sec:2 Collaborative Filtering}

The basic concept behind collaborative based filtering methods is that it harnesses the collective behavior of a community of users to make personalized recommendations, without relying on explicit domain knowledge or item attributes. The paper done by \cite{resnick1994grouplens} serves as one of the seminal pieces of work which played a pivotal role in popularising collaborative filtering techniques for recommenders. While not the absolute first use of recommender systems, it brought attention to the concept's potential applications, especially in personalised recommendations. In 1992, the Tapestry system \cite{goldberg1992using} introduced collaborative filtering, leveraging users' experiences to tailor recommendations without requiring external information about items or users. Specifically, collaborative filtering is defined as a recommendation technique used to personalise the experience of users through recommendations tailored to their interests, leveraging the experiences of other users with similar profiles \cite{herlocker1999algorithmic}. It does this without need for exogenous information about either items or users \cite{shapira2022recommender}. It comprises two primary approaches: memory-based, recommending based on similarity between users or items, and model-based, recommending by developing a predictive model based on user interactions \cite{symeonidis2008collaborative}. 

Both memory-based and model-based approaches have their strengths. Both practical experience and related research have reported that memory-based algorithms present excellent performance, in terms of accuracy, for multi-value (e.g., 1-5 rating scale) rating data. On the other hand, model-based algorithms can efficiently handle scalability\footnote{Scalability refers to the ability of a system to handle increasing amounts of data or workload without sacrificing performance.} to large data sets \cite{symeonidis2008collaborative}. This is not to say that these methods have no weaknesses. These traditional collaborative filtering methods face two primary challenges in a growing e-commerce landscape: real-time scalability and recommendation quality. More details about scalability are discussed later on in Section \ref{subsec:2 Scalability}. Regardless, a major appeal of collaborative filtering methods in general is that it is domain free, yet it can address data aspects that are often elusive and difficult to profile using other recommender techniques such as content filtering \cite{koren2009matrix}. In fact, collaborative filtering is recognised as the most successful recommender system, able to produce user-specific recommendations based on patterns of ratings or usage without the need for additional information \cite{koren2009matrix}. 

The following subsections will outlay the details, background and literature into the two primary branches of collaborative filtering: memory-based and model-based approaches.

\subsection{Memory-Based Algorithms}
\label{subsec:2 Memory-Based Algorithms}

As mentioned, memory-based methods are techniques within collaborative filtering systems which involve recommending items based on a similarity measure. One prominent family of methods within memory-based collaborative filtering are neighbourhood-based methods, which focus on creating a “neighbourhood” of users (or items) based on their similarity to a target user (or item) \cite{adamopoulos2013beyond}. Specifically, these methods make recommendations by first identifying similar users (or items) to a target user (or item) and then utilises the preferences of these entities to make personalised recommendations. The two most prominent neighbourhood methods are user-based and item-based collaborative filtering \cite{herlocker2004evaluating}.

The user-based approach for neighbourhood methods is one of the most popular applications of memory-based collaborative filtering, largely due to its simple implementation \cite{herlocker1999algorithmic}. In the mid-1990s, collaborative filtering predominantly followed a user-based methodology, where the initial step involved searching for other users with similar preferences, such as users having comparable purchase patterns. That is, identifying a set of customers whose purchased and rated items overlap with the target user's preferences (perhaps they purchased similar items or rated common items similarly in the past). By aggregating items from these similar customers, the algorithm eliminates those already purchased or rated by the user and recommends the remaining items \cite{smith2017two}. The original GroupLens system \cite{resnick1994grouplens} (that we have previosuly mentioned) from 1994, implemented a user-based collaborative filtering algorithm, using users' similarities to identify a neighbourhood of nearest users. Numerous improvements to user-based algorithms have since been suggested, enhancing the predictive capabilities of these methods \cite{breese2013empirical}. The original GroupLens system used Pearson correlations\footnote{Pearson correlation is a measure of the linear correlation between two variables. It is often used to quantify the similarity between users based on their item ratings. A higher correlation coefficient indicates a stronger similarity between users' preferences.} to weight user similarity, used all available correlated neighbours, and computed a final prediction by performing a weighted average of deviations from the neighbour’s mean. The Ringo music recommender \cite{shardanand1995social} represented a significant advancement beyond the original GroupLens algorithm. Ringo's innovation involved achieving superior performance by calculating similarity weights through a constrained Pearson correlation coefficient\footnote{Constrained Pearson correlation is a variation of the Pearson correlation coefficient that incorporates constraints to address issues such as sparsity or data scarcity.}. Since, “similarity” is a key component in this (and all memory-based) approach, the way in which similarity is measured had garnered significant research in the early beginnings of neighbourhood-based approaches. An empirical analysis of various neighbourhood-based collaborative filtering algorithms and, specifically, similarity measures, namely Pearson correlation and cosine vector similarity, was conducted by \cite{breese2013empirical}. The findings indicated that Pearson correlation demonstrated superior performance, although subsequent research suggests potential equivalence with cosine similarity \cite{pennock2013collaborative}. User-based collaborative filtering is still synonymous with collaborative filtering till this day. Due to its simplicity and ease of understanding, it is often employed in simple recommendation tasks. 


The other neighbourhood based method is item based collaborative filtering which was first introduced in the late 1990s and was popularised by Amazon - it is based on the items’ similarities (instead of users) for a neighbourhood generation of nearest items \cite{sarwar2001item}. It is a prominent technique under neighbourhood methods, where it evaluates a user's preference for an item based on the ratings of "neighbouring" items by the same user. Amazon has successfully employed item-item collaborative filtering since 2003, generating real-time, scalable, and high-quality recommendations \cite{smith2017two}. It’s successful implementation has led to a large number of research to be focused on this technique. In contrast to user-based approaches, item-to-item collaborative filtering focuses on finding similar items rather than similar customers, offering a unique recommendation approach \cite{linden2003amazon}. Specifically, the item-to-item collaborative filtering algorithm builds a similar-items table to recommend highly correlated items, leveraging the cosine measure for similarity calculations. It excels in recommendation quality even with limited user data and has gained widespread use across platforms like YouTube and Netflix due to its simplicity, scalability, explainability, and immediate updates based on new customer information \cite{davidson2010youtube}. However, like user-based approach, item-based collaborative filtering has its flaws, such as the risk of trapping users in a "similarity hole"\footnote{The term "similarity hole" refers to a situation where a recommendation system continuously suggests items that are too similar to those already consumed by the user, potentially leading to a lack of diversity in recommendations and limiting the discovery of new and potentially relevant items.} by offering overly similar recommendations \cite{rashid2002getting}. The method, like user-based approach, also struggles to scale well to larger volumes of data. 

Further details about these two memory-based approaches will be explored in Chapter \ref{Chapter4}, as they serve as important comparative benchmarks in our analysis. At this point, we can highlight that all memory-based algorithms face scalability challenges when dealing with large volumes of data. To address this, dimensionality reduction techniques have been proposed to balance the trade-off between accuracy and execution time of collaborative filtering algorithms \cite{sarwar2000analysis}. More details on this limitation (scalability) shall be explored in greater detail later on in this chapter, in Section \ref{subsec:2 Scalability}.


\subsection{Model-Based Algorithms}
\label{subsec:2 Model-Based Algorithms}

Model-based algorithms are the second branch of collaborative-based filtering systems. In contrast to memory-based methods, model-based algorithms harness the overall dataset of ratings to build predictive models, often employing statistical or machine learning techniques \cite{adomavicius2005toward}. So, the key distinction between model-based techniques and memory-based approaches lies in their approach to generate predictions. Model-based techniques rely on models (i.e., mathematical representations or algorithms that capture patterns and relationships within the data) to derive predictions whilst memory-based methods generate predictions from the weighted similarity of preferences among users or items \cite{adomavicius2005toward}.

There are a variety of different model-based algorithms. Bayesian models \cite{chien1999bayesian}, probabilistic relational models \cite{getoor1999using}, linear regressions \cite{sarwar2001item}, and Latent Dirichlet Allocation \cite{marlin2003modeling} are among the diverse array of model-based methods. Some of the most successful realisations of model-based methods (and collaborative filtering in general) are latent factor models, namely matrix factorisation \cite{koren2009matrix}. Matrix factorisation is characterised by transforming both items and users into separate latent factor spaces. Each latent factor space represents a set of latent variables (or features) that capture the underlying characteristics of either items or users \cite{koren2009matrix}. Each latent factor is a dimension in the space, and the value of each dimension represents the extent to which the user or item possesses that characteristic.The model is trained to learn these latent factors, and once learned, the model can predict the rating of an item by a user by computing the dot product of the user and item latent factor vectors. Generally, the model is trained to minimise the difference between the predicted ratings and the actual ratings in the training data. The model is then used to predict the ratings of items that the user has not yet rated, and the highest predicted ratings are recommended to the user - i.e., once these latent factors are learned, the recommender system can provide personalised recommendations for each user \cite{koren2008factorization}. For instance, in the context of movies, the identified factors could encompass straightforward aspects like categorising films into genres like comedy or drama, gauging the level of action, or assessing suitability for children. Additionally, these factors might delve into more ambiguous facets such as the depth of character development or the presence of unique qualities, or even include entirely unexplainable dimensions. Regarding users, each factor signifies the extent to which a user favours movies that align with the specific characteristics identified in the corresponding movie factor.

Matrix factorisation, particularly highlighted by its success in the Netflix competition, has rose in popularity, demonstrating superiority over nearest-neighbour techniques for product recommendations \cite{koren2009matrix}. The acclaim for matrix factorisation stems from its exceptional scalability, effective handling of sparse data, and the ability to incorporate additional information, such as implicit feedback\footnote{Implicit feedback refers to user interactions with items that are not explicitly expressed as ratings, such as clicks, views, or purchases. It includes any user behaviour that can be used to infer preferences or satisfaction with items, such as consuming, using or buying an item} \cite{koren2009matrix}, temporal effects, and confidence levels\footnote{Confidence levels represent the degree of certainty associated with observed ratings or interactions. Matrix factorisation algorithms can adaptively weight or adjust the influence of ratings based on their associated confidence levels} \cite{koren2009matrix}. However, this is not to say that matrix factorisation techniques (and additionally all model-based) approaches do not have their own set of limitations. Model-based approaches, like all collaborative filtering approaches, struggle when it comes to recommending items to new users (the cold start problem). Details of these limitations are discussed in greater detail in section \ref{sec:2 Limitations and Challenges}.  Now, having set the stage by discussing the background and different types of collaborative filtering, we shall begin to explore deep learning and the impact it has had on the recommender landscape. 

\section{Deep Learning in Recommender Systems}
\label{sec:2 Deep Learning in Recommender Systems}

Over the past few decades, deep learning has achieved remarkable success in a diverse range of domains like computer vision and speech recognition \cite{seth2022comparative}. Both academia and industry have quickly embraced its application, driven by its ability to tackle complex tasks and deliver state-of-the-art results (\cite{cherkassky2012statistics}; \cite{clark1999scientific}). 

The most basic deep learning model entails a multilayer feed-forward neural network which undergoes training, employing back-propagation\footnote{Back-propagation is a supervised learning algorithm used to train neural networks by adjusting the network's weights in order to minimize the error between the predicted and actual outputs. It works by propagating the error backward from the output layer to the hidden layers, updating the weights based on the error gradients with respect to each parameter.} or other supervised algorithms, to create a predictive statistical model for a specific input–output mapping \cite{zhang2019deep}. The network learns from a set of examples, embedding information in connection weights, allowing it to generalise to examples beyond the training set. Effectively, the neural network serves as a flexible tool that can apply its learned knowledge to new data points, showcasing its ability to generalize beyond the specific examples encountered during training.

The recent (in the last decade) advancements in machine learning and artificial intelligence, have paved the way for refining and enabling recommender systems to first, integrate deep learning models, and second, achieve more accurate and relevant recommendations \cite{he2017neural}. The successful integration and apparent advantageous performance of deep learning in recommenders has led to there being a significant increase in the number of research publications on deep learning-based recommendation methods. The majority of which have provided further strong evidence of the inevitable pervasiveness of deep learning in recommender system research \cite{zhang2019deep}.

Traditionally, the state-of-the-art for collaborative filtering has predominantly entailed matrix factorisation (or some derivative of it) for modelling the interaction between user and item features (see Section \ref{subsec:2 Model-Based Algorithms}). This involved applying an inner product on the latent features of users and items. Much research effort has been devoted to enhancing matrix factorisation, such as integrating it with neighbor-based models \cite{koren2008factorization}, combining it with topic models\footnote{Topic models are statistical models used to discover the abstract topics that occur in a collection of documents} of item content \cite{wang2015collaborative}, and extending it to factorisation machines\footnote{Factorisation machines are a type of machine learning model that extends traditional matrix factorisation techniques by incorporating additional features, such as user and item attributes, into the factorization process.} \cite{rendle2010factorization} for a generic modelling of features. Despite these numerous efforts to enhance matrix factorisation, its performance has been limited by the simplicity of the inner product interaction function (which simply combines the multiplication of latent features linearly), and may not capture the complexity of user interaction data \cite{he2017neural}. One approach to address this limitation is increasing the number of latent factors, but this can lead to overfitting, particularly in sparse settings \cite{rendle2010factorization}. To tackle this challenge, deep learning based approaches have  been proposed, which use a neural network architecture to model the interaction between users and items (replacing the inner product), enabling the model to learn arbitrary functions from data \cite{he2017neural}. Furthermore, the paper by \cite{he2017neural} has established a general framework for using this deep learning based approach for collaborative filtering, named Neural Collaborative Filtering (NCF). In their paper, their NCF model, leveraging a multi-layer perceptron for non-linearities, demonstrated consistent and statistically significant improvements over state-of-art matrix factorisation approaches such as eALS\footnote{eALS, or explicit Alternating Least Squares, is a matrix factorisation algorithm that explicitly models observed user-item interactions using least squares optimisation.} and basic baselines such as item-based collaborative filtering on MovieLens and Pinterest datasets. 

In addition to NCF, neural networks have been applied to develop Deep Matrix Factorisation, a hybrid technique merging matrix factorisation and deep learning \cite{zhang2019deep}. In this approach, a deep neural network is employed to factorise the user-item interaction matrix, creating low-dimensional representations for both users and items \cite{zhang2019deep}. This stands in contrast to NCF, where the approach involves using neural networks to directly learn representations of users and items, eliminating the need for explicit factorisation of the interaction matrix. 

In the contemporary landscape, numerous companies leverage deep learning to enhance the quality of their recommendations, showcasing a notable shift in the recommender system paradigm (\cite{cheng2016wide}; \cite{covington2016deep};\cite{okura2017embedding}). Specifically, there has been a deep neural network-based recommendation algorithm tailored for video recommendations on YouTube \cite{covington2016deep}, a wide and deep model for an App recommender system on Google Play \cite{cheng2016wide}, and an RNN-based news recommender system designed for Yahoo! News \cite{okura2017embedding}. These models underwent rigorous online testing and demonstrated significant improvements over traditional counterparts, illustrating the transformative impact of deep learning on industrial recommender applications. A comprehensive review of deep learning-based recommendation models and their applications is provided by \cite{zhang2019deep}, which serves as an invaluable resource for understanding the evolution of deep learning within the realm of recommender systems.

Ultimately, we follow the approach done in \cite{he2017neural} for NCF. That is, embracing a neural network-based collaborative filtering paradigm that operates to capture and learn the user-item interactions through using multi-layer network, offering non-linear transformations in our recommender system. This can be particularly useful when the relationships between input features and user preferences are complex and non-linear, which may be the case in many real-world scenarios \cite{he2017neural}. Furthermore, neural networks and their inherent layered structure make it easy to incorporate auxiliary information since each layer can process different aspects of the data. This provides an effective platform for which we can evaluate the influence of textual features within our NCF model, aligning with our specified research objectives (see Section \ref{sec:1 Research Questions and Significance}). To that end, we will look into the history and literature of text within recommender systems, also exploring the relevance of text-based recommender systems. 

\section{Text Analysis in Recommender Systems}
\label{sec:2 Text Analysis in Recommender Systems}

An important shortcoming of collaborative filtering systems is that these algorithms are not able to capture the rationale for a user’s rating, and thus can not holistically capture a target user’s preference. This is a key (and unique) problem for collaborative filtering, which generally relies on numerical ratings. To tackle these challenges, integrating additional information to collaborative filtering models have been looked at, including tags \cite{zhou2003learning}, geo-location \cite{hu2014your} and user reviews \cite{srifi2020recommender}. In this paper, we focus on the integration of textual information such as user reviews as a solution to the aforementioned challenge (lack of rationale). Textual information is a rich source of data that can provide nuanced insights into user preferences, and has been utilised to enhance recommender systems in many domains, including movies \cite{diao2014jointly}, hotels \cite{musat2013recommendation}, and e-commerce \cite{he2016ups}. Reviews, often in unstructured text, present a complex yet valuable pool of information \cite{shoja2019customer}. These reviews offer a fine-grained, nuanced, and reliable source of user preference information, enabling the system to construct detailed user preference representations \cite{zhang2014urcf}. 

Recommender systems using text reviews generally use one of three  key elements extracted from review texts: review words, review topics, and overall opinions \cite{chen2015augmenting}. Specifically, using term frequency-inverse document frequency\footnote{Term frequency-inverse document frequency (TF-IDF), a statistical measure which is used to evaluate the importance of a word in a document relative to a collection of documents. TF-IDF weighting helps capture representative terms in reviews by giving higher weights to words that are frequent in the review but rare in the entire corpus.} on review texts, we can identify representative words that capture the essence of the review \cite{chen2015augmenting}. Topics can be detected through methods like latent dirichlet allocation\footnote{Latent Dirichlet Allocation (LDA) is a generative probabilistic model used for topic modeling, which aims to uncover latent topics present in a collection of documents. It can uncover the underlying themes or aspects discussed in the reviews.} \cite{chen2015augmenting}, which can reveal the aspects discussed in reviews. Additionally, overall opinions, reflecting sentiments, can be deduced using sentiment analysis techniques. 

Several works have used review texts and their related rich information like review words, review topics and review sentiments, for improving the rating-based collaborative filtering recommender systems (\cite{he2015trirank}; \cite{hariri2011context}; \cite{zhang2014urcf}). A detailed survey of recent works that integrate review texts and also discusses how these review texts are exploited in order to mitigate the main issues of the standard rating-based systems like sparsity and prediction accuracy problems has been conducted \cite{srifi2020recommender}. The paper reiterated that the elements that can be extracted and used for recommenders are the actual review words, topics and opinions. They also concluded that the inclusion of textual features was associated with positive outcomes in terms of recommender accuracy. The experimental results from the paper demonstrated that the performance of the recommender system by incorporating information from reviews, produces recommendations with higher quality in terms of rating prediction accuracy compared to the baseline methods (which included matrix factorisation).


As mentioned, user sentiment or opinion is another piece of information we can extract from review text and used for recommender systems. To extract this opinion from the review we use sentiment analysis\footnote{Sentiment analysis, also known as opinion mining, is a natural language processing technique used to determine the sentiment or opinion expressed in a piece of text.}. Extracting sentiments and augmenting a recommender model with it, is one of the the more conventional approaches towards the incorporating review text within recommenders \cite{kim2016convolutional}. Techniques like sentiment-based matrix factorisation have been proposed to integrate review sentiments into recommendation models \cite{shen2019sentiment}. This model has been shown to improve the quality of recommendations, particularly in the presence of sparse data \cite{shen2019sentiment}. In fact, sentiment analysis has been shown to be a valuable tool to complement traditional ratings, improving recommendation quality in several works (\cite{shen2019sentiment}; \cite{kim2016convolutional};\cite{dang2021approach};  \cite{dang2020sentiment}; \cite{diao2014jointly}). Effectively, sentiment analysis on review texts to extract user opinion, poses as a viable option to not only addresses sparsity but also as a tool to complement traditional ratings, improving recommendation quality \cite{shoja2019customer}.

Ultimately the integration of review texts into collaborative filtering recommender systems has shown positive impacts on system performance both from a perspective of augmenting the existing user feedback (review words) and incorporating the user sentiment from review texts (\cite{dang2020sentiment}; \cite{hernandez2019comparative}). As such we pursue integrating both user reviews and review sentiment into our neural collaborative filtering architecture - the details of which will be discussed further in Chapter \ref{Chapter4}.


\section{Other Recommenders: a brief overview and history}
\label{sec:2 Other Recommenders: a brief overview and history}

While this paper shall focus solely on the application and enhancement of established techniques in collaborative filtering, it is essential to acknowledge the broader landscape of recommender systems, encompassing various methodologies beyond collaborative filtering. For recommender systems, there are three predominant categories: collaborative filtering, content-based filtering, and hybrid models \cite{sarwar2000analysis}. Collaborative filtering, the central theme of this paper, leverages user preferences and behaviours to provide personalised recommendations \cite{smith2017two}. In contrast, content-based filtering relies on the intrinsic attributes and characteristics of items to offer suggestions \cite{lops2011content}. Hybrid models, as the name implies, integrates elements of both collaborative and content-based approaches, aiming to harness the strengths of each \cite{thorat2015survey}. In this section, we briefly explore the historical development and distinctive features of content-based filtering and hybrid models within the realm of recommender systems, emphasising how they differ from collaborative filtering. Hereby, we can provide a greater depth of understanding of recommender systems and also place collaborative filtering within the broader spectrum of methodologies.



\subsection{Content-Based Filtering}
\label{subsec:2 Content-Based Filtering}

Content-based filtering approaches the recommendation problem as a quest to find related items - that is, to find other items with similar attributes \cite{balabanovic1997fab}. The method relies on the item’s information, neglecting contributions from other users -  in contrast to the workings of collaborative filtering - focusing on recommending items akin to those the user has liked in the past \cite{pazzani1999framework}. The simplicity of content-based filtering lies in its three-step process: extracting item attributes, comparing them with user preferences, and recommending items based on matching features \cite{linden2003amazon}. One notable example of successful content-based filtering is the Music Genome Project, used by Pandora for its internet radio service /cite{koren2009matrix}. Content-based methods are a popular approach when it comes to domains such as streaming platforms for TV shows, music or movies where the items have a rich set of attributes that can be used to describe them and user preferences can be easily inferred from the attributes of the items \cite{chen2017fully}.

Research suggests that most content-based recommender systems use relatively simple algorithms, such as keyword matching or a vector space model with basic TF-IDF weighting \cite{musto2015word}. These models operate by representing user preferences and item descriptions as vectors in a high-dimensional space, measuring their similarity using cosine similarity or other distance metrics \cite{weihong2006commerce}. Recent developments in content-based filtering include word embedding techniques like Latent Semantic Indexing\footnote{Latent Semantic Indexing (LSI) is a technique to analyse the relationships between terms and documents in a corpus. It works by representing documents and terms as vectors in a high-dimensional space, where the similarity between documents or terms is calculated based on the cosine similarity between their corresponding vectors} \cite{chan2011web} and Word2Vec\footnote{Word2Vec is a word embedding technique that learns distributed representations of words in a continuous vector space. It is based on neural network architectures, such as the skip-gram and continuous bag-of-words models, which are trained on large text corpora to predict the context or neighboring words of a target word} \cite{ozsoy2016word}. Effectively, these techniques represent items and user profiles in a low-dimensional vector space, allowing for measuring similarity based on semantic understanding and improving the quality of recommendations \cite{mikolov2013distributed}.

Despite its success in several domains, collaborative filtering methods often outperform content-based ones \cite{thorat2015survey}. However, content-based filtering, unlike collaborative methodologies, are unaffected and alleviate the cold-start problem \cite{chen2017fully} - the challenge when it comes to recommending items to new users (or new items to users). In addition to alleviating the cold-start problem, content-based filtering methods also offer transparency in recommendation explanations by explicitly listing content features that influenced the recommendations, unlike collaborative systems, which are often perceived as black boxes \cite{lops2011content}. However content-based algorithms often deliver recommendations that are either too broad, like best-selling drama DVDs, or overly specific, such as all books by the same author - this problem is termed overspecialisation \cite{linden2003amazon}. The recommendations tend to be confined to items similar to those already rated by the user, limiting the system's ability to suggest novel or unexpected items, a phenomenon known as the serendipity problem. Introducing randomness or filtering out items that are too similar to those the user has seen before are potential solutions that have been explored to overcome these shortfalls \cite{lops2011content}. 

\subsection{Hybrid Models}
\label{subsec:2 Hybrid Models}

Despite the successes of content-based filtering and collaborative-based approaches, each method has its own limitations, including issues like overspecialisation, cold-start problems, sparsity, and scalability. This was acknowledged early into these model inceptions and led to hybrid filtering implementations \cite{vassiliou2006hybrid}.

Hybrid filtering, entails a recommender system which combines two or more filtering techniques (such as collaborative-based and content-based filtering), with the aim to enhance recommender system performance and mitigate the limitations associated with the individual approaches \cite{sanchez2013improving}. These hybrid systems leverage the strengths of each method while compensating for their inherent weaknesses \cite{zhou2010solving}. Several comparative studies demonstrate the superior performance of hybrid recommender systems compared to standalone approaches (\cite{burke2007hybrid};\cite{vassiliou2006hybrid};\cite{ccano2017hybrid}). 

The most common type of hybrid models are the weighted hybrid, cascade hybrid, and switching hybrid recommender systems \cite{ccano2017hybrid}. The weighted hybrid model combines the scores from different recommenders, the cascade hybrid model uses the output of one recommender as input to another, and the switching hybrid model select the most appropriate recommendation model based on certain conditions or user preferences \cite{ccano2017hybrid}. All of these types of hybrid models typically involve combining collaborative and content-based filtering methods. 

Although hybrid models have been shown to outperform standalone methods, they are not without their challenges. These type of models are more complex and require more computational resources \cite{ccano2017hybrid}. Additionally, the performance of hybrid models is highly dependent on the quality of the individual methods being combined \cite{ccano2017hybrid}. Additionally, selecting appropriate weighting or integration strategies for combining different recommendation sources can be non-trivial and may impact system performance \cite{thorat2015survey}. Despite these challenges, hybrid models have been widely adopted in practice, with many commercial recommender systems, such as Netflix and Amazon, using hybrid models to provide recommendations (\cite{ccano2017hybrid}; \cite{thorat2015survey}).

\section{Evaluation Methods}
\label{sec:2 Evaluation Methods}

In general, evaluating recommender systems poses inherent challenges, with algorithms exhibiting variable performance across different datasets \cite{herlocker2004evaluating} due to the fact that generally collaborative filtering models (and indeed all recommender models) have been developed or tailored for specific datasets. The diverse goals of evaluations further complicate matters, as early research focused on predictive accuracy\footnote{Predictive accuracy measures how close the recommender system’s predicted ratings are to the true user ratings.}, while more recent efforts have looked to assess a recommender comprehensively, both from a perspective of predictive accuracy as well as its ability to generate useful and relevant recommendations \cite{zangerle2022evaluating}. 

Predictive accuracy metrics have facilitated early algorithm comparisons and are still widely used in evaluating recommender systems, since they are easy to understand and interpret \cite{zangerle2022evaluating}. There have been many different predictive accuracy metrics applied to collaborative filtering results, including (but not limited to) mean absolute error (\cite{breese2013empirical}; \cite{herlocker1999algorithmic};\cite{pennock2013collaborative}; \cite{resnick1994grouplens}; \cite{shardanand1995social}), correlation (\cite{hill1995recommending}; \cite{sarwar2001item}) and mean squared error (\cite{shardanand1995social}; \cite{burke2015robust}). Empirical experiments have shown that mean absolute error correlates strongly with many other proposed metrics for collaborative filtering \cite{herlocker1999algorithmic}, yet is easier to measure and has well understood significance measures. Furthermore, mean absolute error is still the most frequently used metric among collaborative filtering researchers \cite{zangerle2022evaluating}. We used mean absolute error, mean squared error and root mean square error as our chosen predictive accuracy metrics to report the performance of the prediction problem because they are most commonly used and easiest to interpret directly \cite{zangerle2022evaluating}. More details on these metrics will be discussed in Chapter \ref{Chapter4}.

Although most early research in the field has focused on improving accuracy of recommender systems, an accurate predictive recommender system does not necessitate that the recommender will generate useful or relevant recommendations. As such there have been additional, more user-centric, metrics that have been proposed to evaluate recommender systems. These metrics assess the ability of the recommender to generate useful and relevant recommendations \cite{madadipouya2017literature}. Generally, these user-centric metrics are more appropriate for evaluating the performance of a recommender system when a list of recommendations is the final output. Here, the focus is on the quality of the recommendations, rather than the accuracy of the predictions \cite{zangerle2022evaluating}. This is termed top-\textit{n} evaluation, where the goal is to evaluate the quality of the top-\textit{n} recommendations generated by the system \cite{cremonesi2010performance}. In top-\textit{n} evaluation, we use certain metrics to evaluate a recommenders ability to recommend useful or relevant items to users. These metrics include precision, recall and F1 score \cite{cremonesi2010performance}. Given that top-\textit{n} evaluation is particularly valuable in e-commerce settings, we shall use these metrics to evaluate the performance of our recommender system. We shall expand on these top-\textit{n} metrics further in Chapter \ref{Chapter4}.

Often, depending on the goals of the evaluation, different metrics may be more appropriate. For instance, if the goal is to assess the ability of the recommender to generate useful and relevant recommendations in a list of recommendations only, then top-\textit{n} evaluation metrics are more appropriate. However, if the goal is to assess the accuracy of the predictions only, then predictive accuracy metrics are more appropriate. However, in practice, it is often beneficial to use both predictive accuracy and top-\textit{n} evaluation metrics to evaluate the performance of a recommender system in a holistic manner \cite{zangerle2022evaluating}. To this end, we acknowledge the importance of a holistic view of a recommender system. To facilitate this, we incorporate predictive accuracy metrics as well as top-\textit{n} evaluation metrics to get a clearer picture as to the performance of our recommender and assess it in a comprehensive manner.

\section{Limitations and Challenges}
\label{sec:2 Limitations and Challenges}

Recommenders, particularly collaborative filtering, have achieved success across diverse domains by delivering personalised content tailored to users on various platforms \cite{chen2012critiquing}. Despite their widespread effectiveness, these systems are not immune to certain limitations. While we have briefly touched upon these limitations throughout this Chapter, this Section aims to provide a more comprehensive overview of the challenges and limitations that collaborative filtering recommenders face. These limitations are specific to collaborative filtering models; however, we also provide some domain-specific challenges that e-commerce recommender applications pose. 

\subsection{Sparsity}
\label{subsec:2 Sparsity}

Sparsity, formally, refers to the situation where the available data in a collaborative filtering recommender system is limited, resulting in a sparse user-item interaction matrix with a significant number of missing values \cite{huang2004applying}. In scenarios where the pool of available items is exceptionally large, as observed in major e-commerce platforms with millions of items \cite{ghani2002building}, the overlap between items and users becomes minimal. Effectively, we will often have instances where users don't purchase the same items. In e-commerce, it is not uncommon for users to rate or purchase only a small fraction of the available items, leading to a high degree of sparsity in the user-item interaction matrix - frequently exceeding 99 percent sparsity \cite{ghani2002building}. 

A high level of sparsity critically hinders the effectiveness of collaborative filtering approaches \cite{da2018effects}, since it diminishes the ability of the (collaborative) system to identify meaningful patterns or similarities between users or items, ultimately impacting the quality and reliability of the generated recommendations \cite{burke2015robust}.Specifically, it impedes the calculation of similarities among users, a fundamental aspect of collaborative filtering. When the sparsity is extensive, the system struggles to find a sufficient number of overlapping preferences between users, leading to ineffective similarity computations. And, even when similarities are calculable, they become unreliable due to the inadequacy of information caused by the sparsity \cite{da2018effects}.

There has been several approaches to mitigate the sparsity problem in collaborative filtering. Namely, augmenting the user-item interaction matrix with additional information, such as user demographics, item attributes, or user reviews, has been proposed to alleviate the sparsity problem \cite{srifi2020recommender}. Another approach is to use content-based filtering to alleviate the sparsity problem, as content-based filtering relies on the intrinsic attributes and characteristics of items to offer suggestions, and is not affected by the sparsity problem \cite{lops2011content}.

Ultimately, the challenge when users rate on a few items makes it increasingly difficult to discern their interests accurately. While this paper does not primarily focus on this limitation, it is nonetheless addressed due to the augmenting our collaborative model with review text - an approach well-known for mitigating or assisting recommenders in coping with sparsity issues \cite{srifi2020recommender}.


\subsection{Cold Start Problem}
\label{subsec:2 Cold Start Problem}

The cold start problem is a persistent age-old challenge encountered in recommender systems when dealing with new users or items that lack sufficient information in the system \cite{lika2014facing}. The problem emerges when novel users or items are introduced to the user-item matrix\footnote{}, preventing collaborative filtering methods from generating accurate recommendations due there being not enough ratings available about them \cite{huang2004applying}. This differs from the sparsity problem, which is concerned with the lack of ratings across the entire user-item matrix, whereas the cold start problem is concerned with the lack of ratings for new users or items specifically \cite{lika2014facing}. 

Effectively, the cold start problem encompasses three distinct scenarios within the recommender system domain \cite{lika2014facing}. The first scenario arises when a new user joins the system, and no prior information is available about their preferences, constituting the \textit{new user} cold-start problem. The second scenario emerges with the introduction of a completely new item to the system, lacking any associated ratings—an issue known as the \textit{new item} cold start problem. And finally, the third scenario occurs during the initial launch of the system when both user and item information are absent, characterising the cold start system problem. In these instances, content-based solutions, renowned for their effectiveness in handling information scarcity, can be applied to mitigate the challenges associated with cold start problems.

To address this issue, hybrid recommender techniques, combining both content and collaborative data, have been employed as solutions \cite{lika2014facing}. This is because content-based filtering leverages the inherent characteristics of items, such as textual or numerical features, to provide recommendations even when user-item interactions are limited or absent \cite{lika2014facing}. Another approach is for a recommender to ask for some base information (such as age, location and preferred genres) from the users, also known as active learning (\cite{burke2015robust}; \cite{zheng2010collaborative}). Using this information, the system can profile a user and generate initial recommendations for them, and then use the user's feedback to improve the recommendations \cite{zheng2010collaborative}. Notably, another approach has proposed a novel technique that tracks individual users' activities across multiple e-commerce sites, allowing recommendations for a cold-start user in one site to be informed by their records in other sites \cite{liu2014promoting}.

In our paper, addressing the cold start problem is not considered and is beyond the scope of the work, as such we mitigate any risk of the cold start problem by selecting or using a pool of users and items with sufficient information available (see Section \ref{sec:3 Data Collection and Preprocessing}).

\subsection{Scalability}
\label{subsec:2 Scalability}


Scalability is formally defined as the ability of a system or process to handle a growing amount of work, resources, or an expanding user base, without compromising performance, efficiency, or overall functionality \cite{burke2015robust}. As such, scalability becomes a key concern for many platforms as they grow, and their user base or item catalog rises in numbers. For example, Amazon deals with millions of customers and has a catalog of items equally as big \cite{smith2017two}. The enormity of this available data provides both opportunities and challenges. The challenge lies in
the computational complexity of collaborative filtering algorithms, particularly memory-based methods, which grow quadratically with the number of users or items \cite{singh2020scalability}. This is due to the fact that collaborative filtering algorithms (specifically memory-based methods) rely on the computation of a similarity measure, which becomes prohibitively expensive with larger datasets, hampering scalability and necessitating significant memory and computational power \cite{smith2017two}. The scale at which recommender systems operate, especially for successful internet companies like Amazon, incurs substantial infrastructure costs and limitations due to the escalating volume of processed data \cite{singh2020scalability}. This added dimension makes an interesting problem for building recommenders.

In practice, the issue of scalability is addressed by dividing the prediction generation steps of the recommendation process into offline and online components, where the offline part requires extensive computation, and the online component dynamically generates predictions for users in real time (\cite{singh2020scalability}; \cite{sarwar2000analysis}). Additionally, it is also not uncommon for companies to incorporate collaborative filtering algorithms into distributed computing engines such as Apache Hadoop or Spark, leveraging their speed and efficiency in parallel large-scale data processing (\cite{burke2015robust};\cite{smith2017two}). Other more simplistic techniques to handle scalability involve methods such as sampling users, data partitioning, and omitting high or low-frequency items to manage scalability, yet these strategies face the risk of compromising the quality of recommendations \cite{singh2020scalability}. Dimensionality reduction techniques like clustering and principal component analysis have also been considered too, however, similar to aforementioned techniques they can potentially adversely impact recommendation quality by eliminating low-frequency items \cite{sarwar2000application}. 

These challenges of scalability have been long standing issues in recommender systems landscape and have been present since the inception of collaborative filtering. However, they have become more pertinent obstacles nowadays with the vast amounts of data readily available. Effectively, we acknowledge and discuss this limitation to raise awareness, though it is not an immediate concern for this paper. To address this potential obstacle in our work, we opted to reduce our dataset to a more manageable size. This allows us to eliminate any concerns related to scalability, ensuring a smoother and more efficient execution of our analysis.

\subsection{Domain and Data Challenges}
\label{subsec: Domain and Data Challenges}

Another key challenge faced by collaborative filtering methods, in particular, are rooted in their reliance on users' numeric ratings as the primary source of preference information, leading to a significant limitation in recommendation accuracy due to the often inadequate semantic explanation of scalar rating information (\cite{leino2007case}; \cite{shoja2019customer}). Recognising this drawback, efforts have been made to enhance recommendation accuracy by integrating other information with ratings - such as user reviews. This was briefly touched on in Section \ref{sec:2 Text Analysis in Recommender Systems}, when discussing the role of text for recommenders.

Furthermore, each domain has their inherent  challenges. In the context of e-commerce, there are a variety of characteristics of the field which make it slightly more challenging for collaborative filtering methods. Some of these challenges have been discussed already (cold-start problem and scalability), however, we discuss them below in the context of e-commerce.

\begin{enumerate}
    \item \textbf{Scalability and need for real-time results}: As mentioned in the previous subsection (see Section \ref{subsec:2 Sparsity}), recommenders in E-commerce are often faced with scalability obstacles. Large retailers contend with extensive data, managing tens of millions of customers and millions of distinct catalog items. Navigating this vast landscape poses a considerable challenge to generate useful recommendations. This coupled with the fact that many applications demand real-time results, requiring recommendations to be generated in no more than half a second while maintaining high-quality outputs. This necessity adds a layer of complexity to the operational efficiency of recommendation algorithms \cite{linden2003amazon}.
    \item \textbf{Cold Start Problem}: There is limited information for new customers or items on an e-commerce platform - as discussed in earlier subsections (see Section \ref{subsec:2 Cold Start Problem}). Given the limited or non-existent user history, it is difficult to generate accurate recommendations for new user. In e-commerce platforms, this is particularly challenging as the user base is constantly growing and new items are being added to the catalog \cite{linden2003amazon}.
    \item \textbf{Volatile Customer Data}: Customer interactions in e-commerce are dynamic and volatile, with each new interaction offering valuable data. Effectively, the preferences and interests of users are constantly evolving, and their interactions with items are continuously changing. Recommendation algorithms must promptly respond to this evolving information to ensure relevance and accuracy \cite{linden2003amazon}.
\end{enumerate}

Besides these aforementioned challenges, many novel issues have begun to appear more recently. Generally, progress and propagation of new techniques brings new challenges \cite{smith2017two}. For example, GPS equipped mobile phones have become mainstream and internet access is ubiquitous, as such location-based recommendation is now feasible and increasingly significant\footnote{Websites like Foursquare, Gowalla, Google Latitude, Facebook, Jiapang, and others already provide location-based services and show that many people want to share their location information and get location-based recommendations.}. 

Additionally, one of the more interesting challenges for research on recommender systems is that there are a large number of factors which affect recommendation quality; namely, data partitioning (train and test sizes), similarity measures, recommendation list size amongst some others. These factors are greatly explored in the paper by \cite{symeonidis2008collaborative}. We have thus used the literature from this paper to infer the most commonly used approaches and techniques used in selecting the optimum values for these certain factors. These will be discussed in detail in Chapters \ref{Chapter3} and Chapters \ref{Chapter4}.

Given these discussions on the specific limitations for this domain, the scope of this paper does not extend to a comprehensive discussion or addressing these issues. Primarily, we are concerned with development and analysis of a neural collaborative filtering network together with multiple modalities (text and ratings data) to better improve the performance (in terms of predictive accuracy and top-\textit{n} generation) of recommender systems. Effectively, we have taken the neccesary steps to adjust the dataset to mitigate any potential issues related to scalability and cold start problems. The limitations that these recommender systems approaches suffer from inherently as discussed in this section were to simply build a better picture of collaborative filtering approach.

\section{Conclusion}
\label{sec: Conclusion}

This Chapter reviewed past work which is related to our research. We began this Chapter by presenting a giving a brief history of recommender systems as well as their impact and application in e-commerce, where they play a significant role in better engaging and driving sales between content and consumer. Recommender systems prove to be a key element in the operation of e-commerce platforms looking to meet their customer's needs. These recommender systems filter out information desired or would perhaps be of interest to the user. Ultimately, a good recommender system staves off a user's time, keeps them engaged, enhancing their experience and consequently leads to increased revenue.

In Section \ref{sec:2 Collaborative Filtering}, we delved into the details and literature around collaborative-based filtering, one of the three main recommender system paradigms. We found that this method has its own unique strengths as well as weaknesses. For example, collaborative filtering can excel in providing accurate recommendations based on user preferences, historical behaviors, or item similarities, but its performance can be significantly negatively affected by user feedback sparsity.

We also looked at deep learning and text analysis for recommender systems in Sections \ref{sec:2 Deep Learning in Recommender Systems} and \ref{sec:2 Text Analysis in Recommender Systems} respectively, particularly focusing on the recent implementations of neural collaborative filtering. This historical analysis provides context for the choices made in this paper, such as the decision to use a deep learning approach for collaborative filtering as well as why we augment our NCF model to incorporate textual features and user sentiments. Specifically, there is scope that neural collaborative filtering enhances traditional collaborative filtering methods by capturing intricate patterns and non-linear relationships in user-item interactions, while the incorporation of textual features and user sentiments has been shown to enrich a model's understanding of user preferences, resulting in more nuanced and tailored recommendations (as well as addressing sparsity concerns in the data).

We also addressed all the various aspects of evaluation for recommender systems in Section \ref{sec:2 Evaluation Methods}. This serves as a guide to help support the decision for the metrics used in this paper. We have decided to use predictive accuracy metrics as well as top-\textit{n} evaluation metrics to assess our recommender systems going forward. This decision enables us to have a more comprehensive perspective of the performance of the recommender system as well as alleviate the problems pertaining to the narrow rating prediction (predictive accuracy) metrics.

The final Section in this Chapter addressed the limitations and challenges faced by collaborative based filtering systems and recommender systems as a whole in general. This section discussed in length data sparsity, cold start problem, scalability as well as domain specific problems such as the volatile nature of customer preferences. To this end we outlined that the scope of this paper does not seek to answer or address all of these problems. As such we have taken the necessary steps to mitigate any potential issues related to scalability and cold start problems. The concern of data sparsity is inadvertently addressed through us augmenting our collaborative filtering with textual features.

Ultimately, this review has established context for the rest of the paper. As we transition to the next Chapter on the data used in this paper, it becomes evident that the success of these models is tied to the quality and characteristics of the data they leverage. Understanding the nuances of data used in recommender systems is crucial for establishing our methodologies. In the forthcoming section, we focus on the the data used in our paper. 








 